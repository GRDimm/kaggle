{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72332fee-9d96-4e3b-b248-f13a9a401641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7bb6b6a-9acf-47dc-b3f6-7ffcced23252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Epoch 1/1000\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11155, saving model to best_model.h5\n",
      "197/197 - 6s - loss: 0.4131 - accuracy: 0.8702 - val_loss: 16.3744 - val_accuracy: 0.1115 - 6s/epoch - 32ms/step\n",
      "Epoch 2/1000\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.11155 to 0.11405, saving model to best_model.h5\n",
      "197/197 - 5s - loss: 0.0973 - accuracy: 0.9697 - val_loss: 7.4561 - val_accuracy: 0.1140 - 5s/epoch - 24ms/step\n",
      "Epoch 3/1000\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.11405 to 0.95702, saving model to best_model.h5\n",
      "197/197 - 5s - loss: 0.0680 - accuracy: 0.9786 - val_loss: 0.1553 - val_accuracy: 0.9570 - 5s/epoch - 24ms/step\n",
      "Epoch 4/1000\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.95702 to 0.97619, saving model to best_model.h5\n",
      "197/197 - 5s - loss: 0.0575 - accuracy: 0.9814 - val_loss: 0.0864 - val_accuracy: 0.9762 - 5s/epoch - 24ms/step\n",
      "Epoch 5/1000\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.97619 to 0.98417, saving model to best_model.h5\n",
      "197/197 - 5s - loss: 0.0475 - accuracy: 0.9846 - val_loss: 0.0536 - val_accuracy: 0.9842 - 5s/epoch - 24ms/step\n",
      "Epoch 6/1000\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.98417 to 0.98774, saving model to best_model.h5\n",
      "197/197 - 5s - loss: 0.0434 - accuracy: 0.9862 - val_loss: 0.0454 - val_accuracy: 0.9877 - 5s/epoch - 24ms/step\n",
      "Epoch 7/1000\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.98774\n",
      "197/197 - 5s - loss: 0.0363 - accuracy: 0.9884 - val_loss: 0.0416 - val_accuracy: 0.9876 - 5s/epoch - 23ms/step\n",
      "Epoch 8/1000\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.98774 to 0.99024, saving model to best_model.h5\n",
      "197/197 - 5s - loss: 0.0337 - accuracy: 0.9896 - val_loss: 0.0352 - val_accuracy: 0.9902 - 5s/epoch - 24ms/step\n",
      "Epoch 9/1000\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.99024\n",
      "197/197 - 5s - loss: 0.0282 - accuracy: 0.9903 - val_loss: 0.0441 - val_accuracy: 0.9871 - 5s/epoch - 23ms/step\n",
      "Epoch 10/1000\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.99024\n",
      "197/197 - 5s - loss: 0.0306 - accuracy: 0.9901 - val_loss: 0.0433 - val_accuracy: 0.9881 - 5s/epoch - 23ms/step\n",
      "Epoch 11/1000\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.99024\n",
      "197/197 - 5s - loss: 0.0254 - accuracy: 0.9915 - val_loss: 0.0481 - val_accuracy: 0.9867 - 5s/epoch - 23ms/step\n",
      "Epoch 12/1000\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.99024\n",
      "197/197 - 5s - loss: 0.0244 - accuracy: 0.9923 - val_loss: 0.0373 - val_accuracy: 0.9892 - 5s/epoch - 23ms/step\n",
      "Epoch 13/1000\n",
      "\n",
      "Epoch 13: val_accuracy improved from 0.99024 to 0.99143, saving model to best_model.h5\n",
      "197/197 - 5s - loss: 0.0225 - accuracy: 0.9923 - val_loss: 0.0323 - val_accuracy: 0.9914 - 5s/epoch - 24ms/step\n",
      "Epoch 14/1000\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.99143\n",
      "197/197 - 5s - loss: 0.0216 - accuracy: 0.9930 - val_loss: 0.0357 - val_accuracy: 0.9905 - 5s/epoch - 23ms/step\n",
      "Epoch 15/1000\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.99143\n",
      "197/197 - 5s - loss: 0.0211 - accuracy: 0.9936 - val_loss: 0.0349 - val_accuracy: 0.9908 - 5s/epoch - 23ms/step\n",
      "Epoch 16/1000\n",
      "\n",
      "Epoch 16: val_accuracy improved from 0.99143 to 0.99149, saving model to best_model.h5\n",
      "197/197 - 5s - loss: 0.0204 - accuracy: 0.9933 - val_loss: 0.0342 - val_accuracy: 0.9915 - 5s/epoch - 24ms/step\n",
      "Epoch 17/1000\n",
      "\n",
      "Epoch 17: val_accuracy improved from 0.99149 to 0.99256, saving model to best_model.h5\n",
      "197/197 - 5s - loss: 0.0166 - accuracy: 0.9946 - val_loss: 0.0337 - val_accuracy: 0.9926 - 5s/epoch - 24ms/step\n",
      "Epoch 18/1000\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.99256\n",
      "197/197 - 5s - loss: 0.0142 - accuracy: 0.9950 - val_loss: 0.0509 - val_accuracy: 0.9869 - 5s/epoch - 24ms/step\n",
      "Epoch 19/1000\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.99256\n",
      "197/197 - 5s - loss: 0.0128 - accuracy: 0.9960 - val_loss: 0.0538 - val_accuracy: 0.9871 - 5s/epoch - 24ms/step\n",
      "Epoch 20/1000\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.99256\n",
      "197/197 - 5s - loss: 0.0148 - accuracy: 0.9954 - val_loss: 0.0455 - val_accuracy: 0.9880 - 5s/epoch - 24ms/step\n",
      "Epoch 21/1000\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.99256\n",
      "197/197 - 5s - loss: 0.0156 - accuracy: 0.9947 - val_loss: 0.0398 - val_accuracy: 0.9899 - 5s/epoch - 24ms/step\n",
      "Epoch 22/1000\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.99256\n",
      "197/197 - 5s - loss: 0.0147 - accuracy: 0.9945 - val_loss: 0.0462 - val_accuracy: 0.9894 - 5s/epoch - 24ms/step\n",
      "Epoch 23/1000\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.99256\n",
      "197/197 - 5s - loss: 0.0147 - accuracy: 0.9952 - val_loss: 0.0360 - val_accuracy: 0.9913 - 5s/epoch - 25ms/step\n",
      "Epoch 24/1000\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.99256\n",
      "197/197 - 5s - loss: 0.0110 - accuracy: 0.9964 - val_loss: 0.0333 - val_accuracy: 0.9920 - 5s/epoch - 24ms/step\n",
      "Epoch 25/1000\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.99256\n",
      "197/197 - 5s - loss: 0.0110 - accuracy: 0.9964 - val_loss: 0.0374 - val_accuracy: 0.9912 - 5s/epoch - 25ms/step\n",
      "Epoch 26/1000\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.99256\n",
      "197/197 - 5s - loss: 0.0134 - accuracy: 0.9952 - val_loss: 0.0383 - val_accuracy: 0.9910 - 5s/epoch - 25ms/step\n",
      "Epoch 27/1000\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.99256\n",
      "197/197 - 5s - loss: 0.0133 - accuracy: 0.9958 - val_loss: 0.0452 - val_accuracy: 0.9883 - 5s/epoch - 25ms/step\n",
      "Epoch 28/1000\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.99256\n",
      "197/197 - 5s - loss: 0.0125 - accuracy: 0.9960 - val_loss: 0.0481 - val_accuracy: 0.9885 - 5s/epoch - 25ms/step\n",
      "Epoch 29/1000\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.99256\n",
      "197/197 - 5s - loss: 0.0106 - accuracy: 0.9967 - val_loss: 0.0715 - val_accuracy: 0.9849 - 5s/epoch - 25ms/step\n",
      "Epoch 30/1000\n",
      "\n",
      "Epoch 30: val_accuracy improved from 0.99256 to 0.99262, saving model to best_model.h5\n",
      "197/197 - 5s - loss: 0.0088 - accuracy: 0.9967 - val_loss: 0.0298 - val_accuracy: 0.9926 - 5s/epoch - 26ms/step\n",
      "Epoch 31/1000\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.99262\n",
      "197/197 - 5s - loss: 0.0110 - accuracy: 0.9970 - val_loss: 0.0512 - val_accuracy: 0.9889 - 5s/epoch - 25ms/step\n",
      "Epoch 32/1000\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.99262\n",
      "197/197 - 5s - loss: 0.0118 - accuracy: 0.9962 - val_loss: 0.0848 - val_accuracy: 0.9830 - 5s/epoch - 25ms/step\n",
      "Epoch 33/1000\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.99262\n",
      "197/197 - 6s - loss: 0.0090 - accuracy: 0.9970 - val_loss: 0.0420 - val_accuracy: 0.9895 - 6s/epoch - 28ms/step\n",
      "Epoch 34/1000\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.99262\n",
      "197/197 - 5s - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.0420 - val_accuracy: 0.9908 - 5s/epoch - 26ms/step\n",
      "Epoch 35/1000\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.99262\n",
      "197/197 - 5s - loss: 0.0085 - accuracy: 0.9969 - val_loss: 0.0402 - val_accuracy: 0.9903 - 5s/epoch - 28ms/step\n",
      "Epoch 36/1000\n",
      "\n",
      "Epoch 36: val_accuracy improved from 0.99262 to 0.99280, saving model to best_model.h5\n",
      "197/197 - 6s - loss: 0.0084 - accuracy: 0.9971 - val_loss: 0.0338 - val_accuracy: 0.9928 - 6s/epoch - 30ms/step\n",
      "Epoch 37/1000\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.99280\n",
      "197/197 - 6s - loss: 0.0127 - accuracy: 0.9957 - val_loss: 0.0339 - val_accuracy: 0.9914 - 6s/epoch - 28ms/step\n",
      "Epoch 38/1000\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.99280\n",
      "197/197 - 6s - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.0393 - val_accuracy: 0.9915 - 6s/epoch - 29ms/step\n",
      "Epoch 39/1000\n",
      "\n",
      "Epoch 39: val_accuracy improved from 0.99280 to 0.99304, saving model to best_model.h5\n",
      "197/197 - 6s - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0325 - val_accuracy: 0.9930 - 6s/epoch - 30ms/step\n",
      "Epoch 40/1000\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0046 - accuracy: 0.9983 - val_loss: 0.0453 - val_accuracy: 0.9898 - 6s/epoch - 29ms/step\n",
      "Epoch 41/1000\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0077 - accuracy: 0.9974 - val_loss: 0.0423 - val_accuracy: 0.9901 - 6s/epoch - 29ms/step\n",
      "Epoch 42/1000\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0108 - accuracy: 0.9966 - val_loss: 0.0336 - val_accuracy: 0.9919 - 6s/epoch - 29ms/step\n",
      "Epoch 43/1000\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.0468 - val_accuracy: 0.9887 - 6s/epoch - 29ms/step\n",
      "Epoch 44/1000\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0412 - val_accuracy: 0.9912 - 6s/epoch - 29ms/step\n",
      "Epoch 45/1000\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0083 - accuracy: 0.9969 - val_loss: 0.0365 - val_accuracy: 0.9914 - 6s/epoch - 29ms/step\n",
      "Epoch 46/1000\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.9924 - 6s/epoch - 29ms/step\n",
      "Epoch 47/1000\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0071 - accuracy: 0.9978 - val_loss: 0.0374 - val_accuracy: 0.9923 - 6s/epoch - 29ms/step\n",
      "Epoch 48/1000\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.0420 - val_accuracy: 0.9910 - 6s/epoch - 29ms/step\n",
      "Epoch 49/1000\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0059 - accuracy: 0.9981 - val_loss: 0.0452 - val_accuracy: 0.9905 - 6s/epoch - 29ms/step\n",
      "Epoch 50/1000\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0076 - accuracy: 0.9975 - val_loss: 0.0355 - val_accuracy: 0.9926 - 6s/epoch - 29ms/step\n",
      "Epoch 51/1000\n",
      "\n",
      "Epoch 51: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.0392 - val_accuracy: 0.9917 - 6s/epoch - 29ms/step\n",
      "Epoch 52/1000\n",
      "\n",
      "Epoch 52: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.0380 - val_accuracy: 0.9922 - 6s/epoch - 29ms/step\n",
      "Epoch 53/1000\n",
      "\n",
      "Epoch 53: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0063 - accuracy: 0.9981 - val_loss: 0.0395 - val_accuracy: 0.9925 - 6s/epoch - 29ms/step\n",
      "Epoch 54/1000\n",
      "\n",
      "Epoch 54: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0061 - accuracy: 0.9981 - val_loss: 0.0365 - val_accuracy: 0.9920 - 6s/epoch - 29ms/step\n",
      "Epoch 55/1000\n",
      "\n",
      "Epoch 55: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0057 - accuracy: 0.9981 - val_loss: 0.0539 - val_accuracy: 0.9895 - 6s/epoch - 29ms/step\n",
      "Epoch 56/1000\n",
      "\n",
      "Epoch 56: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0061 - accuracy: 0.9979 - val_loss: 0.0372 - val_accuracy: 0.9919 - 6s/epoch - 29ms/step\n",
      "Epoch 57/1000\n",
      "\n",
      "Epoch 57: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0051 - accuracy: 0.9983 - val_loss: 0.0398 - val_accuracy: 0.9918 - 6s/epoch - 29ms/step\n",
      "Epoch 58/1000\n",
      "\n",
      "Epoch 58: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0071 - accuracy: 0.9976 - val_loss: 0.0456 - val_accuracy: 0.9905 - 6s/epoch - 29ms/step\n",
      "Epoch 59/1000\n",
      "\n",
      "Epoch 59: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0032 - accuracy: 0.9987 - val_loss: 0.0350 - val_accuracy: 0.9925 - 6s/epoch - 29ms/step\n",
      "Epoch 60/1000\n",
      "\n",
      "Epoch 60: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.0368 - val_accuracy: 0.9914 - 6s/epoch - 29ms/step\n",
      "Epoch 61/1000\n",
      "\n",
      "Epoch 61: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.0388 - val_accuracy: 0.9922 - 6s/epoch - 29ms/step\n",
      "Epoch 62/1000\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.0430 - val_accuracy: 0.9916 - 6s/epoch - 29ms/step\n",
      "Epoch 63/1000\n",
      "\n",
      "Epoch 63: val_accuracy did not improve from 0.99304\n",
      "197/197 - 6s - loss: 0.0050 - accuracy: 0.9983 - val_loss: 0.0370 - val_accuracy: 0.9914 - 6s/epoch - 29ms/step\n",
      "Epoch 64/1000\n",
      "\n",
      "Epoch 64: val_accuracy improved from 0.99304 to 0.99357, saving model to best_model.h5\n",
      "197/197 - 6s - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.0307 - val_accuracy: 0.9936 - 6s/epoch - 30ms/step\n",
      "Epoch 65/1000\n",
      "\n",
      "Epoch 65: val_accuracy did not improve from 0.99357\n",
      "197/197 - 6s - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.0429 - val_accuracy: 0.9915 - 6s/epoch - 29ms/step\n",
      "Epoch 66/1000\n",
      "\n",
      "Epoch 66: val_accuracy did not improve from 0.99357\n",
      "197/197 - 6s - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.0329 - val_accuracy: 0.9927 - 6s/epoch - 29ms/step\n",
      "Epoch 67/1000\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.99357\n",
      "197/197 - 6s - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.0341 - val_accuracy: 0.9934 - 6s/epoch - 29ms/step\n",
      "Epoch 68/1000\n",
      "\n",
      "Epoch 68: val_accuracy did not improve from 0.99357\n",
      "197/197 - 6s - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0428 - val_accuracy: 0.9906 - 6s/epoch - 29ms/step\n",
      "Epoch 69/1000\n",
      "\n",
      "Epoch 69: val_accuracy did not improve from 0.99357\n",
      "197/197 - 6s - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0357 - val_accuracy: 0.9922 - 6s/epoch - 29ms/step\n",
      "Epoch 70/1000\n",
      "\n",
      "Epoch 70: val_accuracy did not improve from 0.99357\n",
      "197/197 - 6s - loss: 0.0033 - accuracy: 0.9986 - val_loss: 0.0473 - val_accuracy: 0.9902 - 6s/epoch - 31ms/step\n",
      "Epoch 71/1000\n",
      "\n",
      "Epoch 71: val_accuracy did not improve from 0.99357\n",
      "197/197 - 6s - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0404 - val_accuracy: 0.9914 - 6s/epoch - 32ms/step\n",
      "Epoch 72/1000\n",
      "\n",
      "Epoch 72: val_accuracy did not improve from 0.99357\n",
      "197/197 - 6s - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.0459 - val_accuracy: 0.9909 - 6s/epoch - 32ms/step\n",
      "Epoch 73/1000\n",
      "\n",
      "Epoch 73: val_accuracy did not improve from 0.99357\n",
      "197/197 - 6s - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0358 - val_accuracy: 0.9928 - 6s/epoch - 32ms/step\n",
      "Epoch 74/1000\n",
      "\n",
      "Epoch 74: val_accuracy did not improve from 0.99357\n",
      "197/197 - 6s - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0340 - val_accuracy: 0.9927 - 6s/epoch - 32ms/step\n",
      "Epoch 75/1000\n",
      "\n",
      "Epoch 75: val_accuracy did not improve from 0.99357\n",
      "197/197 - 6s - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0368 - val_accuracy: 0.9927 - 6s/epoch - 32ms/step\n",
      "Epoch 76/1000\n",
      "\n",
      "Epoch 76: val_accuracy did not improve from 0.99357\n",
      "197/197 - 6s - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.0434 - val_accuracy: 0.9912 - 6s/epoch - 32ms/step\n",
      "Epoch 77/1000\n",
      "\n",
      "Epoch 77: val_accuracy did not improve from 0.99357\n",
      "197/197 - 6s - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.0359 - val_accuracy: 0.9924 - 6s/epoch - 32ms/step\n",
      "Epoch 78/1000\n",
      "\n",
      "Epoch 78: val_accuracy did not improve from 0.99357\n",
      "197/197 - 6s - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.0591 - val_accuracy: 0.9892 - 6s/epoch - 32ms/step\n",
      "Epoch 79/1000\n",
      "\n",
      "Epoch 79: val_accuracy did not improve from 0.99357\n",
      "197/197 - 6s - loss: 0.0032 - accuracy: 0.9987 - val_loss: 0.0357 - val_accuracy: 0.9934 - 6s/epoch - 32ms/step\n",
      "Epoch 80/1000\n",
      "\n",
      "Epoch 80: val_accuracy did not improve from 0.99357\n",
      "197/197 - 6s - loss: 0.0027 - accuracy: 0.9990 - val_loss: 0.0365 - val_accuracy: 0.9926 - 6s/epoch - 32ms/step\n",
      "Epoch 81/1000\n",
      "\n",
      "Epoch 81: val_accuracy improved from 0.99357 to 0.99363, saving model to best_model.h5\n",
      "197/197 - 6s - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.0342 - val_accuracy: 0.9936 - 6s/epoch - 33ms/step\n",
      "Epoch 82/1000\n",
      "\n",
      "Epoch 82: val_accuracy did not improve from 0.99363\n",
      "197/197 - 6s - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.0383 - val_accuracy: 0.9920 - 6s/epoch - 32ms/step\n",
      "Epoch 83/1000\n",
      "\n",
      "Epoch 83: val_accuracy did not improve from 0.99363\n",
      "197/197 - 6s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0367 - val_accuracy: 0.9920 - 6s/epoch - 31ms/step\n",
      "Epoch 84/1000\n",
      "\n",
      "Epoch 84: val_accuracy did not improve from 0.99363\n",
      "197/197 - 7s - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0378 - val_accuracy: 0.9929 - 7s/epoch - 33ms/step\n",
      "Epoch 85/1000\n",
      "\n",
      "Epoch 85: val_accuracy did not improve from 0.99363\n",
      "197/197 - 6s - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.0345 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 86/1000\n",
      "\n",
      "Epoch 86: val_accuracy did not improve from 0.99363\n",
      "197/197 - 6s - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.0339 - val_accuracy: 0.9926 - 6s/epoch - 32ms/step\n",
      "Epoch 87/1000\n",
      "\n",
      "Epoch 87: val_accuracy did not improve from 0.99363\n",
      "197/197 - 6s - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0339 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 88/1000\n",
      "\n",
      "Epoch 88: val_accuracy did not improve from 0.99363\n",
      "197/197 - 6s - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0345 - val_accuracy: 0.9930 - 6s/epoch - 32ms/step\n",
      "Epoch 89/1000\n",
      "\n",
      "Epoch 89: val_accuracy did not improve from 0.99363\n",
      "197/197 - 6s - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0357 - val_accuracy: 0.9930 - 6s/epoch - 32ms/step\n",
      "Epoch 90/1000\n",
      "\n",
      "Epoch 90: val_accuracy did not improve from 0.99363\n",
      "197/197 - 6s - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0405 - val_accuracy: 0.9924 - 6s/epoch - 32ms/step\n",
      "Epoch 91/1000\n",
      "\n",
      "Epoch 91: val_accuracy did not improve from 0.99363\n",
      "197/197 - 6s - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0370 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 92/1000\n",
      "\n",
      "Epoch 92: val_accuracy did not improve from 0.99363\n",
      "197/197 - 6s - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.0406 - val_accuracy: 0.9923 - 6s/epoch - 32ms/step\n",
      "Epoch 93/1000\n",
      "\n",
      "Epoch 93: val_accuracy did not improve from 0.99363\n",
      "197/197 - 6s - loss: 0.0025 - accuracy: 0.9990 - val_loss: 0.0415 - val_accuracy: 0.9920 - 6s/epoch - 32ms/step\n",
      "Epoch 94/1000\n",
      "\n",
      "Epoch 94: val_accuracy did not improve from 0.99363\n",
      "197/197 - 6s - loss: 0.0042 - accuracy: 0.9992 - val_loss: 0.0381 - val_accuracy: 0.9930 - 6s/epoch - 32ms/step\n",
      "Epoch 95/1000\n",
      "\n",
      "Epoch 95: val_accuracy did not improve from 0.99363\n",
      "197/197 - 7s - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0428 - val_accuracy: 0.9924 - 7s/epoch - 33ms/step\n",
      "Epoch 96/1000\n",
      "\n",
      "Epoch 96: val_accuracy did not improve from 0.99363\n",
      "197/197 - 7s - loss: 0.0041 - accuracy: 0.9989 - val_loss: 0.0358 - val_accuracy: 0.9929 - 7s/epoch - 37ms/step\n",
      "Epoch 97/1000\n",
      "\n",
      "Epoch 97: val_accuracy did not improve from 0.99363\n",
      "197/197 - 7s - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0368 - val_accuracy: 0.9925 - 7s/epoch - 36ms/step\n",
      "Epoch 98/1000\n",
      "\n",
      "Epoch 98: val_accuracy did not improve from 0.99363\n",
      "197/197 - 6s - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.0353 - val_accuracy: 0.9933 - 6s/epoch - 31ms/step\n",
      "Epoch 99/1000\n",
      "\n",
      "Epoch 99: val_accuracy improved from 0.99363 to 0.99423, saving model to best_model.h5\n",
      "197/197 - 6s - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0282 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 100/1000\n",
      "\n",
      "Epoch 100: val_accuracy did not improve from 0.99423\n",
      "197/197 - 6s - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0348 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 101/1000\n",
      "\n",
      "Epoch 101: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0013 - accuracy: 0.9994 - val_loss: 0.0382 - val_accuracy: 0.9924 - 7s/epoch - 34ms/step\n",
      "Epoch 102/1000\n",
      "\n",
      "Epoch 102: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.0355 - val_accuracy: 0.9934 - 7s/epoch - 34ms/step\n",
      "Epoch 103/1000\n",
      "\n",
      "Epoch 103: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.0401 - val_accuracy: 0.9924 - 7s/epoch - 33ms/step\n",
      "Epoch 104/1000\n",
      "\n",
      "Epoch 104: val_accuracy did not improve from 0.99423\n",
      "197/197 - 6s - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0342 - val_accuracy: 0.9937 - 6s/epoch - 33ms/step\n",
      "Epoch 105/1000\n",
      "\n",
      "Epoch 105: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.0375 - val_accuracy: 0.9932 - 7s/epoch - 37ms/step\n",
      "Epoch 106/1000\n",
      "\n",
      "Epoch 106: val_accuracy did not improve from 0.99423\n",
      "197/197 - 6s - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0403 - val_accuracy: 0.9926 - 6s/epoch - 31ms/step\n",
      "Epoch 107/1000\n",
      "\n",
      "Epoch 107: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.0413 - val_accuracy: 0.9925 - 7s/epoch - 34ms/step\n",
      "Epoch 108/1000\n",
      "\n",
      "Epoch 108: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0538 - val_accuracy: 0.9905 - 7s/epoch - 33ms/step\n",
      "Epoch 109/1000\n",
      "\n",
      "Epoch 109: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0054 - accuracy: 0.9986 - val_loss: 0.0484 - val_accuracy: 0.9914 - 7s/epoch - 33ms/step\n",
      "Epoch 110/1000\n",
      "\n",
      "Epoch 110: val_accuracy did not improve from 0.99423\n",
      "197/197 - 6s - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0411 - val_accuracy: 0.9922 - 6s/epoch - 33ms/step\n",
      "Epoch 111/1000\n",
      "\n",
      "Epoch 111: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0401 - val_accuracy: 0.9925 - 7s/epoch - 33ms/step\n",
      "Epoch 112/1000\n",
      "\n",
      "Epoch 112: val_accuracy did not improve from 0.99423\n",
      "197/197 - 6s - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0415 - val_accuracy: 0.9922 - 6s/epoch - 33ms/step\n",
      "Epoch 113/1000\n",
      "\n",
      "Epoch 113: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.0383 - val_accuracy: 0.9924 - 7s/epoch - 33ms/step\n",
      "Epoch 114/1000\n",
      "\n",
      "Epoch 114: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.0364 - val_accuracy: 0.9932 - 7s/epoch - 38ms/step\n",
      "Epoch 115/1000\n",
      "\n",
      "Epoch 115: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.0362 - val_accuracy: 0.9935 - 7s/epoch - 33ms/step\n",
      "Epoch 116/1000\n",
      "\n",
      "Epoch 116: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.0480 - val_accuracy: 0.9914 - 7s/epoch - 34ms/step\n",
      "Epoch 117/1000\n",
      "\n",
      "Epoch 117: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0027 - accuracy: 0.9990 - val_loss: 0.0393 - val_accuracy: 0.9919 - 7s/epoch - 34ms/step\n",
      "Epoch 118/1000\n",
      "\n",
      "Epoch 118: val_accuracy did not improve from 0.99423\n",
      "197/197 - 8s - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0381 - val_accuracy: 0.9926 - 8s/epoch - 40ms/step\n",
      "Epoch 119/1000\n",
      "\n",
      "Epoch 119: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.0370 - val_accuracy: 0.9936 - 7s/epoch - 38ms/step\n",
      "Epoch 120/1000\n",
      "\n",
      "Epoch 120: val_accuracy did not improve from 0.99423\n",
      "197/197 - 6s - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0379 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 121/1000\n",
      "\n",
      "Epoch 121: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0416 - val_accuracy: 0.9922 - 7s/epoch - 34ms/step\n",
      "Epoch 122/1000\n",
      "\n",
      "Epoch 122: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.0408 - val_accuracy: 0.9933 - 7s/epoch - 35ms/step\n",
      "Epoch 123/1000\n",
      "\n",
      "Epoch 123: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0033 - accuracy: 0.9987 - val_loss: 0.0382 - val_accuracy: 0.9932 - 7s/epoch - 34ms/step\n",
      "Epoch 124/1000\n",
      "\n",
      "Epoch 124: val_accuracy did not improve from 0.99423\n",
      "197/197 - 8s - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0407 - val_accuracy: 0.9928 - 8s/epoch - 39ms/step\n",
      "Epoch 125/1000\n",
      "\n",
      "Epoch 125: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0381 - val_accuracy: 0.9938 - 7s/epoch - 33ms/step\n",
      "Epoch 126/1000\n",
      "\n",
      "Epoch 126: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0339 - val_accuracy: 0.9942 - 7s/epoch - 34ms/step\n",
      "Epoch 127/1000\n",
      "\n",
      "Epoch 127: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0022 - accuracy: 0.9992 - val_loss: 0.0410 - val_accuracy: 0.9923 - 7s/epoch - 37ms/step\n",
      "Epoch 128/1000\n",
      "\n",
      "Epoch 128: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0399 - val_accuracy: 0.9931 - 7s/epoch - 34ms/step\n",
      "Epoch 129/1000\n",
      "\n",
      "Epoch 129: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0360 - val_accuracy: 0.9936 - 7s/epoch - 35ms/step\n",
      "Epoch 130/1000\n",
      "\n",
      "Epoch 130: val_accuracy did not improve from 0.99423\n",
      "197/197 - 8s - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0437 - val_accuracy: 0.9922 - 8s/epoch - 38ms/step\n",
      "Epoch 131/1000\n",
      "\n",
      "Epoch 131: val_accuracy did not improve from 0.99423\n",
      "197/197 - 7s - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0401 - val_accuracy: 0.9929 - 7s/epoch - 35ms/step\n",
      "Epoch 132/1000\n",
      "\n",
      "Epoch 132: val_accuracy did not improve from 0.99423\n",
      "197/197 - 6s - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0368 - val_accuracy: 0.9930 - 6s/epoch - 33ms/step\n",
      "Epoch 133/1000\n",
      "\n",
      "Epoch 133: val_accuracy improved from 0.99423 to 0.99440, saving model to best_model.h5\n",
      "197/197 - 7s - loss: 7.5325e-04 - accuracy: 0.9998 - val_loss: 0.0349 - val_accuracy: 0.9944 - 7s/epoch - 34ms/step\n",
      "Epoch 134/1000\n",
      "\n",
      "Epoch 134: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0383 - val_accuracy: 0.9934 - 7s/epoch - 34ms/step\n",
      "Epoch 135/1000\n",
      "\n",
      "Epoch 135: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 8.1466e-04 - accuracy: 0.9998 - val_loss: 0.0354 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 136/1000\n",
      "\n",
      "Epoch 136: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0016 - accuracy: 0.9993 - val_loss: 0.0443 - val_accuracy: 0.9927 - 6s/epoch - 32ms/step\n",
      "Epoch 137/1000\n",
      "\n",
      "Epoch 137: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.0443 - val_accuracy: 0.9918 - 7s/epoch - 33ms/step\n",
      "Epoch 138/1000\n",
      "\n",
      "Epoch 138: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0393 - val_accuracy: 0.9932 - 7s/epoch - 33ms/step\n",
      "Epoch 139/1000\n",
      "\n",
      "Epoch 139: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0504 - val_accuracy: 0.9913 - 6s/epoch - 32ms/step\n",
      "Epoch 140/1000\n",
      "\n",
      "Epoch 140: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0382 - val_accuracy: 0.9937 - 6s/epoch - 33ms/step\n",
      "Epoch 141/1000\n",
      "\n",
      "Epoch 141: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0329 - val_accuracy: 0.9933 - 7s/epoch - 34ms/step\n",
      "Epoch 142/1000\n",
      "\n",
      "Epoch 142: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0455 - val_accuracy: 0.9915 - 7s/epoch - 36ms/step\n",
      "Epoch 143/1000\n",
      "\n",
      "Epoch 143: val_accuracy did not improve from 0.99440\n",
      "197/197 - 8s - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0373 - val_accuracy: 0.9936 - 8s/epoch - 39ms/step\n",
      "Epoch 144/1000\n",
      "\n",
      "Epoch 144: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.0441 - val_accuracy: 0.9927 - 7s/epoch - 34ms/step\n",
      "Epoch 145/1000\n",
      "\n",
      "Epoch 145: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0409 - val_accuracy: 0.9929 - 6s/epoch - 33ms/step\n",
      "Epoch 146/1000\n",
      "\n",
      "Epoch 146: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0426 - val_accuracy: 0.9929 - 6s/epoch - 33ms/step\n",
      "Epoch 147/1000\n",
      "\n",
      "Epoch 147: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 6.0340e-04 - accuracy: 0.9998 - val_loss: 0.0421 - val_accuracy: 0.9931 - 7s/epoch - 34ms/step\n",
      "Epoch 148/1000\n",
      "\n",
      "Epoch 148: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0364 - val_accuracy: 0.9933 - 7s/epoch - 38ms/step\n",
      "Epoch 149/1000\n",
      "\n",
      "Epoch 149: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0464 - val_accuracy: 0.9912 - 7s/epoch - 36ms/step\n",
      "Epoch 150/1000\n",
      "\n",
      "Epoch 150: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.0387 - val_accuracy: 0.9933 - 7s/epoch - 33ms/step\n",
      "Epoch 151/1000\n",
      "\n",
      "Epoch 151: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0401 - val_accuracy: 0.9933 - 6s/epoch - 33ms/step\n",
      "Epoch 152/1000\n",
      "\n",
      "Epoch 152: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0422 - val_accuracy: 0.9931 - 7s/epoch - 36ms/step\n",
      "Epoch 153/1000\n",
      "\n",
      "Epoch 153: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0353 - val_accuracy: 0.9937 - 7s/epoch - 36ms/step\n",
      "Epoch 154/1000\n",
      "\n",
      "Epoch 154: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0392 - val_accuracy: 0.9938 - 7s/epoch - 35ms/step\n",
      "Epoch 155/1000\n",
      "\n",
      "Epoch 155: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0436 - val_accuracy: 0.9921 - 7s/epoch - 33ms/step\n",
      "Epoch 156/1000\n",
      "\n",
      "Epoch 156: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0376 - val_accuracy: 0.9938 - 7s/epoch - 35ms/step\n",
      "Epoch 157/1000\n",
      "\n",
      "Epoch 157: val_accuracy did not improve from 0.99440\n",
      "197/197 - 8s - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0436 - val_accuracy: 0.9925 - 8s/epoch - 39ms/step\n",
      "Epoch 158/1000\n",
      "\n",
      "Epoch 158: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0544 - val_accuracy: 0.9907 - 7s/epoch - 36ms/step\n",
      "Epoch 159/1000\n",
      "\n",
      "Epoch 159: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.0381 - val_accuracy: 0.9929 - 7s/epoch - 34ms/step\n",
      "Epoch 160/1000\n",
      "\n",
      "Epoch 160: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0338 - val_accuracy: 0.9937 - 6s/epoch - 33ms/step\n",
      "Epoch 161/1000\n",
      "\n",
      "Epoch 161: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 9.3661e-04 - accuracy: 0.9996 - val_loss: 0.0361 - val_accuracy: 0.9937 - 7s/epoch - 33ms/step\n",
      "Epoch 162/1000\n",
      "\n",
      "Epoch 162: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0401 - val_accuracy: 0.9932 - 7s/epoch - 36ms/step\n",
      "Epoch 163/1000\n",
      "\n",
      "Epoch 163: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 5.8211e-04 - accuracy: 0.9999 - val_loss: 0.0346 - val_accuracy: 0.9944 - 7s/epoch - 35ms/step\n",
      "Epoch 164/1000\n",
      "\n",
      "Epoch 164: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.0377 - val_accuracy: 0.9940 - 7s/epoch - 37ms/step\n",
      "Epoch 165/1000\n",
      "\n",
      "Epoch 165: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0376 - val_accuracy: 0.9929 - 7s/epoch - 37ms/step\n",
      "Epoch 166/1000\n",
      "\n",
      "Epoch 166: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0362 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 167/1000\n",
      "\n",
      "Epoch 167: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.0415 - val_accuracy: 0.9932 - 7s/epoch - 33ms/step\n",
      "Epoch 168/1000\n",
      "\n",
      "Epoch 168: val_accuracy did not improve from 0.99440\n",
      "197/197 - 7s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0384 - val_accuracy: 0.9936 - 7s/epoch - 36ms/step\n",
      "Epoch 169/1000\n",
      "\n",
      "Epoch 169: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0434 - val_accuracy: 0.9917 - 6s/epoch - 31ms/step\n",
      "Epoch 170/1000\n",
      "\n",
      "Epoch 170: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0376 - val_accuracy: 0.9943 - 6s/epoch - 31ms/step\n",
      "Epoch 171/1000\n",
      "\n",
      "Epoch 171: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0367 - val_accuracy: 0.9938 - 6s/epoch - 31ms/step\n",
      "Epoch 172/1000\n",
      "\n",
      "Epoch 172: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.0358 - val_accuracy: 0.9941 - 6s/epoch - 31ms/step\n",
      "Epoch 173/1000\n",
      "\n",
      "Epoch 173: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0418 - val_accuracy: 0.9923 - 6s/epoch - 32ms/step\n",
      "Epoch 174/1000\n",
      "\n",
      "Epoch 174: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0496 - val_accuracy: 0.9914 - 6s/epoch - 31ms/step\n",
      "Epoch 175/1000\n",
      "\n",
      "Epoch 175: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0395 - val_accuracy: 0.9930 - 6s/epoch - 31ms/step\n",
      "Epoch 176/1000\n",
      "\n",
      "Epoch 176: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.0334 - val_accuracy: 0.9940 - 6s/epoch - 31ms/step\n",
      "Epoch 177/1000\n",
      "\n",
      "Epoch 177: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 5.5149e-04 - accuracy: 0.9999 - val_loss: 0.0335 - val_accuracy: 0.9942 - 6s/epoch - 31ms/step\n",
      "Epoch 178/1000\n",
      "\n",
      "Epoch 178: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 9.1887e-04 - accuracy: 0.9997 - val_loss: 0.0370 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 179/1000\n",
      "\n",
      "Epoch 179: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 6.4027e-04 - accuracy: 0.9998 - val_loss: 0.0406 - val_accuracy: 0.9932 - 6s/epoch - 31ms/step\n",
      "Epoch 180/1000\n",
      "\n",
      "Epoch 180: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0405 - val_accuracy: 0.9933 - 6s/epoch - 31ms/step\n",
      "Epoch 181/1000\n",
      "\n",
      "Epoch 181: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0595 - val_accuracy: 0.9902 - 6s/epoch - 31ms/step\n",
      "Epoch 182/1000\n",
      "\n",
      "Epoch 182: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0405 - val_accuracy: 0.9928 - 6s/epoch - 31ms/step\n",
      "Epoch 183/1000\n",
      "\n",
      "Epoch 183: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.0326 - val_accuracy: 0.9940 - 6s/epoch - 31ms/step\n",
      "Epoch 184/1000\n",
      "\n",
      "Epoch 184: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.0480 - val_accuracy: 0.9921 - 6s/epoch - 31ms/step\n",
      "Epoch 185/1000\n",
      "\n",
      "Epoch 185: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0397 - val_accuracy: 0.9927 - 6s/epoch - 31ms/step\n",
      "Epoch 186/1000\n",
      "\n",
      "Epoch 186: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0375 - val_accuracy: 0.9935 - 6s/epoch - 31ms/step\n",
      "Epoch 187/1000\n",
      "\n",
      "Epoch 187: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.0500 - val_accuracy: 0.9902 - 6s/epoch - 31ms/step\n",
      "Epoch 188/1000\n",
      "\n",
      "Epoch 188: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0359 - val_accuracy: 0.9938 - 6s/epoch - 31ms/step\n",
      "Epoch 189/1000\n",
      "\n",
      "Epoch 189: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0367 - val_accuracy: 0.9933 - 6s/epoch - 31ms/step\n",
      "Epoch 190/1000\n",
      "\n",
      "Epoch 190: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 9.2572e-04 - accuracy: 0.9997 - val_loss: 0.0349 - val_accuracy: 0.9937 - 6s/epoch - 31ms/step\n",
      "Epoch 191/1000\n",
      "\n",
      "Epoch 191: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.0370 - val_accuracy: 0.9936 - 6s/epoch - 31ms/step\n",
      "Epoch 192/1000\n",
      "\n",
      "Epoch 192: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0417 - val_accuracy: 0.9929 - 6s/epoch - 32ms/step\n",
      "Epoch 193/1000\n",
      "\n",
      "Epoch 193: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0357 - val_accuracy: 0.9936 - 6s/epoch - 31ms/step\n",
      "Epoch 194/1000\n",
      "\n",
      "Epoch 194: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 8.2427e-04 - accuracy: 0.9997 - val_loss: 0.0315 - val_accuracy: 0.9942 - 6s/epoch - 31ms/step\n",
      "Epoch 195/1000\n",
      "\n",
      "Epoch 195: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 6.1840e-04 - accuracy: 0.9997 - val_loss: 0.0392 - val_accuracy: 0.9940 - 6s/epoch - 31ms/step\n",
      "Epoch 196/1000\n",
      "\n",
      "Epoch 196: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 5.9890e-04 - accuracy: 0.9998 - val_loss: 0.0404 - val_accuracy: 0.9930 - 6s/epoch - 31ms/step\n",
      "Epoch 197/1000\n",
      "\n",
      "Epoch 197: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 6.9971e-04 - accuracy: 0.9998 - val_loss: 0.0366 - val_accuracy: 0.9937 - 6s/epoch - 31ms/step\n",
      "Epoch 198/1000\n",
      "\n",
      "Epoch 198: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0445 - val_accuracy: 0.9932 - 6s/epoch - 31ms/step\n",
      "Epoch 199/1000\n",
      "\n",
      "Epoch 199: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0401 - val_accuracy: 0.9937 - 6s/epoch - 31ms/step\n",
      "Epoch 200/1000\n",
      "\n",
      "Epoch 200: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0010 - accuracy: 0.9996 - val_loss: 0.0371 - val_accuracy: 0.9934 - 6s/epoch - 31ms/step\n",
      "Epoch 201/1000\n",
      "\n",
      "Epoch 201: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 7.6488e-04 - accuracy: 0.9998 - val_loss: 0.0373 - val_accuracy: 0.9937 - 6s/epoch - 31ms/step\n",
      "Epoch 202/1000\n",
      "\n",
      "Epoch 202: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 8.3235e-04 - accuracy: 0.9998 - val_loss: 0.0402 - val_accuracy: 0.9930 - 6s/epoch - 31ms/step\n",
      "Epoch 203/1000\n",
      "\n",
      "Epoch 203: val_accuracy did not improve from 0.99440\n",
      "197/197 - 6s - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0337 - val_accuracy: 0.9940 - 6s/epoch - 31ms/step\n",
      "Epoch 204/1000\n",
      "\n",
      "Epoch 204: val_accuracy improved from 0.99440 to 0.99464, saving model to best_model.h5\n",
      "197/197 - 6s - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.0331 - val_accuracy: 0.9946 - 6s/epoch - 32ms/step\n",
      "Epoch 205/1000\n",
      "\n",
      "Epoch 205: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0333 - val_accuracy: 0.9941 - 6s/epoch - 32ms/step\n",
      "Epoch 206/1000\n",
      "\n",
      "Epoch 206: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0334 - val_accuracy: 0.9946 - 6s/epoch - 29ms/step\n",
      "Epoch 207/1000\n",
      "\n",
      "Epoch 207: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.0378 - val_accuracy: 0.9937 - 6s/epoch - 29ms/step\n",
      "Epoch 208/1000\n",
      "\n",
      "Epoch 208: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 3.6500e-04 - accuracy: 0.9999 - val_loss: 0.0365 - val_accuracy: 0.9936 - 6s/epoch - 31ms/step\n",
      "Epoch 209/1000\n",
      "\n",
      "Epoch 209: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0375 - val_accuracy: 0.9942 - 6s/epoch - 31ms/step\n",
      "Epoch 210/1000\n",
      "\n",
      "Epoch 210: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 9.3682e-04 - accuracy: 0.9996 - val_loss: 0.0393 - val_accuracy: 0.9937 - 6s/epoch - 31ms/step\n",
      "Epoch 211/1000\n",
      "\n",
      "Epoch 211: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0433 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 212/1000\n",
      "\n",
      "Epoch 212: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0454 - val_accuracy: 0.9933 - 6s/epoch - 31ms/step\n",
      "Epoch 213/1000\n",
      "\n",
      "Epoch 213: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 7.4063e-04 - accuracy: 0.9997 - val_loss: 0.0372 - val_accuracy: 0.9945 - 6s/epoch - 31ms/step\n",
      "Epoch 214/1000\n",
      "\n",
      "Epoch 214: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0358 - val_accuracy: 0.9944 - 6s/epoch - 31ms/step\n",
      "Epoch 215/1000\n",
      "\n",
      "Epoch 215: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0379 - val_accuracy: 0.9931 - 6s/epoch - 31ms/step\n",
      "Epoch 216/1000\n",
      "\n",
      "Epoch 216: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0385 - val_accuracy: 0.9937 - 6s/epoch - 31ms/step\n",
      "Epoch 217/1000\n",
      "\n",
      "Epoch 217: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.0364 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 218/1000\n",
      "\n",
      "Epoch 218: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 5.5353e-04 - accuracy: 0.9999 - val_loss: 0.0428 - val_accuracy: 0.9926 - 6s/epoch - 31ms/step\n",
      "Epoch 219/1000\n",
      "\n",
      "Epoch 219: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0393 - val_accuracy: 0.9936 - 6s/epoch - 31ms/step\n",
      "Epoch 220/1000\n",
      "\n",
      "Epoch 220: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 8.6147e-04 - accuracy: 0.9998 - val_loss: 0.0433 - val_accuracy: 0.9930 - 6s/epoch - 31ms/step\n",
      "Epoch 221/1000\n",
      "\n",
      "Epoch 221: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 3.9506e-04 - accuracy: 0.9999 - val_loss: 0.0369 - val_accuracy: 0.9935 - 6s/epoch - 31ms/step\n",
      "Epoch 222/1000\n",
      "\n",
      "Epoch 222: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 8.5653e-04 - accuracy: 0.9998 - val_loss: 0.0403 - val_accuracy: 0.9937 - 6s/epoch - 31ms/step\n",
      "Epoch 223/1000\n",
      "\n",
      "Epoch 223: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 3.9781e-04 - accuracy: 0.9999 - val_loss: 0.0418 - val_accuracy: 0.9933 - 6s/epoch - 31ms/step\n",
      "Epoch 224/1000\n",
      "\n",
      "Epoch 224: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0465 - val_accuracy: 0.9920 - 6s/epoch - 31ms/step\n",
      "Epoch 225/1000\n",
      "\n",
      "Epoch 225: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0534 - val_accuracy: 0.9923 - 6s/epoch - 31ms/step\n",
      "Epoch 226/1000\n",
      "\n",
      "Epoch 226: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0565 - val_accuracy: 0.9907 - 6s/epoch - 31ms/step\n",
      "Epoch 227/1000\n",
      "\n",
      "Epoch 227: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0412 - val_accuracy: 0.9935 - 6s/epoch - 31ms/step\n",
      "Epoch 228/1000\n",
      "\n",
      "Epoch 228: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.0408 - val_accuracy: 0.9942 - 6s/epoch - 31ms/step\n",
      "Epoch 229/1000\n",
      "\n",
      "Epoch 229: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 6.9473e-04 - accuracy: 0.9998 - val_loss: 0.0389 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 230/1000\n",
      "\n",
      "Epoch 230: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 7.8004e-04 - accuracy: 0.9998 - val_loss: 0.0372 - val_accuracy: 0.9942 - 6s/epoch - 31ms/step\n",
      "Epoch 231/1000\n",
      "\n",
      "Epoch 231: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 9.2919e-04 - accuracy: 0.9997 - val_loss: 0.0386 - val_accuracy: 0.9935 - 6s/epoch - 31ms/step\n",
      "Epoch 232/1000\n",
      "\n",
      "Epoch 232: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 6.5784e-04 - accuracy: 0.9997 - val_loss: 0.0389 - val_accuracy: 0.9936 - 6s/epoch - 31ms/step\n",
      "Epoch 233/1000\n",
      "\n",
      "Epoch 233: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 7.6276e-04 - accuracy: 0.9999 - val_loss: 0.0382 - val_accuracy: 0.9933 - 6s/epoch - 31ms/step\n",
      "Epoch 234/1000\n",
      "\n",
      "Epoch 234: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0426 - val_accuracy: 0.9929 - 6s/epoch - 31ms/step\n",
      "Epoch 235/1000\n",
      "\n",
      "Epoch 235: val_accuracy did not improve from 0.99464\n",
      "197/197 - 6s - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0400 - val_accuracy: 0.9938 - 6s/epoch - 31ms/step\n",
      "Epoch 236/1000\n",
      "\n",
      "Epoch 236: val_accuracy improved from 0.99464 to 0.99506, saving model to best_model.h5\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0336 - val_accuracy: 0.9951 - 6s/epoch - 31ms/step\n",
      "Epoch 237/1000\n",
      "\n",
      "Epoch 237: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.0389 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 238/1000\n",
      "\n",
      "Epoch 238: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 5.4259e-04 - accuracy: 0.9997 - val_loss: 0.0357 - val_accuracy: 0.9945 - 6s/epoch - 30ms/step\n",
      "Epoch 239/1000\n",
      "\n",
      "Epoch 239: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0379 - val_accuracy: 0.9944 - 6s/epoch - 29ms/step\n",
      "Epoch 240/1000\n",
      "\n",
      "Epoch 240: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 4.4852e-04 - accuracy: 0.9999 - val_loss: 0.0359 - val_accuracy: 0.9945 - 6s/epoch - 29ms/step\n",
      "Epoch 241/1000\n",
      "\n",
      "Epoch 241: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 7.4700e-04 - accuracy: 0.9998 - val_loss: 0.0417 - val_accuracy: 0.9935 - 6s/epoch - 29ms/step\n",
      "Epoch 242/1000\n",
      "\n",
      "Epoch 242: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 7.3135e-04 - accuracy: 0.9998 - val_loss: 0.0431 - val_accuracy: 0.9933 - 6s/epoch - 32ms/step\n",
      "Epoch 243/1000\n",
      "\n",
      "Epoch 243: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0406 - val_accuracy: 0.9938 - 6s/epoch - 30ms/step\n",
      "Epoch 244/1000\n",
      "\n",
      "Epoch 244: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 5.5358e-04 - accuracy: 0.9998 - val_loss: 0.0384 - val_accuracy: 0.9943 - 6s/epoch - 31ms/step\n",
      "Epoch 245/1000\n",
      "\n",
      "Epoch 245: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0435 - val_accuracy: 0.9940 - 6s/epoch - 31ms/step\n",
      "Epoch 246/1000\n",
      "\n",
      "Epoch 246: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0426 - val_accuracy: 0.9933 - 6s/epoch - 31ms/step\n",
      "Epoch 247/1000\n",
      "\n",
      "Epoch 247: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.0375 - val_accuracy: 0.9938 - 6s/epoch - 31ms/step\n",
      "Epoch 248/1000\n",
      "\n",
      "Epoch 248: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 1.4719e-04 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 0.9944 - 6s/epoch - 31ms/step\n",
      "Epoch 249/1000\n",
      "\n",
      "Epoch 249: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 6.2501e-04 - accuracy: 0.9998 - val_loss: 0.0359 - val_accuracy: 0.9946 - 6s/epoch - 31ms/step\n",
      "Epoch 250/1000\n",
      "\n",
      "Epoch 250: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 4.3677e-04 - accuracy: 0.9999 - val_loss: 0.0396 - val_accuracy: 0.9938 - 6s/epoch - 31ms/step\n",
      "Epoch 251/1000\n",
      "\n",
      "Epoch 251: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0399 - val_accuracy: 0.9938 - 6s/epoch - 31ms/step\n",
      "Epoch 252/1000\n",
      "\n",
      "Epoch 252: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 6.6423e-04 - accuracy: 0.9998 - val_loss: 0.0411 - val_accuracy: 0.9941 - 6s/epoch - 31ms/step\n",
      "Epoch 253/1000\n",
      "\n",
      "Epoch 253: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 4.8286e-04 - accuracy: 0.9998 - val_loss: 0.0432 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 254/1000\n",
      "\n",
      "Epoch 254: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 7.5416e-04 - accuracy: 0.9997 - val_loss: 0.0479 - val_accuracy: 0.9925 - 6s/epoch - 31ms/step\n",
      "Epoch 255/1000\n",
      "\n",
      "Epoch 255: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 3.7566e-04 - accuracy: 0.9999 - val_loss: 0.0423 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 256/1000\n",
      "\n",
      "Epoch 256: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 8.0545e-04 - accuracy: 0.9999 - val_loss: 0.0442 - val_accuracy: 0.9933 - 6s/epoch - 32ms/step\n",
      "Epoch 257/1000\n",
      "\n",
      "Epoch 257: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 8.3375e-04 - accuracy: 0.9997 - val_loss: 0.0431 - val_accuracy: 0.9942 - 6s/epoch - 31ms/step\n",
      "Epoch 258/1000\n",
      "\n",
      "Epoch 258: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0422 - val_accuracy: 0.9936 - 7s/epoch - 34ms/step\n",
      "Epoch 259/1000\n",
      "\n",
      "Epoch 259: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.0374 - val_accuracy: 0.9942 - 7s/epoch - 34ms/step\n",
      "Epoch 260/1000\n",
      "\n",
      "Epoch 260: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0337 - val_accuracy: 0.9943 - 7s/epoch - 34ms/step\n",
      "Epoch 261/1000\n",
      "\n",
      "Epoch 261: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 7.0199e-04 - accuracy: 0.9999 - val_loss: 0.0335 - val_accuracy: 0.9945 - 6s/epoch - 31ms/step\n",
      "Epoch 262/1000\n",
      "\n",
      "Epoch 262: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 2.8477e-04 - accuracy: 0.9999 - val_loss: 0.0387 - val_accuracy: 0.9940 - 6s/epoch - 31ms/step\n",
      "Epoch 263/1000\n",
      "\n",
      "Epoch 263: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 2.8773e-04 - accuracy: 0.9999 - val_loss: 0.0398 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 264/1000\n",
      "\n",
      "Epoch 264: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 6.3299e-04 - accuracy: 0.9998 - val_loss: 0.0442 - val_accuracy: 0.9935 - 6s/epoch - 31ms/step\n",
      "Epoch 265/1000\n",
      "\n",
      "Epoch 265: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 4.0861e-04 - accuracy: 0.9999 - val_loss: 0.0521 - val_accuracy: 0.9922 - 6s/epoch - 31ms/step\n",
      "Epoch 266/1000\n",
      "\n",
      "Epoch 266: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 9.1060e-04 - accuracy: 0.9997 - val_loss: 0.0419 - val_accuracy: 0.9942 - 7s/epoch - 36ms/step\n",
      "Epoch 267/1000\n",
      "\n",
      "Epoch 267: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 2.5342e-04 - accuracy: 0.9999 - val_loss: 0.0391 - val_accuracy: 0.9942 - 7s/epoch - 33ms/step\n",
      "Epoch 268/1000\n",
      "\n",
      "Epoch 268: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 7.7974e-04 - accuracy: 0.9997 - val_loss: 0.0410 - val_accuracy: 0.9925 - 7s/epoch - 34ms/step\n",
      "Epoch 269/1000\n",
      "\n",
      "Epoch 269: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 5.8748e-04 - accuracy: 0.9998 - val_loss: 0.0439 - val_accuracy: 0.9936 - 6s/epoch - 31ms/step\n",
      "Epoch 270/1000\n",
      "\n",
      "Epoch 270: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 4.1719e-04 - accuracy: 0.9999 - val_loss: 0.0440 - val_accuracy: 0.9940 - 6s/epoch - 32ms/step\n",
      "Epoch 271/1000\n",
      "\n",
      "Epoch 271: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0411 - val_accuracy: 0.9942 - 7s/epoch - 34ms/step\n",
      "Epoch 272/1000\n",
      "\n",
      "Epoch 272: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 5.3032e-04 - accuracy: 0.9999 - val_loss: 0.0400 - val_accuracy: 0.9940 - 6s/epoch - 32ms/step\n",
      "Epoch 273/1000\n",
      "\n",
      "Epoch 273: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 2.7092e-04 - accuracy: 0.9999 - val_loss: 0.0406 - val_accuracy: 0.9941 - 6s/epoch - 32ms/step\n",
      "Epoch 274/1000\n",
      "\n",
      "Epoch 274: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0414 - val_accuracy: 0.9937 - 6s/epoch - 32ms/step\n",
      "Epoch 275/1000\n",
      "\n",
      "Epoch 275: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0491 - val_accuracy: 0.9920 - 6s/epoch - 32ms/step\n",
      "Epoch 276/1000\n",
      "\n",
      "Epoch 276: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0411 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 277/1000\n",
      "\n",
      "Epoch 277: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 7.9455e-04 - accuracy: 0.9998 - val_loss: 0.0375 - val_accuracy: 0.9943 - 6s/epoch - 32ms/step\n",
      "Epoch 278/1000\n",
      "\n",
      "Epoch 278: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0522 - val_accuracy: 0.9912 - 6s/epoch - 32ms/step\n",
      "Epoch 279/1000\n",
      "\n",
      "Epoch 279: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0400 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 280/1000\n",
      "\n",
      "Epoch 280: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 8.4913e-04 - accuracy: 0.9998 - val_loss: 0.0386 - val_accuracy: 0.9934 - 6s/epoch - 32ms/step\n",
      "Epoch 281/1000\n",
      "\n",
      "Epoch 281: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 9.3688e-04 - accuracy: 0.9996 - val_loss: 0.0420 - val_accuracy: 0.9938 - 6s/epoch - 32ms/step\n",
      "Epoch 282/1000\n",
      "\n",
      "Epoch 282: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 7.7130e-04 - accuracy: 0.9997 - val_loss: 0.0407 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 283/1000\n",
      "\n",
      "Epoch 283: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0387 - val_accuracy: 0.9941 - 6s/epoch - 32ms/step\n",
      "Epoch 284/1000\n",
      "\n",
      "Epoch 284: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 5.3960e-04 - accuracy: 0.9998 - val_loss: 0.0368 - val_accuracy: 0.9946 - 6s/epoch - 32ms/step\n",
      "Epoch 285/1000\n",
      "\n",
      "Epoch 285: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 6.6934e-04 - accuracy: 0.9997 - val_loss: 0.0502 - val_accuracy: 0.9924 - 7s/epoch - 34ms/step\n",
      "Epoch 286/1000\n",
      "\n",
      "Epoch 286: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0466 - val_accuracy: 0.9920 - 6s/epoch - 32ms/step\n",
      "Epoch 287/1000\n",
      "\n",
      "Epoch 287: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0423 - val_accuracy: 0.9929 - 6s/epoch - 32ms/step\n",
      "Epoch 288/1000\n",
      "\n",
      "Epoch 288: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 6.3042e-04 - accuracy: 0.9998 - val_loss: 0.0426 - val_accuracy: 0.9932 - 6s/epoch - 32ms/step\n",
      "Epoch 289/1000\n",
      "\n",
      "Epoch 289: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 8.0519e-04 - accuracy: 0.9997 - val_loss: 0.0408 - val_accuracy: 0.9938 - 6s/epoch - 32ms/step\n",
      "Epoch 290/1000\n",
      "\n",
      "Epoch 290: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 5.5655e-04 - accuracy: 0.9998 - val_loss: 0.0402 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 291/1000\n",
      "\n",
      "Epoch 291: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 8.9792e-04 - accuracy: 0.9996 - val_loss: 0.0403 - val_accuracy: 0.9934 - 6s/epoch - 32ms/step\n",
      "Epoch 292/1000\n",
      "\n",
      "Epoch 292: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 4.2653e-04 - accuracy: 0.9998 - val_loss: 0.0379 - val_accuracy: 0.9946 - 7s/epoch - 34ms/step\n",
      "Epoch 293/1000\n",
      "\n",
      "Epoch 293: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 1.8633e-04 - accuracy: 1.0000 - val_loss: 0.0411 - val_accuracy: 0.9939 - 7s/epoch - 34ms/step\n",
      "Epoch 294/1000\n",
      "\n",
      "Epoch 294: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 9.6771e-04 - accuracy: 0.9997 - val_loss: 0.0423 - val_accuracy: 0.9942 - 7s/epoch - 37ms/step\n",
      "Epoch 295/1000\n",
      "\n",
      "Epoch 295: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0423 - val_accuracy: 0.9937 - 7s/epoch - 37ms/step\n",
      "Epoch 296/1000\n",
      "\n",
      "Epoch 296: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0384 - val_accuracy: 0.9943 - 7s/epoch - 36ms/step\n",
      "Epoch 297/1000\n",
      "\n",
      "Epoch 297: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 9.4753e-04 - accuracy: 0.9997 - val_loss: 0.0387 - val_accuracy: 0.9939 - 7s/epoch - 33ms/step\n",
      "Epoch 298/1000\n",
      "\n",
      "Epoch 298: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0432 - val_accuracy: 0.9936 - 7s/epoch - 37ms/step\n",
      "Epoch 299/1000\n",
      "\n",
      "Epoch 299: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 6.3920e-04 - accuracy: 0.9998 - val_loss: 0.0423 - val_accuracy: 0.9937 - 6s/epoch - 31ms/step\n",
      "Epoch 300/1000\n",
      "\n",
      "Epoch 300: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 7.1637e-04 - accuracy: 0.9996 - val_loss: 0.0431 - val_accuracy: 0.9938 - 6s/epoch - 31ms/step\n",
      "Epoch 301/1000\n",
      "\n",
      "Epoch 301: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0442 - val_accuracy: 0.9937 - 6s/epoch - 32ms/step\n",
      "Epoch 302/1000\n",
      "\n",
      "Epoch 302: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0441 - val_accuracy: 0.9934 - 7s/epoch - 35ms/step\n",
      "Epoch 303/1000\n",
      "\n",
      "Epoch 303: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 3.4381e-04 - accuracy: 0.9998 - val_loss: 0.0425 - val_accuracy: 0.9931 - 7s/epoch - 33ms/step\n",
      "Epoch 304/1000\n",
      "\n",
      "Epoch 304: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 2.1754e-04 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 0.9937 - 7s/epoch - 35ms/step\n",
      "Epoch 305/1000\n",
      "\n",
      "Epoch 305: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 2.2674e-04 - accuracy: 0.9999 - val_loss: 0.0419 - val_accuracy: 0.9935 - 7s/epoch - 35ms/step\n",
      "Epoch 306/1000\n",
      "\n",
      "Epoch 306: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0524 - val_accuracy: 0.9910 - 8s/epoch - 38ms/step\n",
      "Epoch 307/1000\n",
      "\n",
      "Epoch 307: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 6.6531e-04 - accuracy: 0.9998 - val_loss: 0.0411 - val_accuracy: 0.9940 - 7s/epoch - 37ms/step\n",
      "Epoch 308/1000\n",
      "\n",
      "Epoch 308: val_accuracy did not improve from 0.99506\n",
      "197/197 - 9s - loss: 3.2447e-04 - accuracy: 0.9999 - val_loss: 0.0390 - val_accuracy: 0.9941 - 9s/epoch - 43ms/step\n",
      "Epoch 309/1000\n",
      "\n",
      "Epoch 309: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.0429 - val_accuracy: 0.9935 - 8s/epoch - 38ms/step\n",
      "Epoch 310/1000\n",
      "\n",
      "Epoch 310: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0387 - val_accuracy: 0.9937 - 8s/epoch - 40ms/step\n",
      "Epoch 311/1000\n",
      "\n",
      "Epoch 311: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0388 - val_accuracy: 0.9939 - 7s/epoch - 37ms/step\n",
      "Epoch 312/1000\n",
      "\n",
      "Epoch 312: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 4.7679e-04 - accuracy: 0.9999 - val_loss: 0.0361 - val_accuracy: 0.9942 - 7s/epoch - 36ms/step\n",
      "Epoch 313/1000\n",
      "\n",
      "Epoch 313: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 1.0500e-04 - accuracy: 1.0000 - val_loss: 0.0364 - val_accuracy: 0.9944 - 8s/epoch - 38ms/step\n",
      "Epoch 314/1000\n",
      "\n",
      "Epoch 314: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0424 - val_accuracy: 0.9934 - 8s/epoch - 39ms/step\n",
      "Epoch 315/1000\n",
      "\n",
      "Epoch 315: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 4.7587e-04 - accuracy: 0.9998 - val_loss: 0.0412 - val_accuracy: 0.9933 - 7s/epoch - 36ms/step\n",
      "Epoch 316/1000\n",
      "\n",
      "Epoch 316: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0412 - val_accuracy: 0.9937 - 8s/epoch - 39ms/step\n",
      "Epoch 317/1000\n",
      "\n",
      "Epoch 317: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.0414 - val_accuracy: 0.9930 - 7s/epoch - 35ms/step\n",
      "Epoch 318/1000\n",
      "\n",
      "Epoch 318: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 9.2607e-04 - accuracy: 0.9997 - val_loss: 0.0471 - val_accuracy: 0.9926 - 8s/epoch - 39ms/step\n",
      "Epoch 319/1000\n",
      "\n",
      "Epoch 319: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0401 - val_accuracy: 0.9935 - 7s/epoch - 38ms/step\n",
      "Epoch 320/1000\n",
      "\n",
      "Epoch 320: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 5.9881e-04 - accuracy: 0.9998 - val_loss: 0.0414 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 321/1000\n",
      "\n",
      "Epoch 321: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 3.2118e-04 - accuracy: 0.9999 - val_loss: 0.0410 - val_accuracy: 0.9941 - 6s/epoch - 32ms/step\n",
      "Epoch 322/1000\n",
      "\n",
      "Epoch 322: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 9.5382e-04 - accuracy: 0.9997 - val_loss: 0.0450 - val_accuracy: 0.9929 - 7s/epoch - 34ms/step\n",
      "Epoch 323/1000\n",
      "\n",
      "Epoch 323: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 8.8096e-04 - accuracy: 0.9996 - val_loss: 0.0493 - val_accuracy: 0.9919 - 7s/epoch - 36ms/step\n",
      "Epoch 324/1000\n",
      "\n",
      "Epoch 324: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.0405 - val_accuracy: 0.9940 - 7s/epoch - 36ms/step\n",
      "Epoch 325/1000\n",
      "\n",
      "Epoch 325: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 8.2033e-04 - accuracy: 0.9998 - val_loss: 0.0403 - val_accuracy: 0.9943 - 6s/epoch - 31ms/step\n",
      "Epoch 326/1000\n",
      "\n",
      "Epoch 326: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 7.2855e-04 - accuracy: 0.9997 - val_loss: 0.0423 - val_accuracy: 0.9936 - 6s/epoch - 31ms/step\n",
      "Epoch 327/1000\n",
      "\n",
      "Epoch 327: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0432 - val_accuracy: 0.9932 - 6s/epoch - 31ms/step\n",
      "Epoch 328/1000\n",
      "\n",
      "Epoch 328: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 3.7598e-04 - accuracy: 0.9999 - val_loss: 0.0425 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 329/1000\n",
      "\n",
      "Epoch 329: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 3.9918e-04 - accuracy: 0.9999 - val_loss: 0.0429 - val_accuracy: 0.9934 - 7s/epoch - 35ms/step\n",
      "Epoch 330/1000\n",
      "\n",
      "Epoch 330: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 2.7944e-04 - accuracy: 0.9999 - val_loss: 0.0419 - val_accuracy: 0.9931 - 6s/epoch - 33ms/step\n",
      "Epoch 331/1000\n",
      "\n",
      "Epoch 331: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 7.6256e-04 - accuracy: 0.9998 - val_loss: 0.0400 - val_accuracy: 0.9938 - 7s/epoch - 34ms/step\n",
      "Epoch 332/1000\n",
      "\n",
      "Epoch 332: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 9.5540e-05 - accuracy: 1.0000 - val_loss: 0.0409 - val_accuracy: 0.9939 - 7s/epoch - 35ms/step\n",
      "Epoch 333/1000\n",
      "\n",
      "Epoch 333: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 2.2007e-04 - accuracy: 1.0000 - val_loss: 0.0418 - val_accuracy: 0.9937 - 6s/epoch - 33ms/step\n",
      "Epoch 334/1000\n",
      "\n",
      "Epoch 334: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 9.7106e-05 - accuracy: 1.0000 - val_loss: 0.0414 - val_accuracy: 0.9937 - 7s/epoch - 34ms/step\n",
      "Epoch 335/1000\n",
      "\n",
      "Epoch 335: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 7.8452e-04 - accuracy: 0.9998 - val_loss: 0.0420 - val_accuracy: 0.9929 - 6s/epoch - 32ms/step\n",
      "Epoch 336/1000\n",
      "\n",
      "Epoch 336: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 1.3281e-04 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 337/1000\n",
      "\n",
      "Epoch 337: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 2.5278e-04 - accuracy: 0.9999 - val_loss: 0.0385 - val_accuracy: 0.9937 - 7s/epoch - 35ms/step\n",
      "Epoch 338/1000\n",
      "\n",
      "Epoch 338: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 1.1003e-04 - accuracy: 1.0000 - val_loss: 0.0398 - val_accuracy: 0.9941 - 7s/epoch - 33ms/step\n",
      "Epoch 339/1000\n",
      "\n",
      "Epoch 339: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 2.3022e-04 - accuracy: 1.0000 - val_loss: 0.0461 - val_accuracy: 0.9931 - 7s/epoch - 33ms/step\n",
      "Epoch 340/1000\n",
      "\n",
      "Epoch 340: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 8.2533e-04 - accuracy: 0.9999 - val_loss: 0.0488 - val_accuracy: 0.9932 - 7s/epoch - 33ms/step\n",
      "Epoch 341/1000\n",
      "\n",
      "Epoch 341: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0495 - val_accuracy: 0.9932 - 8s/epoch - 40ms/step\n",
      "Epoch 342/1000\n",
      "\n",
      "Epoch 342: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 7.8799e-04 - accuracy: 0.9998 - val_loss: 0.0487 - val_accuracy: 0.9935 - 8s/epoch - 41ms/step\n",
      "Epoch 343/1000\n",
      "\n",
      "Epoch 343: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.0424 - val_accuracy: 0.9935 - 8s/epoch - 39ms/step\n",
      "Epoch 344/1000\n",
      "\n",
      "Epoch 344: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 5.4717e-04 - accuracy: 0.9998 - val_loss: 0.0465 - val_accuracy: 0.9935 - 8s/epoch - 41ms/step\n",
      "Epoch 345/1000\n",
      "\n",
      "Epoch 345: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0484 - val_accuracy: 0.9927 - 7s/epoch - 35ms/step\n",
      "Epoch 346/1000\n",
      "\n",
      "Epoch 346: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0488 - val_accuracy: 0.9923 - 7s/epoch - 35ms/step\n",
      "Epoch 347/1000\n",
      "\n",
      "Epoch 347: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 5.1921e-04 - accuracy: 0.9999 - val_loss: 0.0419 - val_accuracy: 0.9937 - 7s/epoch - 35ms/step\n",
      "Epoch 348/1000\n",
      "\n",
      "Epoch 348: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 2.7786e-04 - accuracy: 0.9999 - val_loss: 0.0412 - val_accuracy: 0.9939 - 7s/epoch - 36ms/step\n",
      "Epoch 349/1000\n",
      "\n",
      "Epoch 349: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0466 - val_accuracy: 0.9930 - 8s/epoch - 40ms/step\n",
      "Epoch 350/1000\n",
      "\n",
      "Epoch 350: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 7.8295e-04 - accuracy: 0.9999 - val_loss: 0.0452 - val_accuracy: 0.9930 - 7s/epoch - 36ms/step\n",
      "Epoch 351/1000\n",
      "\n",
      "Epoch 351: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 3.8228e-04 - accuracy: 0.9999 - val_loss: 0.0411 - val_accuracy: 0.9936 - 7s/epoch - 35ms/step\n",
      "Epoch 352/1000\n",
      "\n",
      "Epoch 352: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 5.7844e-04 - accuracy: 0.9997 - val_loss: 0.0407 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 353/1000\n",
      "\n",
      "Epoch 353: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 6.1037e-04 - accuracy: 0.9998 - val_loss: 0.0381 - val_accuracy: 0.9943 - 7s/epoch - 34ms/step\n",
      "Epoch 354/1000\n",
      "\n",
      "Epoch 354: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 5.2794e-04 - accuracy: 0.9998 - val_loss: 0.0452 - val_accuracy: 0.9935 - 7s/epoch - 33ms/step\n",
      "Epoch 355/1000\n",
      "\n",
      "Epoch 355: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 5.8214e-04 - accuracy: 0.9997 - val_loss: 0.0422 - val_accuracy: 0.9936 - 7s/epoch - 33ms/step\n",
      "Epoch 356/1000\n",
      "\n",
      "Epoch 356: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 4.2734e-04 - accuracy: 0.9998 - val_loss: 0.0430 - val_accuracy: 0.9936 - 7s/epoch - 33ms/step\n",
      "Epoch 357/1000\n",
      "\n",
      "Epoch 357: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 8.0438e-05 - accuracy: 1.0000 - val_loss: 0.0431 - val_accuracy: 0.9937 - 7s/epoch - 33ms/step\n",
      "Epoch 358/1000\n",
      "\n",
      "Epoch 358: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 1.2169e-04 - accuracy: 1.0000 - val_loss: 0.0464 - val_accuracy: 0.9935 - 7s/epoch - 33ms/step\n",
      "Epoch 359/1000\n",
      "\n",
      "Epoch 359: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 1.9166e-04 - accuracy: 0.9999 - val_loss: 0.0486 - val_accuracy: 0.9930 - 7s/epoch - 33ms/step\n",
      "Epoch 360/1000\n",
      "\n",
      "Epoch 360: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0471 - val_accuracy: 0.9930 - 7s/epoch - 33ms/step\n",
      "Epoch 361/1000\n",
      "\n",
      "Epoch 361: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0440 - val_accuracy: 0.9938 - 7s/epoch - 35ms/step\n",
      "Epoch 362/1000\n",
      "\n",
      "Epoch 362: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 5.5676e-04 - accuracy: 0.9998 - val_loss: 0.0429 - val_accuracy: 0.9936 - 7s/epoch - 35ms/step\n",
      "Epoch 363/1000\n",
      "\n",
      "Epoch 363: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0539 - val_accuracy: 0.9924 - 7s/epoch - 38ms/step\n",
      "Epoch 364/1000\n",
      "\n",
      "Epoch 364: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.0434 - val_accuracy: 0.9934 - 7s/epoch - 37ms/step\n",
      "Epoch 365/1000\n",
      "\n",
      "Epoch 365: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 3.1362e-04 - accuracy: 0.9999 - val_loss: 0.0411 - val_accuracy: 0.9937 - 6s/epoch - 31ms/step\n",
      "Epoch 366/1000\n",
      "\n",
      "Epoch 366: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 9.8607e-04 - accuracy: 0.9997 - val_loss: 0.0398 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 367/1000\n",
      "\n",
      "Epoch 367: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 9.4736e-04 - accuracy: 0.9997 - val_loss: 0.0390 - val_accuracy: 0.9945 - 7s/epoch - 34ms/step\n",
      "Epoch 368/1000\n",
      "\n",
      "Epoch 368: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 6.3934e-04 - accuracy: 0.9998 - val_loss: 0.0387 - val_accuracy: 0.9942 - 7s/epoch - 36ms/step\n",
      "Epoch 369/1000\n",
      "\n",
      "Epoch 369: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 4.6630e-05 - accuracy: 1.0000 - val_loss: 0.0380 - val_accuracy: 0.9947 - 6s/epoch - 33ms/step\n",
      "Epoch 370/1000\n",
      "\n",
      "Epoch 370: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 6.2746e-04 - accuracy: 0.9997 - val_loss: 0.0440 - val_accuracy: 0.9933 - 7s/epoch - 33ms/step\n",
      "Epoch 371/1000\n",
      "\n",
      "Epoch 371: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0418 - val_accuracy: 0.9937 - 7s/epoch - 34ms/step\n",
      "Epoch 372/1000\n",
      "\n",
      "Epoch 372: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 3.7951e-04 - accuracy: 0.9998 - val_loss: 0.0368 - val_accuracy: 0.9943 - 6s/epoch - 32ms/step\n",
      "Epoch 373/1000\n",
      "\n",
      "Epoch 373: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 3.4167e-04 - accuracy: 0.9999 - val_loss: 0.0419 - val_accuracy: 0.9937 - 7s/epoch - 34ms/step\n",
      "Epoch 374/1000\n",
      "\n",
      "Epoch 374: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 2.1790e-04 - accuracy: 0.9999 - val_loss: 0.0401 - val_accuracy: 0.9939 - 7s/epoch - 37ms/step\n",
      "Epoch 375/1000\n",
      "\n",
      "Epoch 375: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 5.2550e-04 - accuracy: 0.9999 - val_loss: 0.0372 - val_accuracy: 0.9944 - 7s/epoch - 33ms/step\n",
      "Epoch 376/1000\n",
      "\n",
      "Epoch 376: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0431 - val_accuracy: 0.9935 - 7s/epoch - 37ms/step\n",
      "Epoch 377/1000\n",
      "\n",
      "Epoch 377: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0408 - val_accuracy: 0.9937 - 7s/epoch - 34ms/step\n",
      "Epoch 378/1000\n",
      "\n",
      "Epoch 378: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0432 - val_accuracy: 0.9936 - 6s/epoch - 33ms/step\n",
      "Epoch 379/1000\n",
      "\n",
      "Epoch 379: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 8.9844e-04 - accuracy: 0.9998 - val_loss: 0.0377 - val_accuracy: 0.9942 - 7s/epoch - 33ms/step\n",
      "Epoch 380/1000\n",
      "\n",
      "Epoch 380: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 8.3537e-04 - accuracy: 0.9998 - val_loss: 0.0386 - val_accuracy: 0.9942 - 7s/epoch - 34ms/step\n",
      "Epoch 381/1000\n",
      "\n",
      "Epoch 381: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0362 - val_accuracy: 0.9942 - 7s/epoch - 36ms/step\n",
      "Epoch 382/1000\n",
      "\n",
      "Epoch 382: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0403 - val_accuracy: 0.9934 - 8s/epoch - 39ms/step\n",
      "Epoch 383/1000\n",
      "\n",
      "Epoch 383: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 9.0030e-04 - accuracy: 0.9997 - val_loss: 0.0416 - val_accuracy: 0.9941 - 7s/epoch - 38ms/step\n",
      "Epoch 384/1000\n",
      "\n",
      "Epoch 384: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 9.0860e-04 - accuracy: 0.9998 - val_loss: 0.0348 - val_accuracy: 0.9949 - 7s/epoch - 35ms/step\n",
      "Epoch 385/1000\n",
      "\n",
      "Epoch 385: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 4.9187e-04 - accuracy: 0.9999 - val_loss: 0.0364 - val_accuracy: 0.9940 - 7s/epoch - 34ms/step\n",
      "Epoch 386/1000\n",
      "\n",
      "Epoch 386: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 1.2450e-04 - accuracy: 1.0000 - val_loss: 0.0390 - val_accuracy: 0.9942 - 7s/epoch - 37ms/step\n",
      "Epoch 387/1000\n",
      "\n",
      "Epoch 387: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 9.2844e-04 - accuracy: 0.9997 - val_loss: 0.0362 - val_accuracy: 0.9946 - 7s/epoch - 36ms/step\n",
      "Epoch 388/1000\n",
      "\n",
      "Epoch 388: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 8.1291e-04 - accuracy: 0.9998 - val_loss: 0.0367 - val_accuracy: 0.9942 - 7s/epoch - 36ms/step\n",
      "Epoch 389/1000\n",
      "\n",
      "Epoch 389: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0367 - val_accuracy: 0.9942 - 7s/epoch - 34ms/step\n",
      "Epoch 390/1000\n",
      "\n",
      "Epoch 390: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 6.7945e-04 - accuracy: 0.9997 - val_loss: 0.0390 - val_accuracy: 0.9939 - 8s/epoch - 39ms/step\n",
      "Epoch 391/1000\n",
      "\n",
      "Epoch 391: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 7.8185e-04 - accuracy: 0.9998 - val_loss: 0.0398 - val_accuracy: 0.9933 - 6s/epoch - 32ms/step\n",
      "Epoch 392/1000\n",
      "\n",
      "Epoch 392: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 4.3629e-04 - accuracy: 0.9999 - val_loss: 0.0411 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 393/1000\n",
      "\n",
      "Epoch 393: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 4.1973e-04 - accuracy: 0.9999 - val_loss: 0.0386 - val_accuracy: 0.9939 - 7s/epoch - 37ms/step\n",
      "Epoch 394/1000\n",
      "\n",
      "Epoch 394: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 5.4545e-04 - accuracy: 0.9998 - val_loss: 0.0401 - val_accuracy: 0.9931 - 7s/epoch - 37ms/step\n",
      "Epoch 395/1000\n",
      "\n",
      "Epoch 395: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 3.3623e-04 - accuracy: 0.9999 - val_loss: 0.0411 - val_accuracy: 0.9938 - 7s/epoch - 35ms/step\n",
      "Epoch 396/1000\n",
      "\n",
      "Epoch 396: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 4.0690e-04 - accuracy: 0.9999 - val_loss: 0.0390 - val_accuracy: 0.9941 - 7s/epoch - 33ms/step\n",
      "Epoch 397/1000\n",
      "\n",
      "Epoch 397: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0386 - val_accuracy: 0.9939 - 6s/epoch - 33ms/step\n",
      "Epoch 398/1000\n",
      "\n",
      "Epoch 398: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.0467 - val_accuracy: 0.9930 - 7s/epoch - 33ms/step\n",
      "Epoch 399/1000\n",
      "\n",
      "Epoch 399: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0416 - val_accuracy: 0.9932 - 7s/epoch - 33ms/step\n",
      "Epoch 400/1000\n",
      "\n",
      "Epoch 400: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 5.0708e-04 - accuracy: 0.9998 - val_loss: 0.0401 - val_accuracy: 0.9933 - 7s/epoch - 35ms/step\n",
      "Epoch 401/1000\n",
      "\n",
      "Epoch 401: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0403 - val_accuracy: 0.9932 - 7s/epoch - 35ms/step\n",
      "Epoch 402/1000\n",
      "\n",
      "Epoch 402: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 2.2916e-04 - accuracy: 0.9999 - val_loss: 0.0402 - val_accuracy: 0.9940 - 7s/epoch - 34ms/step\n",
      "Epoch 403/1000\n",
      "\n",
      "Epoch 403: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 2.7648e-04 - accuracy: 0.9999 - val_loss: 0.0418 - val_accuracy: 0.9942 - 6s/epoch - 33ms/step\n",
      "Epoch 404/1000\n",
      "\n",
      "Epoch 404: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 1.6770e-04 - accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 0.9934 - 7s/epoch - 38ms/step\n",
      "Epoch 405/1000\n",
      "\n",
      "Epoch 405: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 1.4522e-04 - accuracy: 1.0000 - val_loss: 0.0450 - val_accuracy: 0.9933 - 7s/epoch - 38ms/step\n",
      "Epoch 406/1000\n",
      "\n",
      "Epoch 406: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 3.5680e-04 - accuracy: 0.9999 - val_loss: 0.0424 - val_accuracy: 0.9936 - 7s/epoch - 35ms/step\n",
      "Epoch 407/1000\n",
      "\n",
      "Epoch 407: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 1.0964e-04 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 0.9935 - 6s/epoch - 31ms/step\n",
      "Epoch 408/1000\n",
      "\n",
      "Epoch 408: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 6.8739e-04 - accuracy: 0.9998 - val_loss: 0.0411 - val_accuracy: 0.9935 - 7s/epoch - 36ms/step\n",
      "Epoch 409/1000\n",
      "\n",
      "Epoch 409: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 5.6287e-04 - accuracy: 0.9998 - val_loss: 0.0449 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 410/1000\n",
      "\n",
      "Epoch 410: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 1.3248e-04 - accuracy: 1.0000 - val_loss: 0.0417 - val_accuracy: 0.9942 - 6s/epoch - 31ms/step\n",
      "Epoch 411/1000\n",
      "\n",
      "Epoch 411: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 4.8761e-04 - accuracy: 0.9998 - val_loss: 0.0416 - val_accuracy: 0.9940 - 6s/epoch - 31ms/step\n",
      "Epoch 412/1000\n",
      "\n",
      "Epoch 412: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 2.6284e-04 - accuracy: 0.9999 - val_loss: 0.0437 - val_accuracy: 0.9936 - 6s/epoch - 33ms/step\n",
      "Epoch 413/1000\n",
      "\n",
      "Epoch 413: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 5.3118e-04 - accuracy: 0.9998 - val_loss: 0.0544 - val_accuracy: 0.9919 - 6s/epoch - 32ms/step\n",
      "Epoch 414/1000\n",
      "\n",
      "Epoch 414: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 7.7035e-04 - accuracy: 0.9997 - val_loss: 0.0441 - val_accuracy: 0.9936 - 7s/epoch - 35ms/step\n",
      "Epoch 415/1000\n",
      "\n",
      "Epoch 415: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 7.9770e-04 - accuracy: 0.9998 - val_loss: 0.0429 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 416/1000\n",
      "\n",
      "Epoch 416: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 4.3014e-04 - accuracy: 1.0000 - val_loss: 0.0424 - val_accuracy: 0.9941 - 6s/epoch - 32ms/step\n",
      "Epoch 417/1000\n",
      "\n",
      "Epoch 417: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 5.2446e-04 - accuracy: 0.9997 - val_loss: 0.0483 - val_accuracy: 0.9931 - 6s/epoch - 32ms/step\n",
      "Epoch 418/1000\n",
      "\n",
      "Epoch 418: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 8.0496e-04 - accuracy: 0.9999 - val_loss: 0.0450 - val_accuracy: 0.9933 - 6s/epoch - 32ms/step\n",
      "Epoch 419/1000\n",
      "\n",
      "Epoch 419: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 2.6626e-04 - accuracy: 0.9999 - val_loss: 0.0388 - val_accuracy: 0.9938 - 6s/epoch - 32ms/step\n",
      "Epoch 420/1000\n",
      "\n",
      "Epoch 420: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 6.5655e-04 - accuracy: 0.9998 - val_loss: 0.0442 - val_accuracy: 0.9929 - 6s/epoch - 32ms/step\n",
      "Epoch 421/1000\n",
      "\n",
      "Epoch 421: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 6.4833e-04 - accuracy: 0.9998 - val_loss: 0.0430 - val_accuracy: 0.9938 - 7s/epoch - 35ms/step\n",
      "Epoch 422/1000\n",
      "\n",
      "Epoch 422: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0466 - val_accuracy: 0.9933 - 7s/epoch - 34ms/step\n",
      "Epoch 423/1000\n",
      "\n",
      "Epoch 423: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 4.8418e-04 - accuracy: 0.9998 - val_loss: 0.0456 - val_accuracy: 0.9931 - 8s/epoch - 40ms/step\n",
      "Epoch 424/1000\n",
      "\n",
      "Epoch 424: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 6.1627e-05 - accuracy: 1.0000 - val_loss: 0.0437 - val_accuracy: 0.9934 - 7s/epoch - 37ms/step\n",
      "Epoch 425/1000\n",
      "\n",
      "Epoch 425: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 1.5479e-04 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 0.9939 - 7s/epoch - 33ms/step\n",
      "Epoch 426/1000\n",
      "\n",
      "Epoch 426: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 6.1638e-05 - accuracy: 1.0000 - val_loss: 0.0406 - val_accuracy: 0.9937 - 6s/epoch - 32ms/step\n",
      "Epoch 427/1000\n",
      "\n",
      "Epoch 427: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 6.5484e-04 - accuracy: 0.9998 - val_loss: 0.0478 - val_accuracy: 0.9932 - 6s/epoch - 32ms/step\n",
      "Epoch 428/1000\n",
      "\n",
      "Epoch 428: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 9.2443e-04 - accuracy: 0.9998 - val_loss: 0.0426 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 429/1000\n",
      "\n",
      "Epoch 429: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 3.6500e-04 - accuracy: 0.9999 - val_loss: 0.0429 - val_accuracy: 0.9946 - 6s/epoch - 32ms/step\n",
      "Epoch 430/1000\n",
      "\n",
      "Epoch 430: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 2.7216e-04 - accuracy: 0.9998 - val_loss: 0.0471 - val_accuracy: 0.9935 - 6s/epoch - 29ms/step\n",
      "Epoch 431/1000\n",
      "\n",
      "Epoch 431: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 3.4327e-04 - accuracy: 0.9999 - val_loss: 0.0453 - val_accuracy: 0.9945 - 6s/epoch - 32ms/step\n",
      "Epoch 432/1000\n",
      "\n",
      "Epoch 432: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 4.0129e-04 - accuracy: 0.9999 - val_loss: 0.0449 - val_accuracy: 0.9937 - 6s/epoch - 32ms/step\n",
      "Epoch 433/1000\n",
      "\n",
      "Epoch 433: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0425 - val_accuracy: 0.9940 - 6s/epoch - 31ms/step\n",
      "Epoch 434/1000\n",
      "\n",
      "Epoch 434: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0380 - val_accuracy: 0.9944 - 6s/epoch - 31ms/step\n",
      "Epoch 435/1000\n",
      "\n",
      "Epoch 435: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 5.2469e-05 - accuracy: 1.0000 - val_loss: 0.0394 - val_accuracy: 0.9940 - 6s/epoch - 31ms/step\n",
      "Epoch 436/1000\n",
      "\n",
      "Epoch 436: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 1.9943e-04 - accuracy: 0.9999 - val_loss: 0.0398 - val_accuracy: 0.9942 - 6s/epoch - 31ms/step\n",
      "Epoch 437/1000\n",
      "\n",
      "Epoch 437: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 9.3307e-04 - accuracy: 0.9996 - val_loss: 0.0393 - val_accuracy: 0.9944 - 6s/epoch - 32ms/step\n",
      "Epoch 438/1000\n",
      "\n",
      "Epoch 438: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 7.5831e-04 - accuracy: 0.9998 - val_loss: 0.0410 - val_accuracy: 0.9946 - 6s/epoch - 32ms/step\n",
      "Epoch 439/1000\n",
      "\n",
      "Epoch 439: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 1.1784e-04 - accuracy: 1.0000 - val_loss: 0.0399 - val_accuracy: 0.9949 - 6s/epoch - 32ms/step\n",
      "Epoch 440/1000\n",
      "\n",
      "Epoch 440: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 4.4604e-04 - accuracy: 0.9999 - val_loss: 0.0428 - val_accuracy: 0.9943 - 6s/epoch - 32ms/step\n",
      "Epoch 441/1000\n",
      "\n",
      "Epoch 441: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 1.9746e-04 - accuracy: 0.9999 - val_loss: 0.0447 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 442/1000\n",
      "\n",
      "Epoch 442: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0447 - val_accuracy: 0.9937 - 6s/epoch - 32ms/step\n",
      "Epoch 443/1000\n",
      "\n",
      "Epoch 443: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 3.3134e-04 - accuracy: 0.9999 - val_loss: 0.0409 - val_accuracy: 0.9943 - 6s/epoch - 32ms/step\n",
      "Epoch 444/1000\n",
      "\n",
      "Epoch 444: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 3.2460e-04 - accuracy: 0.9999 - val_loss: 0.0504 - val_accuracy: 0.9929 - 6s/epoch - 32ms/step\n",
      "Epoch 445/1000\n",
      "\n",
      "Epoch 445: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 8.0003e-04 - accuracy: 0.9998 - val_loss: 0.0395 - val_accuracy: 0.9940 - 6s/epoch - 32ms/step\n",
      "Epoch 446/1000\n",
      "\n",
      "Epoch 446: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 1.0657e-04 - accuracy: 1.0000 - val_loss: 0.0408 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 447/1000\n",
      "\n",
      "Epoch 447: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 7.7079e-04 - accuracy: 0.9998 - val_loss: 0.0438 - val_accuracy: 0.9933 - 7s/epoch - 36ms/step\n",
      "Epoch 448/1000\n",
      "\n",
      "Epoch 448: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 4.5508e-04 - accuracy: 0.9998 - val_loss: 0.0405 - val_accuracy: 0.9944 - 7s/epoch - 37ms/step\n",
      "Epoch 449/1000\n",
      "\n",
      "Epoch 449: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 1.5046e-04 - accuracy: 1.0000 - val_loss: 0.0432 - val_accuracy: 0.9936 - 8s/epoch - 38ms/step\n",
      "Epoch 450/1000\n",
      "\n",
      "Epoch 450: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 7.4820e-05 - accuracy: 1.0000 - val_loss: 0.0426 - val_accuracy: 0.9939 - 7s/epoch - 36ms/step\n",
      "Epoch 451/1000\n",
      "\n",
      "Epoch 451: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 6.9921e-04 - accuracy: 0.9997 - val_loss: 0.0429 - val_accuracy: 0.9940 - 6s/epoch - 33ms/step\n",
      "Epoch 452/1000\n",
      "\n",
      "Epoch 452: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.0458 - val_accuracy: 0.9933 - 6s/epoch - 31ms/step\n",
      "Epoch 453/1000\n",
      "\n",
      "Epoch 453: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 3.3535e-04 - accuracy: 0.9999 - val_loss: 0.0452 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 454/1000\n",
      "\n",
      "Epoch 454: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0417 - val_accuracy: 0.9932 - 6s/epoch - 31ms/step\n",
      "Epoch 455/1000\n",
      "\n",
      "Epoch 455: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 3.3542e-04 - accuracy: 0.9999 - val_loss: 0.0416 - val_accuracy: 0.9940 - 7s/epoch - 33ms/step\n",
      "Epoch 456/1000\n",
      "\n",
      "Epoch 456: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 5.9164e-04 - accuracy: 0.9998 - val_loss: 0.0414 - val_accuracy: 0.9943 - 6s/epoch - 33ms/step\n",
      "Epoch 457/1000\n",
      "\n",
      "Epoch 457: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.0446 - val_accuracy: 0.9938 - 6s/epoch - 32ms/step\n",
      "Epoch 458/1000\n",
      "\n",
      "Epoch 458: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 2.2733e-04 - accuracy: 0.9999 - val_loss: 0.0456 - val_accuracy: 0.9934 - 7s/epoch - 34ms/step\n",
      "Epoch 459/1000\n",
      "\n",
      "Epoch 459: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 6.5433e-04 - accuracy: 0.9998 - val_loss: 0.0429 - val_accuracy: 0.9937 - 7s/epoch - 34ms/step\n",
      "Epoch 460/1000\n",
      "\n",
      "Epoch 460: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 4.5492e-04 - accuracy: 0.9998 - val_loss: 0.0409 - val_accuracy: 0.9942 - 8s/epoch - 39ms/step\n",
      "Epoch 461/1000\n",
      "\n",
      "Epoch 461: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 3.4758e-04 - accuracy: 0.9999 - val_loss: 0.0426 - val_accuracy: 0.9940 - 8s/epoch - 41ms/step\n",
      "Epoch 462/1000\n",
      "\n",
      "Epoch 462: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 4.5978e-04 - accuracy: 0.9998 - val_loss: 0.0375 - val_accuracy: 0.9947 - 8s/epoch - 38ms/step\n",
      "Epoch 463/1000\n",
      "\n",
      "Epoch 463: val_accuracy did not improve from 0.99506\n",
      "197/197 - 8s - loss: 8.8727e-04 - accuracy: 0.9997 - val_loss: 0.0412 - val_accuracy: 0.9940 - 8s/epoch - 41ms/step\n",
      "Epoch 464/1000\n",
      "\n",
      "Epoch 464: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 4.9330e-04 - accuracy: 0.9999 - val_loss: 0.0378 - val_accuracy: 0.9948 - 6s/epoch - 33ms/step\n",
      "Epoch 465/1000\n",
      "\n",
      "Epoch 465: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 6.6988e-05 - accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 0.9945 - 6s/epoch - 32ms/step\n",
      "Epoch 466/1000\n",
      "\n",
      "Epoch 466: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 5.4150e-04 - accuracy: 0.9998 - val_loss: 0.0431 - val_accuracy: 0.9943 - 7s/epoch - 37ms/step\n",
      "Epoch 467/1000\n",
      "\n",
      "Epoch 467: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 1.3955e-04 - accuracy: 1.0000 - val_loss: 0.0401 - val_accuracy: 0.9944 - 7s/epoch - 33ms/step\n",
      "Epoch 468/1000\n",
      "\n",
      "Epoch 468: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 2.1594e-04 - accuracy: 0.9999 - val_loss: 0.0420 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 469/1000\n",
      "\n",
      "Epoch 469: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 3.1699e-04 - accuracy: 0.9999 - val_loss: 0.0405 - val_accuracy: 0.9948 - 7s/epoch - 34ms/step\n",
      "Epoch 470/1000\n",
      "\n",
      "Epoch 470: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 2.8392e-04 - accuracy: 0.9998 - val_loss: 0.0400 - val_accuracy: 0.9945 - 6s/epoch - 33ms/step\n",
      "Epoch 471/1000\n",
      "\n",
      "Epoch 471: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 4.0234e-05 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 0.9949 - 7s/epoch - 36ms/step\n",
      "Epoch 472/1000\n",
      "\n",
      "Epoch 472: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 5.7892e-04 - accuracy: 0.9998 - val_loss: 0.0465 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 473/1000\n",
      "\n",
      "Epoch 473: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 3.0106e-04 - accuracy: 0.9999 - val_loss: 0.0414 - val_accuracy: 0.9948 - 6s/epoch - 30ms/step\n",
      "Epoch 474/1000\n",
      "\n",
      "Epoch 474: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 4.2169e-04 - accuracy: 0.9998 - val_loss: 0.0457 - val_accuracy: 0.9941 - 6s/epoch - 32ms/step\n",
      "Epoch 475/1000\n",
      "\n",
      "Epoch 475: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 6.8244e-04 - accuracy: 0.9998 - val_loss: 0.0419 - val_accuracy: 0.9946 - 7s/epoch - 34ms/step\n",
      "Epoch 476/1000\n",
      "\n",
      "Epoch 476: val_accuracy did not improve from 0.99506\n",
      "197/197 - 7s - loss: 4.7516e-04 - accuracy: 0.9998 - val_loss: 0.0433 - val_accuracy: 0.9940 - 7s/epoch - 36ms/step\n",
      "Epoch 477/1000\n",
      "\n",
      "Epoch 477: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.0494 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 478/1000\n",
      "\n",
      "Epoch 478: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 9.3626e-04 - accuracy: 0.9998 - val_loss: 0.0401 - val_accuracy: 0.9943 - 6s/epoch - 32ms/step\n",
      "Epoch 479/1000\n",
      "\n",
      "Epoch 479: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 3.2594e-04 - accuracy: 0.9998 - val_loss: 0.0368 - val_accuracy: 0.9948 - 6s/epoch - 32ms/step\n",
      "Epoch 480/1000\n",
      "\n",
      "Epoch 480: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 8.1436e-04 - accuracy: 0.9997 - val_loss: 0.0389 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 481/1000\n",
      "\n",
      "Epoch 481: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0362 - val_accuracy: 0.9948 - 6s/epoch - 32ms/step\n",
      "Epoch 482/1000\n",
      "\n",
      "Epoch 482: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 9.2283e-04 - accuracy: 0.9998 - val_loss: 0.0356 - val_accuracy: 0.9949 - 6s/epoch - 32ms/step\n",
      "Epoch 483/1000\n",
      "\n",
      "Epoch 483: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 5.5443e-04 - accuracy: 0.9998 - val_loss: 0.0353 - val_accuracy: 0.9951 - 6s/epoch - 32ms/step\n",
      "Epoch 484/1000\n",
      "\n",
      "Epoch 484: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 5.7424e-04 - accuracy: 0.9998 - val_loss: 0.0400 - val_accuracy: 0.9947 - 6s/epoch - 32ms/step\n",
      "Epoch 485/1000\n",
      "\n",
      "Epoch 485: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 7.1905e-04 - accuracy: 0.9998 - val_loss: 0.0381 - val_accuracy: 0.9948 - 6s/epoch - 32ms/step\n",
      "Epoch 486/1000\n",
      "\n",
      "Epoch 486: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 6.1185e-04 - accuracy: 0.9998 - val_loss: 0.0418 - val_accuracy: 0.9937 - 6s/epoch - 32ms/step\n",
      "Epoch 487/1000\n",
      "\n",
      "Epoch 487: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 2.1176e-04 - accuracy: 1.0000 - val_loss: 0.0340 - val_accuracy: 0.9951 - 6s/epoch - 32ms/step\n",
      "Epoch 488/1000\n",
      "\n",
      "Epoch 488: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 7.7667e-04 - accuracy: 0.9998 - val_loss: 0.0386 - val_accuracy: 0.9940 - 6s/epoch - 32ms/step\n",
      "Epoch 489/1000\n",
      "\n",
      "Epoch 489: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 6.7520e-04 - accuracy: 0.9999 - val_loss: 0.0483 - val_accuracy: 0.9928 - 6s/epoch - 32ms/step\n",
      "Epoch 490/1000\n",
      "\n",
      "Epoch 490: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 6.9337e-04 - accuracy: 0.9998 - val_loss: 0.0429 - val_accuracy: 0.9929 - 6s/epoch - 32ms/step\n",
      "Epoch 491/1000\n",
      "\n",
      "Epoch 491: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 2.6591e-04 - accuracy: 0.9999 - val_loss: 0.0386 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 492/1000\n",
      "\n",
      "Epoch 492: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 4.8770e-04 - accuracy: 0.9999 - val_loss: 0.0371 - val_accuracy: 0.9942 - 6s/epoch - 31ms/step\n",
      "Epoch 493/1000\n",
      "\n",
      "Epoch 493: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 2.3548e-04 - accuracy: 0.9999 - val_loss: 0.0431 - val_accuracy: 0.9940 - 6s/epoch - 31ms/step\n",
      "Epoch 494/1000\n",
      "\n",
      "Epoch 494: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 1.2990e-04 - accuracy: 1.0000 - val_loss: 0.0354 - val_accuracy: 0.9948 - 6s/epoch - 29ms/step\n",
      "Epoch 495/1000\n",
      "\n",
      "Epoch 495: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 1.9080e-04 - accuracy: 0.9999 - val_loss: 0.0399 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 496/1000\n",
      "\n",
      "Epoch 496: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 4.8146e-04 - accuracy: 0.9998 - val_loss: 0.0493 - val_accuracy: 0.9930 - 6s/epoch - 29ms/step\n",
      "Epoch 497/1000\n",
      "\n",
      "Epoch 497: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 6.0360e-04 - accuracy: 0.9997 - val_loss: 0.0379 - val_accuracy: 0.9946 - 6s/epoch - 29ms/step\n",
      "Epoch 498/1000\n",
      "\n",
      "Epoch 498: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 3.7533e-04 - accuracy: 0.9998 - val_loss: 0.0437 - val_accuracy: 0.9940 - 6s/epoch - 31ms/step\n",
      "Epoch 499/1000\n",
      "\n",
      "Epoch 499: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 9.4070e-04 - accuracy: 0.9998 - val_loss: 0.0402 - val_accuracy: 0.9943 - 6s/epoch - 30ms/step\n",
      "Epoch 500/1000\n",
      "\n",
      "Epoch 500: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 4.0183e-04 - accuracy: 0.9998 - val_loss: 0.0447 - val_accuracy: 0.9939 - 6s/epoch - 30ms/step\n",
      "Epoch 501/1000\n",
      "\n",
      "Epoch 501: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 7.7866e-04 - accuracy: 0.9998 - val_loss: 0.0430 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 502/1000\n",
      "\n",
      "Epoch 502: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 5.0937e-04 - accuracy: 0.9998 - val_loss: 0.0372 - val_accuracy: 0.9945 - 6s/epoch - 32ms/step\n",
      "Epoch 503/1000\n",
      "\n",
      "Epoch 503: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0402 - val_accuracy: 0.9942 - 6s/epoch - 31ms/step\n",
      "Epoch 504/1000\n",
      "\n",
      "Epoch 504: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 6.6326e-04 - accuracy: 0.9998 - val_loss: 0.0397 - val_accuracy: 0.9944 - 6s/epoch - 31ms/step\n",
      "Epoch 505/1000\n",
      "\n",
      "Epoch 505: val_accuracy did not improve from 0.99506\n",
      "197/197 - 6s - loss: 4.7519e-04 - accuracy: 0.9999 - val_loss: 0.0394 - val_accuracy: 0.9949 - 6s/epoch - 31ms/step\n",
      "Epoch 506/1000\n",
      "\n",
      "Epoch 506: val_accuracy improved from 0.99506 to 0.99512, saving model to best_model.h5\n",
      "197/197 - 6s - loss: 4.1894e-04 - accuracy: 0.9999 - val_loss: 0.0385 - val_accuracy: 0.9951 - 6s/epoch - 31ms/step\n",
      "Epoch 507/1000\n",
      "\n",
      "Epoch 507: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 2.1518e-04 - accuracy: 0.9999 - val_loss: 0.0403 - val_accuracy: 0.9948 - 6s/epoch - 31ms/step\n",
      "Epoch 508/1000\n",
      "\n",
      "Epoch 508: val_accuracy did not improve from 0.99512\n",
      "197/197 - 7s - loss: 7.6810e-04 - accuracy: 0.9999 - val_loss: 0.0396 - val_accuracy: 0.9946 - 7s/epoch - 33ms/step\n",
      "Epoch 509/1000\n",
      "\n",
      "Epoch 509: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 5.2811e-05 - accuracy: 1.0000 - val_loss: 0.0378 - val_accuracy: 0.9949 - 6s/epoch - 32ms/step\n",
      "Epoch 510/1000\n",
      "\n",
      "Epoch 510: val_accuracy did not improve from 0.99512\n",
      "197/197 - 7s - loss: 9.2791e-05 - accuracy: 1.0000 - val_loss: 0.0414 - val_accuracy: 0.9948 - 7s/epoch - 35ms/step\n",
      "Epoch 511/1000\n",
      "\n",
      "Epoch 511: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 3.8115e-04 - accuracy: 0.9999 - val_loss: 0.0432 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 512/1000\n",
      "\n",
      "Epoch 512: val_accuracy did not improve from 0.99512\n",
      "197/197 - 7s - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0406 - val_accuracy: 0.9944 - 7s/epoch - 34ms/step\n",
      "Epoch 513/1000\n",
      "\n",
      "Epoch 513: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 2.1282e-04 - accuracy: 0.9999 - val_loss: 0.0415 - val_accuracy: 0.9945 - 6s/epoch - 29ms/step\n",
      "Epoch 514/1000\n",
      "\n",
      "Epoch 514: val_accuracy did not improve from 0.99512\n",
      "197/197 - 7s - loss: 1.0897e-04 - accuracy: 1.0000 - val_loss: 0.0407 - val_accuracy: 0.9943 - 7s/epoch - 34ms/step\n",
      "Epoch 515/1000\n",
      "\n",
      "Epoch 515: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 4.2784e-04 - accuracy: 0.9999 - val_loss: 0.0416 - val_accuracy: 0.9945 - 6s/epoch - 33ms/step\n",
      "Epoch 516/1000\n",
      "\n",
      "Epoch 516: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 1.3517e-04 - accuracy: 0.9999 - val_loss: 0.0434 - val_accuracy: 0.9941 - 6s/epoch - 33ms/step\n",
      "Epoch 517/1000\n",
      "\n",
      "Epoch 517: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 1.3479e-04 - accuracy: 1.0000 - val_loss: 0.0407 - val_accuracy: 0.9949 - 6s/epoch - 33ms/step\n",
      "Epoch 518/1000\n",
      "\n",
      "Epoch 518: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0448 - val_accuracy: 0.9940 - 6s/epoch - 32ms/step\n",
      "Epoch 519/1000\n",
      "\n",
      "Epoch 519: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 8.3868e-04 - accuracy: 0.9998 - val_loss: 0.0378 - val_accuracy: 0.9943 - 6s/epoch - 32ms/step\n",
      "Epoch 520/1000\n",
      "\n",
      "Epoch 520: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 8.3644e-04 - accuracy: 0.9998 - val_loss: 0.0427 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 521/1000\n",
      "\n",
      "Epoch 521: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 4.8877e-04 - accuracy: 0.9999 - val_loss: 0.0414 - val_accuracy: 0.9941 - 6s/epoch - 32ms/step\n",
      "Epoch 522/1000\n",
      "\n",
      "Epoch 522: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 4.5794e-04 - accuracy: 0.9999 - val_loss: 0.0414 - val_accuracy: 0.9943 - 6s/epoch - 33ms/step\n",
      "Epoch 523/1000\n",
      "\n",
      "Epoch 523: val_accuracy did not improve from 0.99512\n",
      "197/197 - 14s - loss: 8.3331e-04 - accuracy: 0.9997 - val_loss: 0.0407 - val_accuracy: 0.9946 - 14s/epoch - 70ms/step\n",
      "Epoch 524/1000\n",
      "\n",
      "Epoch 524: val_accuracy did not improve from 0.99512\n",
      "197/197 - 21s - loss: 3.9382e-04 - accuracy: 0.9999 - val_loss: 0.0403 - val_accuracy: 0.9942 - 21s/epoch - 107ms/step\n",
      "Epoch 525/1000\n",
      "\n",
      "Epoch 525: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 3.7319e-04 - accuracy: 0.9998 - val_loss: 0.0453 - val_accuracy: 0.9937 - 6s/epoch - 30ms/step\n",
      "Epoch 526/1000\n",
      "\n",
      "Epoch 526: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 3.0726e-04 - accuracy: 0.9999 - val_loss: 0.0466 - val_accuracy: 0.9938 - 6s/epoch - 29ms/step\n",
      "Epoch 527/1000\n",
      "\n",
      "Epoch 527: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 3.4607e-04 - accuracy: 0.9999 - val_loss: 0.0455 - val_accuracy: 0.9937 - 6s/epoch - 29ms/step\n",
      "Epoch 528/1000\n",
      "\n",
      "Epoch 528: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 2.1055e-04 - accuracy: 0.9999 - val_loss: 0.0460 - val_accuracy: 0.9936 - 6s/epoch - 29ms/step\n",
      "Epoch 529/1000\n",
      "\n",
      "Epoch 529: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 1.7937e-04 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.9941 - 6s/epoch - 29ms/step\n",
      "Epoch 530/1000\n",
      "\n",
      "Epoch 530: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 5.3583e-04 - accuracy: 0.9999 - val_loss: 0.0453 - val_accuracy: 0.9936 - 6s/epoch - 29ms/step\n",
      "Epoch 531/1000\n",
      "\n",
      "Epoch 531: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 8.6418e-04 - accuracy: 0.9998 - val_loss: 0.0412 - val_accuracy: 0.9943 - 6s/epoch - 29ms/step\n",
      "Epoch 532/1000\n",
      "\n",
      "Epoch 532: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 4.8472e-04 - accuracy: 0.9998 - val_loss: 0.0462 - val_accuracy: 0.9933 - 6s/epoch - 29ms/step\n",
      "Epoch 533/1000\n",
      "\n",
      "Epoch 533: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 2.3079e-04 - accuracy: 0.9999 - val_loss: 0.0404 - val_accuracy: 0.9942 - 6s/epoch - 29ms/step\n",
      "Epoch 534/1000\n",
      "\n",
      "Epoch 534: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 6.7418e-04 - accuracy: 0.9998 - val_loss: 0.0423 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 535/1000\n",
      "\n",
      "Epoch 535: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 2.1773e-04 - accuracy: 0.9999 - val_loss: 0.0432 - val_accuracy: 0.9943 - 6s/epoch - 29ms/step\n",
      "Epoch 536/1000\n",
      "\n",
      "Epoch 536: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.0391 - val_accuracy: 0.9945 - 6s/epoch - 29ms/step\n",
      "Epoch 537/1000\n",
      "\n",
      "Epoch 537: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0460 - val_accuracy: 0.9937 - 6s/epoch - 29ms/step\n",
      "Epoch 538/1000\n",
      "\n",
      "Epoch 538: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0496 - val_accuracy: 0.9931 - 6s/epoch - 29ms/step\n",
      "Epoch 539/1000\n",
      "\n",
      "Epoch 539: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 6.4913e-04 - accuracy: 0.9999 - val_loss: 0.0434 - val_accuracy: 0.9938 - 6s/epoch - 29ms/step\n",
      "Epoch 540/1000\n",
      "\n",
      "Epoch 540: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 3.0821e-04 - accuracy: 0.9999 - val_loss: 0.0444 - val_accuracy: 0.9936 - 6s/epoch - 29ms/step\n",
      "Epoch 541/1000\n",
      "\n",
      "Epoch 541: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 1.1969e-04 - accuracy: 0.9999 - val_loss: 0.0459 - val_accuracy: 0.9942 - 6s/epoch - 29ms/step\n",
      "Epoch 542/1000\n",
      "\n",
      "Epoch 542: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 4.1025e-04 - accuracy: 0.9999 - val_loss: 0.0462 - val_accuracy: 0.9935 - 6s/epoch - 29ms/step\n",
      "Epoch 543/1000\n",
      "\n",
      "Epoch 543: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 4.9143e-04 - accuracy: 0.9998 - val_loss: 0.0458 - val_accuracy: 0.9939 - 6s/epoch - 29ms/step\n",
      "Epoch 544/1000\n",
      "\n",
      "Epoch 544: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 4.6184e-04 - accuracy: 0.9998 - val_loss: 0.0437 - val_accuracy: 0.9943 - 6s/epoch - 29ms/step\n",
      "Epoch 545/1000\n",
      "\n",
      "Epoch 545: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 8.1394e-05 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 0.9939 - 6s/epoch - 29ms/step\n",
      "Epoch 546/1000\n",
      "\n",
      "Epoch 546: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 8.7682e-05 - accuracy: 1.0000 - val_loss: 0.0457 - val_accuracy: 0.9939 - 6s/epoch - 29ms/step\n",
      "Epoch 547/1000\n",
      "\n",
      "Epoch 547: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 3.3318e-05 - accuracy: 1.0000 - val_loss: 0.0438 - val_accuracy: 0.9942 - 6s/epoch - 29ms/step\n",
      "Epoch 548/1000\n",
      "\n",
      "Epoch 548: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 1.0354e-04 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 549/1000\n",
      "\n",
      "Epoch 549: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 1.6501e-04 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 550/1000\n",
      "\n",
      "Epoch 550: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 6.5828e-04 - accuracy: 0.9998 - val_loss: 0.0457 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 551/1000\n",
      "\n",
      "Epoch 551: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.0542 - val_accuracy: 0.9923 - 6s/epoch - 32ms/step\n",
      "Epoch 552/1000\n",
      "\n",
      "Epoch 552: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 5.0710e-04 - accuracy: 0.9998 - val_loss: 0.0504 - val_accuracy: 0.9931 - 6s/epoch - 31ms/step\n",
      "Epoch 553/1000\n",
      "\n",
      "Epoch 553: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 7.0283e-04 - accuracy: 0.9998 - val_loss: 0.0478 - val_accuracy: 0.9936 - 6s/epoch - 31ms/step\n",
      "Epoch 554/1000\n",
      "\n",
      "Epoch 554: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 7.0038e-04 - accuracy: 0.9998 - val_loss: 0.0446 - val_accuracy: 0.9938 - 6s/epoch - 31ms/step\n",
      "Epoch 555/1000\n",
      "\n",
      "Epoch 555: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 3.5513e-04 - accuracy: 0.9998 - val_loss: 0.0440 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 556/1000\n",
      "\n",
      "Epoch 556: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 4.4794e-04 - accuracy: 0.9999 - val_loss: 0.0425 - val_accuracy: 0.9942 - 6s/epoch - 31ms/step\n",
      "Epoch 557/1000\n",
      "\n",
      "Epoch 557: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 4.8694e-04 - accuracy: 0.9998 - val_loss: 0.0440 - val_accuracy: 0.9935 - 6s/epoch - 31ms/step\n",
      "Epoch 558/1000\n",
      "\n",
      "Epoch 558: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 6.5307e-04 - accuracy: 0.9997 - val_loss: 0.0418 - val_accuracy: 0.9941 - 6s/epoch - 29ms/step\n",
      "Epoch 559/1000\n",
      "\n",
      "Epoch 559: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 9.7160e-04 - accuracy: 0.9998 - val_loss: 0.0471 - val_accuracy: 0.9933 - 6s/epoch - 29ms/step\n",
      "Epoch 560/1000\n",
      "\n",
      "Epoch 560: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 5.3708e-04 - accuracy: 0.9999 - val_loss: 0.0414 - val_accuracy: 0.9943 - 6s/epoch - 29ms/step\n",
      "Epoch 561/1000\n",
      "\n",
      "Epoch 561: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 1.1934e-04 - accuracy: 1.0000 - val_loss: 0.0420 - val_accuracy: 0.9943 - 6s/epoch - 29ms/step\n",
      "Epoch 562/1000\n",
      "\n",
      "Epoch 562: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 2.8621e-04 - accuracy: 1.0000 - val_loss: 0.0394 - val_accuracy: 0.9945 - 6s/epoch - 29ms/step\n",
      "Epoch 563/1000\n",
      "\n",
      "Epoch 563: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 9.2106e-05 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 0.9942 - 6s/epoch - 29ms/step\n",
      "Epoch 564/1000\n",
      "\n",
      "Epoch 564: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 4.8713e-04 - accuracy: 0.9999 - val_loss: 0.0377 - val_accuracy: 0.9948 - 6s/epoch - 29ms/step\n",
      "Epoch 565/1000\n",
      "\n",
      "Epoch 565: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 1.6891e-04 - accuracy: 1.0000 - val_loss: 0.0399 - val_accuracy: 0.9945 - 6s/epoch - 29ms/step\n",
      "Epoch 566/1000\n",
      "\n",
      "Epoch 566: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 4.3593e-04 - accuracy: 0.9999 - val_loss: 0.0390 - val_accuracy: 0.9946 - 6s/epoch - 29ms/step\n",
      "Epoch 567/1000\n",
      "\n",
      "Epoch 567: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 3.9060e-04 - accuracy: 0.9999 - val_loss: 0.0398 - val_accuracy: 0.9941 - 6s/epoch - 29ms/step\n",
      "Epoch 568/1000\n",
      "\n",
      "Epoch 568: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 5.1757e-04 - accuracy: 0.9998 - val_loss: 0.0413 - val_accuracy: 0.9945 - 6s/epoch - 30ms/step\n",
      "Epoch 569/1000\n",
      "\n",
      "Epoch 569: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 6.0009e-04 - accuracy: 0.9998 - val_loss: 0.0422 - val_accuracy: 0.9941 - 6s/epoch - 32ms/step\n",
      "Epoch 570/1000\n",
      "\n",
      "Epoch 570: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 7.0192e-05 - accuracy: 1.0000 - val_loss: 0.0440 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 571/1000\n",
      "\n",
      "Epoch 571: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 4.9735e-04 - accuracy: 0.9998 - val_loss: 0.0411 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 572/1000\n",
      "\n",
      "Epoch 572: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 3.5797e-04 - accuracy: 0.9998 - val_loss: 0.0425 - val_accuracy: 0.9947 - 6s/epoch - 32ms/step\n",
      "Epoch 573/1000\n",
      "\n",
      "Epoch 573: val_accuracy did not improve from 0.99512\n",
      "197/197 - 7s - loss: 6.8786e-04 - accuracy: 0.9998 - val_loss: 0.0440 - val_accuracy: 0.9940 - 7s/epoch - 35ms/step\n",
      "Epoch 574/1000\n",
      "\n",
      "Epoch 574: val_accuracy did not improve from 0.99512\n",
      "197/197 - 7s - loss: 1.6463e-04 - accuracy: 0.9999 - val_loss: 0.0426 - val_accuracy: 0.9942 - 7s/epoch - 35ms/step\n",
      "Epoch 575/1000\n",
      "\n",
      "Epoch 575: val_accuracy did not improve from 0.99512\n",
      "197/197 - 7s - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.0399 - val_accuracy: 0.9943 - 7s/epoch - 35ms/step\n",
      "Epoch 576/1000\n",
      "\n",
      "Epoch 576: val_accuracy did not improve from 0.99512\n",
      "197/197 - 7s - loss: 0.0010 - accuracy: 0.9996 - val_loss: 0.0396 - val_accuracy: 0.9943 - 7s/epoch - 34ms/step\n",
      "Epoch 577/1000\n",
      "\n",
      "Epoch 577: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 6.6595e-04 - accuracy: 0.9998 - val_loss: 0.0395 - val_accuracy: 0.9941 - 6s/epoch - 29ms/step\n",
      "Epoch 578/1000\n",
      "\n",
      "Epoch 578: val_accuracy did not improve from 0.99512\n",
      "197/197 - 6s - loss: 3.5960e-04 - accuracy: 0.9998 - val_loss: 0.0400 - val_accuracy: 0.9942 - 6s/epoch - 29ms/step\n",
      "Epoch 579/1000\n",
      "\n",
      "Epoch 579: val_accuracy improved from 0.99512 to 0.99518, saving model to best_model.h5\n",
      "197/197 - 6s - loss: 7.8809e-04 - accuracy: 0.9998 - val_loss: 0.0374 - val_accuracy: 0.9952 - 6s/epoch - 29ms/step\n",
      "Epoch 580/1000\n",
      "\n",
      "Epoch 580: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.1946e-04 - accuracy: 0.9999 - val_loss: 0.0382 - val_accuracy: 0.9946 - 6s/epoch - 29ms/step\n",
      "Epoch 581/1000\n",
      "\n",
      "Epoch 581: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.7784e-04 - accuracy: 0.9999 - val_loss: 0.0404 - val_accuracy: 0.9946 - 6s/epoch - 31ms/step\n",
      "Epoch 582/1000\n",
      "\n",
      "Epoch 582: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.4460e-05 - accuracy: 1.0000 - val_loss: 0.0395 - val_accuracy: 0.9945 - 6s/epoch - 29ms/step\n",
      "Epoch 583/1000\n",
      "\n",
      "Epoch 583: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.4199e-04 - accuracy: 0.9999 - val_loss: 0.0432 - val_accuracy: 0.9944 - 6s/epoch - 29ms/step\n",
      "Epoch 584/1000\n",
      "\n",
      "Epoch 584: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.8976e-05 - accuracy: 1.0000 - val_loss: 0.0407 - val_accuracy: 0.9949 - 6s/epoch - 29ms/step\n",
      "Epoch 585/1000\n",
      "\n",
      "Epoch 585: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.0542e-04 - accuracy: 0.9999 - val_loss: 0.0415 - val_accuracy: 0.9941 - 6s/epoch - 29ms/step\n",
      "Epoch 586/1000\n",
      "\n",
      "Epoch 586: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.4725e-04 - accuracy: 0.9999 - val_loss: 0.0384 - val_accuracy: 0.9948 - 6s/epoch - 29ms/step\n",
      "Epoch 587/1000\n",
      "\n",
      "Epoch 587: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.0977e-04 - accuracy: 1.0000 - val_loss: 0.0417 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 588/1000\n",
      "\n",
      "Epoch 588: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.2357e-04 - accuracy: 0.9999 - val_loss: 0.0370 - val_accuracy: 0.9941 - 6s/epoch - 29ms/step\n",
      "Epoch 589/1000\n",
      "\n",
      "Epoch 589: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.0477e-05 - accuracy: 1.0000 - val_loss: 0.0380 - val_accuracy: 0.9943 - 6s/epoch - 29ms/step\n",
      "Epoch 590/1000\n",
      "\n",
      "Epoch 590: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.9000e-04 - accuracy: 0.9999 - val_loss: 0.0387 - val_accuracy: 0.9947 - 6s/epoch - 29ms/step\n",
      "Epoch 591/1000\n",
      "\n",
      "Epoch 591: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.9238e-04 - accuracy: 0.9999 - val_loss: 0.0390 - val_accuracy: 0.9939 - 6s/epoch - 29ms/step\n",
      "Epoch 592/1000\n",
      "\n",
      "Epoch 592: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.6032e-04 - accuracy: 0.9998 - val_loss: 0.0395 - val_accuracy: 0.9949 - 6s/epoch - 29ms/step\n",
      "Epoch 593/1000\n",
      "\n",
      "Epoch 593: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.1865e-04 - accuracy: 1.0000 - val_loss: 0.0403 - val_accuracy: 0.9943 - 6s/epoch - 29ms/step\n",
      "Epoch 594/1000\n",
      "\n",
      "Epoch 594: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.9295e-04 - accuracy: 0.9999 - val_loss: 0.0423 - val_accuracy: 0.9940 - 6s/epoch - 30ms/step\n",
      "Epoch 595/1000\n",
      "\n",
      "Epoch 595: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.6674e-04 - accuracy: 0.9999 - val_loss: 0.0438 - val_accuracy: 0.9939 - 6s/epoch - 29ms/step\n",
      "Epoch 596/1000\n",
      "\n",
      "Epoch 596: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.0692e-04 - accuracy: 0.9998 - val_loss: 0.0451 - val_accuracy: 0.9937 - 6s/epoch - 31ms/step\n",
      "Epoch 597/1000\n",
      "\n",
      "Epoch 597: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.6348e-04 - accuracy: 0.9999 - val_loss: 0.0447 - val_accuracy: 0.9941 - 6s/epoch - 31ms/step\n",
      "Epoch 598/1000\n",
      "\n",
      "Epoch 598: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.9952e-04 - accuracy: 1.0000 - val_loss: 0.0463 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 599/1000\n",
      "\n",
      "Epoch 599: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.7846e-05 - accuracy: 1.0000 - val_loss: 0.0444 - val_accuracy: 0.9942 - 6s/epoch - 31ms/step\n",
      "Epoch 600/1000\n",
      "\n",
      "Epoch 600: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.4595e-04 - accuracy: 0.9999 - val_loss: 0.0441 - val_accuracy: 0.9943 - 6s/epoch - 31ms/step\n",
      "Epoch 601/1000\n",
      "\n",
      "Epoch 601: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.7062e-04 - accuracy: 0.9999 - val_loss: 0.0437 - val_accuracy: 0.9939 - 6s/epoch - 30ms/step\n",
      "Epoch 602/1000\n",
      "\n",
      "Epoch 602: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.0646e-05 - accuracy: 1.0000 - val_loss: 0.0458 - val_accuracy: 0.9945 - 6s/epoch - 29ms/step\n",
      "Epoch 603/1000\n",
      "\n",
      "Epoch 603: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.4902e-04 - accuracy: 0.9999 - val_loss: 0.0466 - val_accuracy: 0.9945 - 6s/epoch - 29ms/step\n",
      "Epoch 604/1000\n",
      "\n",
      "Epoch 604: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.5524e-04 - accuracy: 0.9999 - val_loss: 0.0474 - val_accuracy: 0.9935 - 6s/epoch - 29ms/step\n",
      "Epoch 605/1000\n",
      "\n",
      "Epoch 605: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.1605e-04 - accuracy: 0.9999 - val_loss: 0.0456 - val_accuracy: 0.9934 - 6s/epoch - 29ms/step\n",
      "Epoch 606/1000\n",
      "\n",
      "Epoch 606: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.0551e-04 - accuracy: 0.9997 - val_loss: 0.0499 - val_accuracy: 0.9930 - 6s/epoch - 29ms/step\n",
      "Epoch 607/1000\n",
      "\n",
      "Epoch 607: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.1366e-04 - accuracy: 0.9997 - val_loss: 0.0489 - val_accuracy: 0.9938 - 6s/epoch - 29ms/step\n",
      "Epoch 608/1000\n",
      "\n",
      "Epoch 608: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.8520e-04 - accuracy: 0.9999 - val_loss: 0.0438 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 609/1000\n",
      "\n",
      "Epoch 609: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.3348e-05 - accuracy: 1.0000 - val_loss: 0.0430 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 610/1000\n",
      "\n",
      "Epoch 610: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.8149e-05 - accuracy: 1.0000 - val_loss: 0.0412 - val_accuracy: 0.9946 - 6s/epoch - 29ms/step\n",
      "Epoch 611/1000\n",
      "\n",
      "Epoch 611: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.7672e-04 - accuracy: 0.9998 - val_loss: 0.0474 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 612/1000\n",
      "\n",
      "Epoch 612: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.2847e-04 - accuracy: 0.9999 - val_loss: 0.0464 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 613/1000\n",
      "\n",
      "Epoch 613: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.9162e-04 - accuracy: 0.9999 - val_loss: 0.0434 - val_accuracy: 0.9940 - 6s/epoch - 28ms/step\n",
      "Epoch 614/1000\n",
      "\n",
      "Epoch 614: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.0113e-04 - accuracy: 0.9998 - val_loss: 0.0427 - val_accuracy: 0.9936 - 6s/epoch - 29ms/step\n",
      "Epoch 615/1000\n",
      "\n",
      "Epoch 615: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.4072e-04 - accuracy: 1.0000 - val_loss: 0.0474 - val_accuracy: 0.9935 - 6s/epoch - 29ms/step\n",
      "Epoch 616/1000\n",
      "\n",
      "Epoch 616: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.5752e-04 - accuracy: 1.0000 - val_loss: 0.0453 - val_accuracy: 0.9942 - 6s/epoch - 29ms/step\n",
      "Epoch 617/1000\n",
      "\n",
      "Epoch 617: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.2827e-04 - accuracy: 1.0000 - val_loss: 0.0512 - val_accuracy: 0.9929 - 6s/epoch - 31ms/step\n",
      "Epoch 618/1000\n",
      "\n",
      "Epoch 618: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0487 - val_accuracy: 0.9934 - 6s/epoch - 32ms/step\n",
      "Epoch 619/1000\n",
      "\n",
      "Epoch 619: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0520 - val_accuracy: 0.9928 - 6s/epoch - 32ms/step\n",
      "Epoch 620/1000\n",
      "\n",
      "Epoch 620: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.1371e-04 - accuracy: 0.9999 - val_loss: 0.0457 - val_accuracy: 0.9936 - 6s/epoch - 31ms/step\n",
      "Epoch 621/1000\n",
      "\n",
      "Epoch 621: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.1196e-04 - accuracy: 0.9997 - val_loss: 0.0452 - val_accuracy: 0.9943 - 6s/epoch - 29ms/step\n",
      "Epoch 622/1000\n",
      "\n",
      "Epoch 622: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0454 - val_accuracy: 0.9942 - 6s/epoch - 29ms/step\n",
      "Epoch 623/1000\n",
      "\n",
      "Epoch 623: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.1689e-04 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 0.9938 - 6s/epoch - 29ms/step\n",
      "Epoch 624/1000\n",
      "\n",
      "Epoch 624: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.4452e-05 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.9937 - 6s/epoch - 29ms/step\n",
      "Epoch 625/1000\n",
      "\n",
      "Epoch 625: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.0075e-05 - accuracy: 1.0000 - val_loss: 0.0437 - val_accuracy: 0.9943 - 6s/epoch - 29ms/step\n",
      "Epoch 626/1000\n",
      "\n",
      "Epoch 626: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.3488e-05 - accuracy: 1.0000 - val_loss: 0.0427 - val_accuracy: 0.9946 - 6s/epoch - 29ms/step\n",
      "Epoch 627/1000\n",
      "\n",
      "Epoch 627: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.5170e-05 - accuracy: 1.0000 - val_loss: 0.0431 - val_accuracy: 0.9948 - 6s/epoch - 29ms/step\n",
      "Epoch 628/1000\n",
      "\n",
      "Epoch 628: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.5014e-04 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 0.9942 - 6s/epoch - 29ms/step\n",
      "Epoch 629/1000\n",
      "\n",
      "Epoch 629: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.2185e-04 - accuracy: 0.9998 - val_loss: 0.0475 - val_accuracy: 0.9939 - 6s/epoch - 29ms/step\n",
      "Epoch 630/1000\n",
      "\n",
      "Epoch 630: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.1887e-04 - accuracy: 0.9999 - val_loss: 0.0496 - val_accuracy: 0.9931 - 6s/epoch - 29ms/step\n",
      "Epoch 631/1000\n",
      "\n",
      "Epoch 631: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.0287e-04 - accuracy: 1.0000 - val_loss: 0.0480 - val_accuracy: 0.9937 - 6s/epoch - 29ms/step\n",
      "Epoch 632/1000\n",
      "\n",
      "Epoch 632: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.0051e-04 - accuracy: 0.9999 - val_loss: 0.0512 - val_accuracy: 0.9933 - 6s/epoch - 30ms/step\n",
      "Epoch 633/1000\n",
      "\n",
      "Epoch 633: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0515 - val_accuracy: 0.9937 - 6s/epoch - 30ms/step\n",
      "Epoch 634/1000\n",
      "\n",
      "Epoch 634: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.2335e-04 - accuracy: 0.9999 - val_loss: 0.0455 - val_accuracy: 0.9942 - 6s/epoch - 30ms/step\n",
      "Epoch 635/1000\n",
      "\n",
      "Epoch 635: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.1805e-04 - accuracy: 0.9998 - val_loss: 0.0486 - val_accuracy: 0.9933 - 6s/epoch - 30ms/step\n",
      "Epoch 636/1000\n",
      "\n",
      "Epoch 636: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.6742e-04 - accuracy: 0.9999 - val_loss: 0.0488 - val_accuracy: 0.9932 - 6s/epoch - 30ms/step\n",
      "Epoch 637/1000\n",
      "\n",
      "Epoch 637: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0499 - val_accuracy: 0.9931 - 6s/epoch - 30ms/step\n",
      "Epoch 638/1000\n",
      "\n",
      "Epoch 638: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0442 - val_accuracy: 0.9937 - 6s/epoch - 30ms/step\n",
      "Epoch 639/1000\n",
      "\n",
      "Epoch 639: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.7214e-04 - accuracy: 0.9998 - val_loss: 0.0496 - val_accuracy: 0.9929 - 6s/epoch - 30ms/step\n",
      "Epoch 640/1000\n",
      "\n",
      "Epoch 640: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.0124e-04 - accuracy: 1.0000 - val_loss: 0.0449 - val_accuracy: 0.9937 - 6s/epoch - 30ms/step\n",
      "Epoch 641/1000\n",
      "\n",
      "Epoch 641: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.0251e-04 - accuracy: 1.0000 - val_loss: 0.0438 - val_accuracy: 0.9940 - 6s/epoch - 30ms/step\n",
      "Epoch 642/1000\n",
      "\n",
      "Epoch 642: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.1743e-04 - accuracy: 0.9998 - val_loss: 0.0460 - val_accuracy: 0.9931 - 6s/epoch - 29ms/step\n",
      "Epoch 643/1000\n",
      "\n",
      "Epoch 643: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.8625e-04 - accuracy: 0.9998 - val_loss: 0.0467 - val_accuracy: 0.9936 - 6s/epoch - 29ms/step\n",
      "Epoch 644/1000\n",
      "\n",
      "Epoch 644: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.7963e-04 - accuracy: 0.9998 - val_loss: 0.0448 - val_accuracy: 0.9935 - 6s/epoch - 29ms/step\n",
      "Epoch 645/1000\n",
      "\n",
      "Epoch 645: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.6936e-04 - accuracy: 0.9998 - val_loss: 0.0427 - val_accuracy: 0.9941 - 6s/epoch - 29ms/step\n",
      "Epoch 646/1000\n",
      "\n",
      "Epoch 646: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.0197e-04 - accuracy: 0.9999 - val_loss: 0.0369 - val_accuracy: 0.9942 - 6s/epoch - 29ms/step\n",
      "Epoch 647/1000\n",
      "\n",
      "Epoch 647: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.8184e-04 - accuracy: 0.9999 - val_loss: 0.0393 - val_accuracy: 0.9942 - 6s/epoch - 29ms/step\n",
      "Epoch 648/1000\n",
      "\n",
      "Epoch 648: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.5884e-04 - accuracy: 0.9999 - val_loss: 0.0416 - val_accuracy: 0.9942 - 6s/epoch - 29ms/step\n",
      "Epoch 649/1000\n",
      "\n",
      "Epoch 649: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.9713e-04 - accuracy: 0.9999 - val_loss: 0.0435 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 650/1000\n",
      "\n",
      "Epoch 650: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.3413e-04 - accuracy: 0.9998 - val_loss: 0.0437 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 651/1000\n",
      "\n",
      "Epoch 651: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.0869e-05 - accuracy: 1.0000 - val_loss: 0.0436 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 652/1000\n",
      "\n",
      "Epoch 652: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.4389e-04 - accuracy: 0.9999 - val_loss: 0.0418 - val_accuracy: 0.9939 - 6s/epoch - 30ms/step\n",
      "Epoch 653/1000\n",
      "\n",
      "Epoch 653: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0440 - val_accuracy: 0.9942 - 6s/epoch - 29ms/step\n",
      "Epoch 654/1000\n",
      "\n",
      "Epoch 654: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.1268e-04 - accuracy: 0.9999 - val_loss: 0.0417 - val_accuracy: 0.9937 - 6s/epoch - 29ms/step\n",
      "Epoch 655/1000\n",
      "\n",
      "Epoch 655: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.8242e-04 - accuracy: 0.9998 - val_loss: 0.0432 - val_accuracy: 0.9939 - 6s/epoch - 29ms/step\n",
      "Epoch 656/1000\n",
      "\n",
      "Epoch 656: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.2198e-04 - accuracy: 0.9999 - val_loss: 0.0410 - val_accuracy: 0.9939 - 6s/epoch - 29ms/step\n",
      "Epoch 657/1000\n",
      "\n",
      "Epoch 657: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.8133e-05 - accuracy: 1.0000 - val_loss: 0.0412 - val_accuracy: 0.9939 - 6s/epoch - 29ms/step\n",
      "Epoch 658/1000\n",
      "\n",
      "Epoch 658: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.7089e-05 - accuracy: 1.0000 - val_loss: 0.0400 - val_accuracy: 0.9939 - 6s/epoch - 29ms/step\n",
      "Epoch 659/1000\n",
      "\n",
      "Epoch 659: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.6875e-04 - accuracy: 0.9998 - val_loss: 0.0442 - val_accuracy: 0.9937 - 6s/epoch - 29ms/step\n",
      "Epoch 660/1000\n",
      "\n",
      "Epoch 660: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.8735e-04 - accuracy: 1.0000 - val_loss: 0.0407 - val_accuracy: 0.9939 - 6s/epoch - 29ms/step\n",
      "Epoch 661/1000\n",
      "\n",
      "Epoch 661: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.3259e-04 - accuracy: 0.9999 - val_loss: 0.0407 - val_accuracy: 0.9945 - 6s/epoch - 29ms/step\n",
      "Epoch 662/1000\n",
      "\n",
      "Epoch 662: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.3395e-04 - accuracy: 1.0000 - val_loss: 0.0417 - val_accuracy: 0.9943 - 6s/epoch - 29ms/step\n",
      "Epoch 663/1000\n",
      "\n",
      "Epoch 663: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.3590e-05 - accuracy: 1.0000 - val_loss: 0.0406 - val_accuracy: 0.9942 - 6s/epoch - 29ms/step\n",
      "Epoch 664/1000\n",
      "\n",
      "Epoch 664: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.1936e-05 - accuracy: 1.0000 - val_loss: 0.0409 - val_accuracy: 0.9941 - 6s/epoch - 29ms/step\n",
      "Epoch 665/1000\n",
      "\n",
      "Epoch 665: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.2473e-04 - accuracy: 0.9999 - val_loss: 0.0466 - val_accuracy: 0.9936 - 6s/epoch - 29ms/step\n",
      "Epoch 666/1000\n",
      "\n",
      "Epoch 666: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.5002e-04 - accuracy: 0.9999 - val_loss: 0.0430 - val_accuracy: 0.9942 - 6s/epoch - 29ms/step\n",
      "Epoch 667/1000\n",
      "\n",
      "Epoch 667: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.6652e-05 - accuracy: 1.0000 - val_loss: 0.0432 - val_accuracy: 0.9937 - 6s/epoch - 29ms/step\n",
      "Epoch 668/1000\n",
      "\n",
      "Epoch 668: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.9677e-04 - accuracy: 1.0000 - val_loss: 0.0443 - val_accuracy: 0.9937 - 6s/epoch - 29ms/step\n",
      "Epoch 669/1000\n",
      "\n",
      "Epoch 669: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.7936e-04 - accuracy: 1.0000 - val_loss: 0.0424 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 670/1000\n",
      "\n",
      "Epoch 670: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.6351e-04 - accuracy: 0.9999 - val_loss: 0.0442 - val_accuracy: 0.9943 - 6s/epoch - 29ms/step\n",
      "Epoch 671/1000\n",
      "\n",
      "Epoch 671: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.2877e-04 - accuracy: 0.9999 - val_loss: 0.0506 - val_accuracy: 0.9934 - 6s/epoch - 29ms/step\n",
      "Epoch 672/1000\n",
      "\n",
      "Epoch 672: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.5507e-04 - accuracy: 0.9997 - val_loss: 0.0459 - val_accuracy: 0.9934 - 6s/epoch - 29ms/step\n",
      "Epoch 673/1000\n",
      "\n",
      "Epoch 673: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.6423e-04 - accuracy: 0.9998 - val_loss: 0.0426 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 674/1000\n",
      "\n",
      "Epoch 674: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.6614e-04 - accuracy: 0.9997 - val_loss: 0.0445 - val_accuracy: 0.9939 - 6s/epoch - 29ms/step\n",
      "Epoch 675/1000\n",
      "\n",
      "Epoch 675: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.0317e-04 - accuracy: 1.0000 - val_loss: 0.0478 - val_accuracy: 0.9935 - 6s/epoch - 31ms/step\n",
      "Epoch 676/1000\n",
      "\n",
      "Epoch 676: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.8818e-05 - accuracy: 1.0000 - val_loss: 0.0451 - val_accuracy: 0.9942 - 6s/epoch - 30ms/step\n",
      "Epoch 677/1000\n",
      "\n",
      "Epoch 677: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.1973e-05 - accuracy: 1.0000 - val_loss: 0.0441 - val_accuracy: 0.9943 - 6s/epoch - 30ms/step\n",
      "Epoch 678/1000\n",
      "\n",
      "Epoch 678: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.3692e-04 - accuracy: 0.9999 - val_loss: 0.0512 - val_accuracy: 0.9935 - 6s/epoch - 30ms/step\n",
      "Epoch 679/1000\n",
      "\n",
      "Epoch 679: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.9042e-05 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 0.9935 - 6s/epoch - 30ms/step\n",
      "Epoch 680/1000\n",
      "\n",
      "Epoch 680: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.8031e-04 - accuracy: 0.9999 - val_loss: 0.0488 - val_accuracy: 0.9939 - 6s/epoch - 30ms/step\n",
      "Epoch 681/1000\n",
      "\n",
      "Epoch 681: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.1200e-04 - accuracy: 0.9999 - val_loss: 0.0568 - val_accuracy: 0.9930 - 6s/epoch - 30ms/step\n",
      "Epoch 682/1000\n",
      "\n",
      "Epoch 682: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.0635e-04 - accuracy: 0.9998 - val_loss: 0.0556 - val_accuracy: 0.9929 - 6s/epoch - 30ms/step\n",
      "Epoch 683/1000\n",
      "\n",
      "Epoch 683: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.2198e-04 - accuracy: 1.0000 - val_loss: 0.0545 - val_accuracy: 0.9934 - 6s/epoch - 30ms/step\n",
      "Epoch 684/1000\n",
      "\n",
      "Epoch 684: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.8609e-05 - accuracy: 1.0000 - val_loss: 0.0540 - val_accuracy: 0.9935 - 6s/epoch - 30ms/step\n",
      "Epoch 685/1000\n",
      "\n",
      "Epoch 685: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.1985e-04 - accuracy: 0.9999 - val_loss: 0.0523 - val_accuracy: 0.9934 - 6s/epoch - 30ms/step\n",
      "Epoch 686/1000\n",
      "\n",
      "Epoch 686: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.9811e-05 - accuracy: 1.0000 - val_loss: 0.0502 - val_accuracy: 0.9937 - 6s/epoch - 30ms/step\n",
      "Epoch 687/1000\n",
      "\n",
      "Epoch 687: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.4268e-05 - accuracy: 1.0000 - val_loss: 0.0464 - val_accuracy: 0.9943 - 6s/epoch - 30ms/step\n",
      "Epoch 688/1000\n",
      "\n",
      "Epoch 688: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.7813e-04 - accuracy: 0.9999 - val_loss: 0.0447 - val_accuracy: 0.9948 - 6s/epoch - 30ms/step\n",
      "Epoch 689/1000\n",
      "\n",
      "Epoch 689: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.0323e-04 - accuracy: 0.9999 - val_loss: 0.0494 - val_accuracy: 0.9939 - 6s/epoch - 30ms/step\n",
      "Epoch 690/1000\n",
      "\n",
      "Epoch 690: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.6014e-04 - accuracy: 0.9999 - val_loss: 0.0414 - val_accuracy: 0.9947 - 6s/epoch - 30ms/step\n",
      "Epoch 691/1000\n",
      "\n",
      "Epoch 691: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.3384e-04 - accuracy: 1.0000 - val_loss: 0.0460 - val_accuracy: 0.9946 - 6s/epoch - 29ms/step\n",
      "Epoch 692/1000\n",
      "\n",
      "Epoch 692: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.0672e-04 - accuracy: 1.0000 - val_loss: 0.0477 - val_accuracy: 0.9943 - 6s/epoch - 29ms/step\n",
      "Epoch 693/1000\n",
      "\n",
      "Epoch 693: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.9546e-04 - accuracy: 0.9999 - val_loss: 0.0527 - val_accuracy: 0.9938 - 6s/epoch - 28ms/step\n",
      "Epoch 694/1000\n",
      "\n",
      "Epoch 694: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.8449e-04 - accuracy: 0.9999 - val_loss: 0.0497 - val_accuracy: 0.9939 - 6s/epoch - 29ms/step\n",
      "Epoch 695/1000\n",
      "\n",
      "Epoch 695: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.3071e-05 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 696/1000\n",
      "\n",
      "Epoch 696: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.6096e-05 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 0.9935 - 6s/epoch - 29ms/step\n",
      "Epoch 697/1000\n",
      "\n",
      "Epoch 697: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.2019e-04 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 0.9937 - 6s/epoch - 28ms/step\n",
      "Epoch 698/1000\n",
      "\n",
      "Epoch 698: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.8899e-04 - accuracy: 0.9999 - val_loss: 0.0521 - val_accuracy: 0.9936 - 6s/epoch - 29ms/step\n",
      "Epoch 699/1000\n",
      "\n",
      "Epoch 699: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.7037e-04 - accuracy: 0.9998 - val_loss: 0.0581 - val_accuracy: 0.9930 - 6s/epoch - 29ms/step\n",
      "Epoch 700/1000\n",
      "\n",
      "Epoch 700: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.9152e-04 - accuracy: 1.0000 - val_loss: 0.0540 - val_accuracy: 0.9936 - 6s/epoch - 28ms/step\n",
      "Epoch 701/1000\n",
      "\n",
      "Epoch 701: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.6994e-04 - accuracy: 0.9998 - val_loss: 0.0531 - val_accuracy: 0.9936 - 6s/epoch - 28ms/step\n",
      "Epoch 702/1000\n",
      "\n",
      "Epoch 702: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.7459e-04 - accuracy: 0.9999 - val_loss: 0.0534 - val_accuracy: 0.9932 - 6s/epoch - 28ms/step\n",
      "Epoch 703/1000\n",
      "\n",
      "Epoch 703: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.0479e-05 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9936 - 6s/epoch - 29ms/step\n",
      "Epoch 704/1000\n",
      "\n",
      "Epoch 704: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0593 - val_accuracy: 0.9927 - 6s/epoch - 29ms/step\n",
      "Epoch 705/1000\n",
      "\n",
      "Epoch 705: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.6684e-04 - accuracy: 0.9998 - val_loss: 0.0469 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 706/1000\n",
      "\n",
      "Epoch 706: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.2682e-04 - accuracy: 1.0000 - val_loss: 0.0475 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 707/1000\n",
      "\n",
      "Epoch 707: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.3061e-04 - accuracy: 0.9998 - val_loss: 0.0514 - val_accuracy: 0.9931 - 6s/epoch - 30ms/step\n",
      "Epoch 708/1000\n",
      "\n",
      "Epoch 708: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.5406e-04 - accuracy: 0.9999 - val_loss: 0.0558 - val_accuracy: 0.9926 - 6s/epoch - 29ms/step\n",
      "Epoch 709/1000\n",
      "\n",
      "Epoch 709: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.0853e-04 - accuracy: 0.9999 - val_loss: 0.0532 - val_accuracy: 0.9932 - 6s/epoch - 29ms/step\n",
      "Epoch 710/1000\n",
      "\n",
      "Epoch 710: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.9628e-04 - accuracy: 0.9998 - val_loss: 0.0526 - val_accuracy: 0.9932 - 6s/epoch - 32ms/step\n",
      "Epoch 711/1000\n",
      "\n",
      "Epoch 711: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.9808e-04 - accuracy: 0.9999 - val_loss: 0.0549 - val_accuracy: 0.9926 - 6s/epoch - 32ms/step\n",
      "Epoch 712/1000\n",
      "\n",
      "Epoch 712: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.1099e-04 - accuracy: 0.9999 - val_loss: 0.0488 - val_accuracy: 0.9936 - 6s/epoch - 29ms/step\n",
      "Epoch 713/1000\n",
      "\n",
      "Epoch 713: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.6293e-04 - accuracy: 0.9998 - val_loss: 0.0490 - val_accuracy: 0.9938 - 6s/epoch - 29ms/step\n",
      "Epoch 714/1000\n",
      "\n",
      "Epoch 714: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.6079e-04 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 715/1000\n",
      "\n",
      "Epoch 715: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.8981e-04 - accuracy: 0.9999 - val_loss: 0.0466 - val_accuracy: 0.9941 - 6s/epoch - 29ms/step\n",
      "Epoch 716/1000\n",
      "\n",
      "Epoch 716: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.5166e-04 - accuracy: 1.0000 - val_loss: 0.0467 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 717/1000\n",
      "\n",
      "Epoch 717: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.0322e-04 - accuracy: 0.9998 - val_loss: 0.0514 - val_accuracy: 0.9935 - 6s/epoch - 29ms/step\n",
      "Epoch 718/1000\n",
      "\n",
      "Epoch 718: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.0707e-04 - accuracy: 0.9998 - val_loss: 0.0516 - val_accuracy: 0.9937 - 6s/epoch - 29ms/step\n",
      "Epoch 719/1000\n",
      "\n",
      "Epoch 719: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.9802e-05 - accuracy: 1.0000 - val_loss: 0.0509 - val_accuracy: 0.9934 - 6s/epoch - 29ms/step\n",
      "Epoch 720/1000\n",
      "\n",
      "Epoch 720: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.5495e-04 - accuracy: 0.9998 - val_loss: 0.0498 - val_accuracy: 0.9938 - 6s/epoch - 31ms/step\n",
      "Epoch 721/1000\n",
      "\n",
      "Epoch 721: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.9158e-04 - accuracy: 0.9999 - val_loss: 0.0515 - val_accuracy: 0.9937 - 6s/epoch - 30ms/step\n",
      "Epoch 722/1000\n",
      "\n",
      "Epoch 722: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.0455e-04 - accuracy: 0.9999 - val_loss: 0.0509 - val_accuracy: 0.9937 - 6s/epoch - 30ms/step\n",
      "Epoch 723/1000\n",
      "\n",
      "Epoch 723: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.2142e-04 - accuracy: 0.9999 - val_loss: 0.0533 - val_accuracy: 0.9931 - 6s/epoch - 30ms/step\n",
      "Epoch 724/1000\n",
      "\n",
      "Epoch 724: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.4202e-04 - accuracy: 0.9998 - val_loss: 0.0475 - val_accuracy: 0.9939 - 6s/epoch - 30ms/step\n",
      "Epoch 725/1000\n",
      "\n",
      "Epoch 725: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.0328e-05 - accuracy: 1.0000 - val_loss: 0.0463 - val_accuracy: 0.9939 - 6s/epoch - 30ms/step\n",
      "Epoch 726/1000\n",
      "\n",
      "Epoch 726: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.4424e-04 - accuracy: 0.9999 - val_loss: 0.0489 - val_accuracy: 0.9935 - 6s/epoch - 30ms/step\n",
      "Epoch 727/1000\n",
      "\n",
      "Epoch 727: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.5116e-04 - accuracy: 0.9998 - val_loss: 0.0442 - val_accuracy: 0.9937 - 6s/epoch - 29ms/step\n",
      "Epoch 728/1000\n",
      "\n",
      "Epoch 728: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.9807e-04 - accuracy: 0.9998 - val_loss: 0.0429 - val_accuracy: 0.9942 - 6s/epoch - 28ms/step\n",
      "Epoch 729/1000\n",
      "\n",
      "Epoch 729: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.1648e-04 - accuracy: 0.9998 - val_loss: 0.0521 - val_accuracy: 0.9930 - 6s/epoch - 28ms/step\n",
      "Epoch 730/1000\n",
      "\n",
      "Epoch 730: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.9837e-04 - accuracy: 0.9999 - val_loss: 0.0518 - val_accuracy: 0.9933 - 6s/epoch - 29ms/step\n",
      "Epoch 731/1000\n",
      "\n",
      "Epoch 731: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.1340e-04 - accuracy: 1.0000 - val_loss: 0.0478 - val_accuracy: 0.9938 - 6s/epoch - 29ms/step\n",
      "Epoch 732/1000\n",
      "\n",
      "Epoch 732: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.9865e-04 - accuracy: 0.9999 - val_loss: 0.0442 - val_accuracy: 0.9938 - 6s/epoch - 31ms/step\n",
      "Epoch 733/1000\n",
      "\n",
      "Epoch 733: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.5034e-04 - accuracy: 0.9999 - val_loss: 0.0464 - val_accuracy: 0.9941 - 6s/epoch - 31ms/step\n",
      "Epoch 734/1000\n",
      "\n",
      "Epoch 734: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.3994e-04 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 0.9941 - 6s/epoch - 31ms/step\n",
      "Epoch 735/1000\n",
      "\n",
      "Epoch 735: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.4553e-05 - accuracy: 1.0000 - val_loss: 0.0460 - val_accuracy: 0.9943 - 6s/epoch - 28ms/step\n",
      "Epoch 736/1000\n",
      "\n",
      "Epoch 736: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.0654e-04 - accuracy: 0.9997 - val_loss: 0.0512 - val_accuracy: 0.9939 - 6s/epoch - 28ms/step\n",
      "Epoch 737/1000\n",
      "\n",
      "Epoch 737: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.9312e-04 - accuracy: 0.9999 - val_loss: 0.0499 - val_accuracy: 0.9944 - 6s/epoch - 28ms/step\n",
      "Epoch 738/1000\n",
      "\n",
      "Epoch 738: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.2889e-04 - accuracy: 0.9999 - val_loss: 0.0485 - val_accuracy: 0.9941 - 6s/epoch - 30ms/step\n",
      "Epoch 739/1000\n",
      "\n",
      "Epoch 739: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.8380e-05 - accuracy: 1.0000 - val_loss: 0.0472 - val_accuracy: 0.9942 - 6s/epoch - 30ms/step\n",
      "Epoch 740/1000\n",
      "\n",
      "Epoch 740: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.5068e-04 - accuracy: 0.9999 - val_loss: 0.0479 - val_accuracy: 0.9942 - 6s/epoch - 30ms/step\n",
      "Epoch 741/1000\n",
      "\n",
      "Epoch 741: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.1575e-05 - accuracy: 1.0000 - val_loss: 0.0485 - val_accuracy: 0.9944 - 6s/epoch - 30ms/step\n",
      "Epoch 742/1000\n",
      "\n",
      "Epoch 742: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.8067e-05 - accuracy: 1.0000 - val_loss: 0.0471 - val_accuracy: 0.9945 - 6s/epoch - 32ms/step\n",
      "Epoch 743/1000\n",
      "\n",
      "Epoch 743: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.0368e-04 - accuracy: 1.0000 - val_loss: 0.0470 - val_accuracy: 0.9940 - 6s/epoch - 31ms/step\n",
      "Epoch 744/1000\n",
      "\n",
      "Epoch 744: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.1568e-04 - accuracy: 1.0000 - val_loss: 0.0509 - val_accuracy: 0.9940 - 6s/epoch - 31ms/step\n",
      "Epoch 745/1000\n",
      "\n",
      "Epoch 745: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.2926e-05 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 0.9945 - 6s/epoch - 29ms/step\n",
      "Epoch 746/1000\n",
      "\n",
      "Epoch 746: val_accuracy did not improve from 0.99518\n",
      "197/197 - 7s - loss: 5.1161e-05 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 0.9940 - 7s/epoch - 34ms/step\n",
      "Epoch 747/1000\n",
      "\n",
      "Epoch 747: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0630 - val_accuracy: 0.9923 - 6s/epoch - 30ms/step\n",
      "Epoch 748/1000\n",
      "\n",
      "Epoch 748: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.5902e-04 - accuracy: 0.9998 - val_loss: 0.0540 - val_accuracy: 0.9933 - 6s/epoch - 31ms/step\n",
      "Epoch 749/1000\n",
      "\n",
      "Epoch 749: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.5488e-04 - accuracy: 0.9998 - val_loss: 0.0523 - val_accuracy: 0.9935 - 6s/epoch - 31ms/step\n",
      "Epoch 750/1000\n",
      "\n",
      "Epoch 750: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.5086e-04 - accuracy: 0.9999 - val_loss: 0.0455 - val_accuracy: 0.9945 - 6s/epoch - 31ms/step\n",
      "Epoch 751/1000\n",
      "\n",
      "Epoch 751: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.1998e-04 - accuracy: 1.0000 - val_loss: 0.0479 - val_accuracy: 0.9943 - 6s/epoch - 31ms/step\n",
      "Epoch 752/1000\n",
      "\n",
      "Epoch 752: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.6070e-05 - accuracy: 1.0000 - val_loss: 0.0485 - val_accuracy: 0.9943 - 6s/epoch - 31ms/step\n",
      "Epoch 753/1000\n",
      "\n",
      "Epoch 753: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.8626e-04 - accuracy: 0.9999 - val_loss: 0.0506 - val_accuracy: 0.9935 - 6s/epoch - 31ms/step\n",
      "Epoch 754/1000\n",
      "\n",
      "Epoch 754: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0536 - val_accuracy: 0.9930 - 6s/epoch - 31ms/step\n",
      "Epoch 755/1000\n",
      "\n",
      "Epoch 755: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.3779e-04 - accuracy: 0.9999 - val_loss: 0.0491 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 756/1000\n",
      "\n",
      "Epoch 756: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.7602e-04 - accuracy: 0.9998 - val_loss: 0.0502 - val_accuracy: 0.9933 - 6s/epoch - 31ms/step\n",
      "Epoch 757/1000\n",
      "\n",
      "Epoch 757: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.4775e-04 - accuracy: 0.9997 - val_loss: 0.0495 - val_accuracy: 0.9935 - 6s/epoch - 31ms/step\n",
      "Epoch 758/1000\n",
      "\n",
      "Epoch 758: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.2169e-04 - accuracy: 0.9999 - val_loss: 0.0534 - val_accuracy: 0.9932 - 6s/epoch - 31ms/step\n",
      "Epoch 759/1000\n",
      "\n",
      "Epoch 759: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.2695e-04 - accuracy: 0.9999 - val_loss: 0.0495 - val_accuracy: 0.9936 - 6s/epoch - 31ms/step\n",
      "Epoch 760/1000\n",
      "\n",
      "Epoch 760: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.0227e-04 - accuracy: 0.9999 - val_loss: 0.0553 - val_accuracy: 0.9930 - 6s/epoch - 31ms/step\n",
      "Epoch 761/1000\n",
      "\n",
      "Epoch 761: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.9342e-04 - accuracy: 0.9998 - val_loss: 0.0501 - val_accuracy: 0.9934 - 6s/epoch - 31ms/step\n",
      "Epoch 762/1000\n",
      "\n",
      "Epoch 762: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.2489e-04 - accuracy: 0.9999 - val_loss: 0.0478 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 763/1000\n",
      "\n",
      "Epoch 763: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.0675e-04 - accuracy: 0.9998 - val_loss: 0.0503 - val_accuracy: 0.9937 - 6s/epoch - 31ms/step\n",
      "Epoch 764/1000\n",
      "\n",
      "Epoch 764: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.3291e-04 - accuracy: 0.9999 - val_loss: 0.0500 - val_accuracy: 0.9942 - 6s/epoch - 31ms/step\n",
      "Epoch 765/1000\n",
      "\n",
      "Epoch 765: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.4753e-04 - accuracy: 1.0000 - val_loss: 0.0563 - val_accuracy: 0.9935 - 6s/epoch - 31ms/step\n",
      "Epoch 766/1000\n",
      "\n",
      "Epoch 766: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.5268e-04 - accuracy: 1.0000 - val_loss: 0.0512 - val_accuracy: 0.9936 - 6s/epoch - 31ms/step\n",
      "Epoch 767/1000\n",
      "\n",
      "Epoch 767: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.0783e-04 - accuracy: 0.9998 - val_loss: 0.0465 - val_accuracy: 0.9938 - 6s/epoch - 31ms/step\n",
      "Epoch 768/1000\n",
      "\n",
      "Epoch 768: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.2555e-04 - accuracy: 0.9998 - val_loss: 0.0456 - val_accuracy: 0.9941 - 6s/epoch - 32ms/step\n",
      "Epoch 769/1000\n",
      "\n",
      "Epoch 769: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.3649e-05 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.9942 - 6s/epoch - 30ms/step\n",
      "Epoch 770/1000\n",
      "\n",
      "Epoch 770: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.7034e-04 - accuracy: 0.9998 - val_loss: 0.0477 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 771/1000\n",
      "\n",
      "Epoch 771: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.7753e-05 - accuracy: 1.0000 - val_loss: 0.0458 - val_accuracy: 0.9941 - 6s/epoch - 33ms/step\n",
      "Epoch 772/1000\n",
      "\n",
      "Epoch 772: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.7211e-04 - accuracy: 0.9998 - val_loss: 0.0496 - val_accuracy: 0.9937 - 6s/epoch - 32ms/step\n",
      "Epoch 773/1000\n",
      "\n",
      "Epoch 773: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.3138e-04 - accuracy: 0.9999 - val_loss: 0.0469 - val_accuracy: 0.9940 - 6s/epoch - 32ms/step\n",
      "Epoch 774/1000\n",
      "\n",
      "Epoch 774: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.1450e-04 - accuracy: 0.9999 - val_loss: 0.0428 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 775/1000\n",
      "\n",
      "Epoch 775: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.9361e-04 - accuracy: 0.9999 - val_loss: 0.0479 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 776/1000\n",
      "\n",
      "Epoch 776: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.6544e-04 - accuracy: 0.9999 - val_loss: 0.0517 - val_accuracy: 0.9929 - 6s/epoch - 32ms/step\n",
      "Epoch 777/1000\n",
      "\n",
      "Epoch 777: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.8256e-05 - accuracy: 1.0000 - val_loss: 0.0502 - val_accuracy: 0.9930 - 6s/epoch - 32ms/step\n",
      "Epoch 778/1000\n",
      "\n",
      "Epoch 778: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.7675e-04 - accuracy: 0.9999 - val_loss: 0.0499 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 779/1000\n",
      "\n",
      "Epoch 779: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.4676e-05 - accuracy: 1.0000 - val_loss: 0.0508 - val_accuracy: 0.9935 - 6s/epoch - 31ms/step\n",
      "Epoch 780/1000\n",
      "\n",
      "Epoch 780: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.0683e-04 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 781/1000\n",
      "\n",
      "Epoch 781: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.4277e-04 - accuracy: 0.9998 - val_loss: 0.0567 - val_accuracy: 0.9933 - 6s/epoch - 32ms/step\n",
      "Epoch 782/1000\n",
      "\n",
      "Epoch 782: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.6062e-04 - accuracy: 0.9999 - val_loss: 0.0548 - val_accuracy: 0.9926 - 6s/epoch - 32ms/step\n",
      "Epoch 783/1000\n",
      "\n",
      "Epoch 783: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.5339e-04 - accuracy: 0.9999 - val_loss: 0.0538 - val_accuracy: 0.9933 - 6s/epoch - 32ms/step\n",
      "Epoch 784/1000\n",
      "\n",
      "Epoch 784: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.2574e-04 - accuracy: 0.9999 - val_loss: 0.0517 - val_accuracy: 0.9937 - 6s/epoch - 32ms/step\n",
      "Epoch 785/1000\n",
      "\n",
      "Epoch 785: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.1728e-04 - accuracy: 0.9999 - val_loss: 0.0511 - val_accuracy: 0.9941 - 6s/epoch - 31ms/step\n",
      "Epoch 786/1000\n",
      "\n",
      "Epoch 786: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.0384e-04 - accuracy: 0.9998 - val_loss: 0.0519 - val_accuracy: 0.9927 - 6s/epoch - 32ms/step\n",
      "Epoch 787/1000\n",
      "\n",
      "Epoch 787: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.4305e-04 - accuracy: 0.9999 - val_loss: 0.0566 - val_accuracy: 0.9932 - 6s/epoch - 32ms/step\n",
      "Epoch 788/1000\n",
      "\n",
      "Epoch 788: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.5372e-04 - accuracy: 0.9999 - val_loss: 0.0561 - val_accuracy: 0.9930 - 6s/epoch - 32ms/step\n",
      "Epoch 789/1000\n",
      "\n",
      "Epoch 789: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0487 - val_accuracy: 0.9937 - 6s/epoch - 32ms/step\n",
      "Epoch 790/1000\n",
      "\n",
      "Epoch 790: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.2457e-04 - accuracy: 0.9998 - val_loss: 0.0524 - val_accuracy: 0.9937 - 6s/epoch - 32ms/step\n",
      "Epoch 791/1000\n",
      "\n",
      "Epoch 791: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.0819e-04 - accuracy: 0.9999 - val_loss: 0.0507 - val_accuracy: 0.9932 - 6s/epoch - 32ms/step\n",
      "Epoch 792/1000\n",
      "\n",
      "Epoch 792: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.5440e-04 - accuracy: 1.0000 - val_loss: 0.0466 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 793/1000\n",
      "\n",
      "Epoch 793: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.0518e-04 - accuracy: 0.9999 - val_loss: 0.0488 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 794/1000\n",
      "\n",
      "Epoch 794: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.4309e-05 - accuracy: 1.0000 - val_loss: 0.0518 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 795/1000\n",
      "\n",
      "Epoch 795: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.8264e-04 - accuracy: 0.9999 - val_loss: 0.0498 - val_accuracy: 0.9937 - 6s/epoch - 32ms/step\n",
      "Epoch 796/1000\n",
      "\n",
      "Epoch 796: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.5778e-05 - accuracy: 1.0000 - val_loss: 0.0512 - val_accuracy: 0.9934 - 6s/epoch - 32ms/step\n",
      "Epoch 797/1000\n",
      "\n",
      "Epoch 797: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.8618e-05 - accuracy: 1.0000 - val_loss: 0.0516 - val_accuracy: 0.9933 - 6s/epoch - 32ms/step\n",
      "Epoch 798/1000\n",
      "\n",
      "Epoch 798: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.0106e-04 - accuracy: 0.9999 - val_loss: 0.0496 - val_accuracy: 0.9937 - 6s/epoch - 32ms/step\n",
      "Epoch 799/1000\n",
      "\n",
      "Epoch 799: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.6626e-04 - accuracy: 0.9999 - val_loss: 0.0506 - val_accuracy: 0.9938 - 6s/epoch - 32ms/step\n",
      "Epoch 800/1000\n",
      "\n",
      "Epoch 800: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.1835e-05 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 801/1000\n",
      "\n",
      "Epoch 801: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0486 - val_accuracy: 0.9933 - 6s/epoch - 32ms/step\n",
      "Epoch 802/1000\n",
      "\n",
      "Epoch 802: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.5699e-04 - accuracy: 0.9998 - val_loss: 0.0491 - val_accuracy: 0.9930 - 6s/epoch - 32ms/step\n",
      "Epoch 803/1000\n",
      "\n",
      "Epoch 803: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.6900e-04 - accuracy: 0.9999 - val_loss: 0.0486 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 804/1000\n",
      "\n",
      "Epoch 804: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.7571e-04 - accuracy: 0.9998 - val_loss: 0.0509 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 805/1000\n",
      "\n",
      "Epoch 805: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.0565e-04 - accuracy: 0.9998 - val_loss: 0.0517 - val_accuracy: 0.9933 - 6s/epoch - 32ms/step\n",
      "Epoch 806/1000\n",
      "\n",
      "Epoch 806: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.6949e-04 - accuracy: 0.9999 - val_loss: 0.0559 - val_accuracy: 0.9930 - 6s/epoch - 32ms/step\n",
      "Epoch 807/1000\n",
      "\n",
      "Epoch 807: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.3490e-04 - accuracy: 0.9998 - val_loss: 0.0498 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 808/1000\n",
      "\n",
      "Epoch 808: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.8739e-04 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 809/1000\n",
      "\n",
      "Epoch 809: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.4096e-05 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 0.9937 - 6s/epoch - 32ms/step\n",
      "Epoch 810/1000\n",
      "\n",
      "Epoch 810: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.5297e-04 - accuracy: 0.9998 - val_loss: 0.0554 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 811/1000\n",
      "\n",
      "Epoch 811: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.5897e-05 - accuracy: 1.0000 - val_loss: 0.0558 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 812/1000\n",
      "\n",
      "Epoch 812: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.7435e-05 - accuracy: 1.0000 - val_loss: 0.0554 - val_accuracy: 0.9933 - 6s/epoch - 32ms/step\n",
      "Epoch 813/1000\n",
      "\n",
      "Epoch 813: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.1185e-04 - accuracy: 0.9999 - val_loss: 0.0584 - val_accuracy: 0.9930 - 6s/epoch - 32ms/step\n",
      "Epoch 814/1000\n",
      "\n",
      "Epoch 814: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.3329e-04 - accuracy: 0.9999 - val_loss: 0.0543 - val_accuracy: 0.9933 - 6s/epoch - 32ms/step\n",
      "Epoch 815/1000\n",
      "\n",
      "Epoch 815: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.4708e-04 - accuracy: 0.9999 - val_loss: 0.0547 - val_accuracy: 0.9927 - 6s/epoch - 32ms/step\n",
      "Epoch 816/1000\n",
      "\n",
      "Epoch 816: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.0026e-04 - accuracy: 1.0000 - val_loss: 0.0523 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 817/1000\n",
      "\n",
      "Epoch 817: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.0567e-05 - accuracy: 1.0000 - val_loss: 0.0514 - val_accuracy: 0.9937 - 6s/epoch - 32ms/step\n",
      "Epoch 818/1000\n",
      "\n",
      "Epoch 818: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0525 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 819/1000\n",
      "\n",
      "Epoch 819: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.7090e-04 - accuracy: 0.9999 - val_loss: 0.0532 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 820/1000\n",
      "\n",
      "Epoch 820: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.3565e-04 - accuracy: 0.9998 - val_loss: 0.0571 - val_accuracy: 0.9929 - 6s/epoch - 32ms/step\n",
      "Epoch 821/1000\n",
      "\n",
      "Epoch 821: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.1206e-04 - accuracy: 0.9998 - val_loss: 0.0522 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 822/1000\n",
      "\n",
      "Epoch 822: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.4105e-04 - accuracy: 0.9999 - val_loss: 0.0478 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 823/1000\n",
      "\n",
      "Epoch 823: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.8797e-04 - accuracy: 0.9999 - val_loss: 0.0474 - val_accuracy: 0.9940 - 6s/epoch - 32ms/step\n",
      "Epoch 824/1000\n",
      "\n",
      "Epoch 824: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.9974e-04 - accuracy: 1.0000 - val_loss: 0.0471 - val_accuracy: 0.9941 - 6s/epoch - 32ms/step\n",
      "Epoch 825/1000\n",
      "\n",
      "Epoch 825: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.7559e-04 - accuracy: 1.0000 - val_loss: 0.0519 - val_accuracy: 0.9938 - 6s/epoch - 32ms/step\n",
      "Epoch 826/1000\n",
      "\n",
      "Epoch 826: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.7319e-04 - accuracy: 0.9999 - val_loss: 0.0564 - val_accuracy: 0.9929 - 6s/epoch - 32ms/step\n",
      "Epoch 827/1000\n",
      "\n",
      "Epoch 827: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.4848e-05 - accuracy: 1.0000 - val_loss: 0.0525 - val_accuracy: 0.9932 - 6s/epoch - 32ms/step\n",
      "Epoch 828/1000\n",
      "\n",
      "Epoch 828: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.2461e-04 - accuracy: 1.0000 - val_loss: 0.0489 - val_accuracy: 0.9938 - 6s/epoch - 32ms/step\n",
      "Epoch 829/1000\n",
      "\n",
      "Epoch 829: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.5058e-05 - accuracy: 1.0000 - val_loss: 0.0478 - val_accuracy: 0.9943 - 6s/epoch - 32ms/step\n",
      "Epoch 830/1000\n",
      "\n",
      "Epoch 830: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.0088e-05 - accuracy: 1.0000 - val_loss: 0.0476 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 831/1000\n",
      "\n",
      "Epoch 831: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.3160e-05 - accuracy: 1.0000 - val_loss: 0.0484 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 832/1000\n",
      "\n",
      "Epoch 832: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.0377e-04 - accuracy: 1.0000 - val_loss: 0.0422 - val_accuracy: 0.9945 - 6s/epoch - 32ms/step\n",
      "Epoch 833/1000\n",
      "\n",
      "Epoch 833: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.1819e-05 - accuracy: 1.0000 - val_loss: 0.0444 - val_accuracy: 0.9947 - 6s/epoch - 32ms/step\n",
      "Epoch 834/1000\n",
      "\n",
      "Epoch 834: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.1166e-04 - accuracy: 1.0000 - val_loss: 0.0474 - val_accuracy: 0.9941 - 6s/epoch - 32ms/step\n",
      "Epoch 835/1000\n",
      "\n",
      "Epoch 835: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.3615e-04 - accuracy: 0.9997 - val_loss: 0.0464 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 836/1000\n",
      "\n",
      "Epoch 836: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.7501e-04 - accuracy: 1.0000 - val_loss: 0.0451 - val_accuracy: 0.9944 - 6s/epoch - 32ms/step\n",
      "Epoch 837/1000\n",
      "\n",
      "Epoch 837: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.1115e-04 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.9949 - 6s/epoch - 32ms/step\n",
      "Epoch 838/1000\n",
      "\n",
      "Epoch 838: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.2292e-05 - accuracy: 1.0000 - val_loss: 0.0453 - val_accuracy: 0.9950 - 6s/epoch - 32ms/step\n",
      "Epoch 839/1000\n",
      "\n",
      "Epoch 839: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.3369e-04 - accuracy: 0.9999 - val_loss: 0.0494 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 840/1000\n",
      "\n",
      "Epoch 840: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.4358e-04 - accuracy: 0.9998 - val_loss: 0.0421 - val_accuracy: 0.9945 - 6s/epoch - 32ms/step\n",
      "Epoch 841/1000\n",
      "\n",
      "Epoch 841: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.2995e-05 - accuracy: 1.0000 - val_loss: 0.0450 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 842/1000\n",
      "\n",
      "Epoch 842: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.4027e-04 - accuracy: 0.9999 - val_loss: 0.0505 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 843/1000\n",
      "\n",
      "Epoch 843: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.0760e-05 - accuracy: 1.0000 - val_loss: 0.0490 - val_accuracy: 0.9941 - 6s/epoch - 32ms/step\n",
      "Epoch 844/1000\n",
      "\n",
      "Epoch 844: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.8584e-05 - accuracy: 1.0000 - val_loss: 0.0491 - val_accuracy: 0.9943 - 6s/epoch - 32ms/step\n",
      "Epoch 845/1000\n",
      "\n",
      "Epoch 845: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.0795e-05 - accuracy: 1.0000 - val_loss: 0.0488 - val_accuracy: 0.9943 - 6s/epoch - 32ms/step\n",
      "Epoch 846/1000\n",
      "\n",
      "Epoch 846: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.0618e-04 - accuracy: 0.9998 - val_loss: 0.0486 - val_accuracy: 0.9946 - 6s/epoch - 32ms/step\n",
      "Epoch 847/1000\n",
      "\n",
      "Epoch 847: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.3234e-05 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 848/1000\n",
      "\n",
      "Epoch 848: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.4029e-04 - accuracy: 1.0000 - val_loss: 0.0499 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 849/1000\n",
      "\n",
      "Epoch 849: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.6782e-04 - accuracy: 0.9999 - val_loss: 0.0532 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 850/1000\n",
      "\n",
      "Epoch 850: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.1368e-05 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 851/1000\n",
      "\n",
      "Epoch 851: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.9344e-04 - accuracy: 0.9999 - val_loss: 0.0546 - val_accuracy: 0.9938 - 6s/epoch - 32ms/step\n",
      "Epoch 852/1000\n",
      "\n",
      "Epoch 852: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.1011e-04 - accuracy: 0.9999 - val_loss: 0.0536 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 853/1000\n",
      "\n",
      "Epoch 853: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.6808e-04 - accuracy: 0.9999 - val_loss: 0.0490 - val_accuracy: 0.9946 - 6s/epoch - 32ms/step\n",
      "Epoch 854/1000\n",
      "\n",
      "Epoch 854: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.1214e-04 - accuracy: 1.0000 - val_loss: 0.0490 - val_accuracy: 0.9941 - 6s/epoch - 32ms/step\n",
      "Epoch 855/1000\n",
      "\n",
      "Epoch 855: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.5304e-04 - accuracy: 1.0000 - val_loss: 0.0472 - val_accuracy: 0.9943 - 6s/epoch - 32ms/step\n",
      "Epoch 856/1000\n",
      "\n",
      "Epoch 856: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.3075e-04 - accuracy: 1.0000 - val_loss: 0.0481 - val_accuracy: 0.9945 - 6s/epoch - 32ms/step\n",
      "Epoch 857/1000\n",
      "\n",
      "Epoch 857: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.7296e-05 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 858/1000\n",
      "\n",
      "Epoch 858: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.8767e-04 - accuracy: 0.9999 - val_loss: 0.0538 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 859/1000\n",
      "\n",
      "Epoch 859: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.5508e-04 - accuracy: 0.9998 - val_loss: 0.0509 - val_accuracy: 0.9937 - 6s/epoch - 32ms/step\n",
      "Epoch 860/1000\n",
      "\n",
      "Epoch 860: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.2730e-04 - accuracy: 0.9999 - val_loss: 0.0492 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 861/1000\n",
      "\n",
      "Epoch 861: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.9326e-04 - accuracy: 0.9999 - val_loss: 0.0501 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 862/1000\n",
      "\n",
      "Epoch 862: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.8380e-04 - accuracy: 0.9998 - val_loss: 0.0533 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 863/1000\n",
      "\n",
      "Epoch 863: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.6763e-04 - accuracy: 1.0000 - val_loss: 0.0512 - val_accuracy: 0.9938 - 6s/epoch - 32ms/step\n",
      "Epoch 864/1000\n",
      "\n",
      "Epoch 864: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0482 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 865/1000\n",
      "\n",
      "Epoch 865: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.9846e-05 - accuracy: 1.0000 - val_loss: 0.0472 - val_accuracy: 0.9945 - 6s/epoch - 32ms/step\n",
      "Epoch 866/1000\n",
      "\n",
      "Epoch 866: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.6654e-04 - accuracy: 0.9999 - val_loss: 0.0535 - val_accuracy: 0.9933 - 6s/epoch - 32ms/step\n",
      "Epoch 867/1000\n",
      "\n",
      "Epoch 867: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.2922e-04 - accuracy: 0.9999 - val_loss: 0.0504 - val_accuracy: 0.9938 - 6s/epoch - 32ms/step\n",
      "Epoch 868/1000\n",
      "\n",
      "Epoch 868: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.4788e-04 - accuracy: 0.9999 - val_loss: 0.0511 - val_accuracy: 0.9938 - 6s/epoch - 32ms/step\n",
      "Epoch 869/1000\n",
      "\n",
      "Epoch 869: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.8277e-04 - accuracy: 0.9999 - val_loss: 0.0516 - val_accuracy: 0.9933 - 6s/epoch - 32ms/step\n",
      "Epoch 870/1000\n",
      "\n",
      "Epoch 870: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.3572e-05 - accuracy: 1.0000 - val_loss: 0.0526 - val_accuracy: 0.9934 - 6s/epoch - 32ms/step\n",
      "Epoch 871/1000\n",
      "\n",
      "Epoch 871: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.2671e-04 - accuracy: 0.9999 - val_loss: 0.0533 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 872/1000\n",
      "\n",
      "Epoch 872: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.5608e-04 - accuracy: 0.9998 - val_loss: 0.0562 - val_accuracy: 0.9929 - 6s/epoch - 32ms/step\n",
      "Epoch 873/1000\n",
      "\n",
      "Epoch 873: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.6963e-04 - accuracy: 0.9999 - val_loss: 0.0566 - val_accuracy: 0.9932 - 6s/epoch - 32ms/step\n",
      "Epoch 874/1000\n",
      "\n",
      "Epoch 874: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.1263e-04 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 875/1000\n",
      "\n",
      "Epoch 875: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.1233e-05 - accuracy: 1.0000 - val_loss: 0.0565 - val_accuracy: 0.9933 - 6s/epoch - 32ms/step\n",
      "Epoch 876/1000\n",
      "\n",
      "Epoch 876: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0530 - val_accuracy: 0.9934 - 6s/epoch - 32ms/step\n",
      "Epoch 877/1000\n",
      "\n",
      "Epoch 877: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.1463e-04 - accuracy: 0.9999 - val_loss: 0.0533 - val_accuracy: 0.9931 - 6s/epoch - 32ms/step\n",
      "Epoch 878/1000\n",
      "\n",
      "Epoch 878: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.3404e-04 - accuracy: 0.9999 - val_loss: 0.0521 - val_accuracy: 0.9933 - 6s/epoch - 32ms/step\n",
      "Epoch 879/1000\n",
      "\n",
      "Epoch 879: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.6790e-04 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 880/1000\n",
      "\n",
      "Epoch 880: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.3627e-05 - accuracy: 1.0000 - val_loss: 0.0525 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 881/1000\n",
      "\n",
      "Epoch 881: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.3782e-05 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 882/1000\n",
      "\n",
      "Epoch 882: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.4084e-05 - accuracy: 1.0000 - val_loss: 0.0532 - val_accuracy: 0.9937 - 6s/epoch - 32ms/step\n",
      "Epoch 883/1000\n",
      "\n",
      "Epoch 883: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.3169e-04 - accuracy: 0.9999 - val_loss: 0.0506 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 884/1000\n",
      "\n",
      "Epoch 884: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.2987e-04 - accuracy: 0.9999 - val_loss: 0.0538 - val_accuracy: 0.9933 - 6s/epoch - 32ms/step\n",
      "Epoch 885/1000\n",
      "\n",
      "Epoch 885: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.7782e-04 - accuracy: 0.9997 - val_loss: 0.0626 - val_accuracy: 0.9928 - 6s/epoch - 31ms/step\n",
      "Epoch 886/1000\n",
      "\n",
      "Epoch 886: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.0181e-04 - accuracy: 0.9999 - val_loss: 0.0534 - val_accuracy: 0.9941 - 6s/epoch - 32ms/step\n",
      "Epoch 887/1000\n",
      "\n",
      "Epoch 887: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.8073e-05 - accuracy: 1.0000 - val_loss: 0.0540 - val_accuracy: 0.9938 - 6s/epoch - 32ms/step\n",
      "Epoch 888/1000\n",
      "\n",
      "Epoch 888: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.2269e-04 - accuracy: 0.9999 - val_loss: 0.0552 - val_accuracy: 0.9935 - 6s/epoch - 32ms/step\n",
      "Epoch 889/1000\n",
      "\n",
      "Epoch 889: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.4875e-04 - accuracy: 0.9998 - val_loss: 0.0528 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 890/1000\n",
      "\n",
      "Epoch 890: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.6052e-04 - accuracy: 0.9999 - val_loss: 0.0527 - val_accuracy: 0.9934 - 6s/epoch - 32ms/step\n",
      "Epoch 891/1000\n",
      "\n",
      "Epoch 891: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.0518e-04 - accuracy: 0.9999 - val_loss: 0.0507 - val_accuracy: 0.9934 - 6s/epoch - 32ms/step\n",
      "Epoch 892/1000\n",
      "\n",
      "Epoch 892: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.6933e-04 - accuracy: 0.9998 - val_loss: 0.0466 - val_accuracy: 0.9943 - 6s/epoch - 32ms/step\n",
      "Epoch 893/1000\n",
      "\n",
      "Epoch 893: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.7299e-04 - accuracy: 0.9999 - val_loss: 0.0511 - val_accuracy: 0.9933 - 6s/epoch - 32ms/step\n",
      "Epoch 894/1000\n",
      "\n",
      "Epoch 894: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.5539e-04 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 895/1000\n",
      "\n",
      "Epoch 895: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.0513e-04 - accuracy: 1.0000 - val_loss: 0.0519 - val_accuracy: 0.9938 - 6s/epoch - 32ms/step\n",
      "Epoch 896/1000\n",
      "\n",
      "Epoch 896: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.8099e-05 - accuracy: 1.0000 - val_loss: 0.0508 - val_accuracy: 0.9940 - 6s/epoch - 32ms/step\n",
      "Epoch 897/1000\n",
      "\n",
      "Epoch 897: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.0511e-05 - accuracy: 1.0000 - val_loss: 0.0517 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 898/1000\n",
      "\n",
      "Epoch 898: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.3186e-04 - accuracy: 0.9999 - val_loss: 0.0533 - val_accuracy: 0.9937 - 6s/epoch - 31ms/step\n",
      "Epoch 899/1000\n",
      "\n",
      "Epoch 899: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.5816e-05 - accuracy: 1.0000 - val_loss: 0.0529 - val_accuracy: 0.9936 - 6s/epoch - 31ms/step\n",
      "Epoch 900/1000\n",
      "\n",
      "Epoch 900: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.3740e-04 - accuracy: 0.9998 - val_loss: 0.0477 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 901/1000\n",
      "\n",
      "Epoch 901: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.3882e-05 - accuracy: 1.0000 - val_loss: 0.0489 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 902/1000\n",
      "\n",
      "Epoch 902: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.2765e-04 - accuracy: 0.9999 - val_loss: 0.0493 - val_accuracy: 0.9937 - 6s/epoch - 31ms/step\n",
      "Epoch 903/1000\n",
      "\n",
      "Epoch 903: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.8023e-04 - accuracy: 0.9999 - val_loss: 0.0501 - val_accuracy: 0.9942 - 6s/epoch - 28ms/step\n",
      "Epoch 904/1000\n",
      "\n",
      "Epoch 904: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.9361e-04 - accuracy: 0.9999 - val_loss: 0.0515 - val_accuracy: 0.9936 - 6s/epoch - 29ms/step\n",
      "Epoch 905/1000\n",
      "\n",
      "Epoch 905: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.1970e-05 - accuracy: 1.0000 - val_loss: 0.0482 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 906/1000\n",
      "\n",
      "Epoch 906: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.6796e-05 - accuracy: 1.0000 - val_loss: 0.0484 - val_accuracy: 0.9937 - 6s/epoch - 30ms/step\n",
      "Epoch 907/1000\n",
      "\n",
      "Epoch 907: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.5867e-05 - accuracy: 0.9999 - val_loss: 0.0451 - val_accuracy: 0.9941 - 6s/epoch - 29ms/step\n",
      "Epoch 908/1000\n",
      "\n",
      "Epoch 908: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.6614e-06 - accuracy: 1.0000 - val_loss: 0.0455 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 909/1000\n",
      "\n",
      "Epoch 909: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.8460e-05 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 0.9946 - 6s/epoch - 29ms/step\n",
      "Epoch 910/1000\n",
      "\n",
      "Epoch 910: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.3119e-04 - accuracy: 0.9999 - val_loss: 0.0501 - val_accuracy: 0.9929 - 6s/epoch - 29ms/step\n",
      "Epoch 911/1000\n",
      "\n",
      "Epoch 911: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.7275e-05 - accuracy: 1.0000 - val_loss: 0.0492 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 912/1000\n",
      "\n",
      "Epoch 912: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.8082e-04 - accuracy: 0.9999 - val_loss: 0.0515 - val_accuracy: 0.9936 - 6s/epoch - 30ms/step\n",
      "Epoch 913/1000\n",
      "\n",
      "Epoch 913: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.5385e-05 - accuracy: 1.0000 - val_loss: 0.0526 - val_accuracy: 0.9937 - 6s/epoch - 31ms/step\n",
      "Epoch 914/1000\n",
      "\n",
      "Epoch 914: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.5488e-04 - accuracy: 0.9998 - val_loss: 0.0460 - val_accuracy: 0.9942 - 6s/epoch - 31ms/step\n",
      "Epoch 915/1000\n",
      "\n",
      "Epoch 915: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.2133e-04 - accuracy: 1.0000 - val_loss: 0.0492 - val_accuracy: 0.9940 - 6s/epoch - 31ms/step\n",
      "Epoch 916/1000\n",
      "\n",
      "Epoch 916: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.9040e-04 - accuracy: 0.9998 - val_loss: 0.0553 - val_accuracy: 0.9936 - 6s/epoch - 31ms/step\n",
      "Epoch 917/1000\n",
      "\n",
      "Epoch 917: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.6643e-04 - accuracy: 0.9998 - val_loss: 0.0564 - val_accuracy: 0.9933 - 6s/epoch - 31ms/step\n",
      "Epoch 918/1000\n",
      "\n",
      "Epoch 918: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.7081e-04 - accuracy: 0.9999 - val_loss: 0.0519 - val_accuracy: 0.9936 - 6s/epoch - 31ms/step\n",
      "Epoch 919/1000\n",
      "\n",
      "Epoch 919: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.7893e-04 - accuracy: 1.0000 - val_loss: 0.0550 - val_accuracy: 0.9934 - 6s/epoch - 31ms/step\n",
      "Epoch 920/1000\n",
      "\n",
      "Epoch 920: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.7656e-05 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 0.9940 - 6s/epoch - 31ms/step\n",
      "Epoch 921/1000\n",
      "\n",
      "Epoch 921: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.7011e-05 - accuracy: 1.0000 - val_loss: 0.0512 - val_accuracy: 0.9937 - 6s/epoch - 31ms/step\n",
      "Epoch 922/1000\n",
      "\n",
      "Epoch 922: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.7007e-05 - accuracy: 1.0000 - val_loss: 0.0505 - val_accuracy: 0.9940 - 6s/epoch - 31ms/step\n",
      "Epoch 923/1000\n",
      "\n",
      "Epoch 923: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.4974e-04 - accuracy: 0.9998 - val_loss: 0.0530 - val_accuracy: 0.9941 - 6s/epoch - 31ms/step\n",
      "Epoch 924/1000\n",
      "\n",
      "Epoch 924: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.7700e-04 - accuracy: 0.9999 - val_loss: 0.0546 - val_accuracy: 0.9938 - 6s/epoch - 31ms/step\n",
      "Epoch 925/1000\n",
      "\n",
      "Epoch 925: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.6821e-04 - accuracy: 1.0000 - val_loss: 0.0566 - val_accuracy: 0.9937 - 6s/epoch - 31ms/step\n",
      "Epoch 926/1000\n",
      "\n",
      "Epoch 926: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.7333e-04 - accuracy: 0.9999 - val_loss: 0.0576 - val_accuracy: 0.9932 - 6s/epoch - 30ms/step\n",
      "Epoch 927/1000\n",
      "\n",
      "Epoch 927: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.6617e-04 - accuracy: 0.9998 - val_loss: 0.0552 - val_accuracy: 0.9936 - 6s/epoch - 30ms/step\n",
      "Epoch 928/1000\n",
      "\n",
      "Epoch 928: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.2952e-05 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 0.9939 - 6s/epoch - 30ms/step\n",
      "Epoch 929/1000\n",
      "\n",
      "Epoch 929: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.9513e-04 - accuracy: 0.9999 - val_loss: 0.0506 - val_accuracy: 0.9939 - 6s/epoch - 28ms/step\n",
      "Epoch 930/1000\n",
      "\n",
      "Epoch 930: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.0443e-05 - accuracy: 1.0000 - val_loss: 0.0481 - val_accuracy: 0.9943 - 6s/epoch - 28ms/step\n",
      "Epoch 931/1000\n",
      "\n",
      "Epoch 931: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.0675e-04 - accuracy: 0.9998 - val_loss: 0.0495 - val_accuracy: 0.9940 - 6s/epoch - 28ms/step\n",
      "Epoch 932/1000\n",
      "\n",
      "Epoch 932: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.3359e-04 - accuracy: 0.9999 - val_loss: 0.0489 - val_accuracy: 0.9940 - 6s/epoch - 29ms/step\n",
      "Epoch 933/1000\n",
      "\n",
      "Epoch 933: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.6357e-04 - accuracy: 0.9998 - val_loss: 0.0476 - val_accuracy: 0.9940 - 6s/epoch - 28ms/step\n",
      "Epoch 934/1000\n",
      "\n",
      "Epoch 934: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.7770e-04 - accuracy: 0.9998 - val_loss: 0.0471 - val_accuracy: 0.9939 - 6s/epoch - 29ms/step\n",
      "Epoch 935/1000\n",
      "\n",
      "Epoch 935: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.9299e-05 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 0.9940 - 6s/epoch - 28ms/step\n",
      "Epoch 936/1000\n",
      "\n",
      "Epoch 936: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.8524e-05 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 0.9941 - 6s/epoch - 29ms/step\n",
      "Epoch 937/1000\n",
      "\n",
      "Epoch 937: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.2412e-05 - accuracy: 1.0000 - val_loss: 0.0461 - val_accuracy: 0.9942 - 6s/epoch - 28ms/step\n",
      "Epoch 938/1000\n",
      "\n",
      "Epoch 938: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.3739e-05 - accuracy: 1.0000 - val_loss: 0.0451 - val_accuracy: 0.9945 - 6s/epoch - 28ms/step\n",
      "Epoch 939/1000\n",
      "\n",
      "Epoch 939: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.5627e-04 - accuracy: 0.9999 - val_loss: 0.0508 - val_accuracy: 0.9939 - 6s/epoch - 30ms/step\n",
      "Epoch 940/1000\n",
      "\n",
      "Epoch 940: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.5909e-04 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9934 - 6s/epoch - 30ms/step\n",
      "Epoch 941/1000\n",
      "\n",
      "Epoch 941: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.1265e-04 - accuracy: 0.9998 - val_loss: 0.0538 - val_accuracy: 0.9934 - 6s/epoch - 30ms/step\n",
      "Epoch 942/1000\n",
      "\n",
      "Epoch 942: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.2974e-05 - accuracy: 1.0000 - val_loss: 0.0509 - val_accuracy: 0.9933 - 6s/epoch - 30ms/step\n",
      "Epoch 943/1000\n",
      "\n",
      "Epoch 943: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.3227e-04 - accuracy: 0.9998 - val_loss: 0.0541 - val_accuracy: 0.9935 - 6s/epoch - 30ms/step\n",
      "Epoch 944/1000\n",
      "\n",
      "Epoch 944: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.6610e-04 - accuracy: 0.9999 - val_loss: 0.0486 - val_accuracy: 0.9941 - 6s/epoch - 30ms/step\n",
      "Epoch 945/1000\n",
      "\n",
      "Epoch 945: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.7214e-05 - accuracy: 1.0000 - val_loss: 0.0479 - val_accuracy: 0.9945 - 6s/epoch - 30ms/step\n",
      "Epoch 946/1000\n",
      "\n",
      "Epoch 946: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.5401e-05 - accuracy: 1.0000 - val_loss: 0.0469 - val_accuracy: 0.9944 - 6s/epoch - 30ms/step\n",
      "Epoch 947/1000\n",
      "\n",
      "Epoch 947: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.6797e-04 - accuracy: 1.0000 - val_loss: 0.0427 - val_accuracy: 0.9945 - 6s/epoch - 30ms/step\n",
      "Epoch 948/1000\n",
      "\n",
      "Epoch 948: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.9634e-05 - accuracy: 1.0000 - val_loss: 0.0443 - val_accuracy: 0.9943 - 6s/epoch - 30ms/step\n",
      "Epoch 949/1000\n",
      "\n",
      "Epoch 949: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.4583e-05 - accuracy: 1.0000 - val_loss: 0.0452 - val_accuracy: 0.9945 - 6s/epoch - 30ms/step\n",
      "Epoch 950/1000\n",
      "\n",
      "Epoch 950: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.3689e-04 - accuracy: 0.9999 - val_loss: 0.0517 - val_accuracy: 0.9938 - 6s/epoch - 30ms/step\n",
      "Epoch 951/1000\n",
      "\n",
      "Epoch 951: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.4952e-04 - accuracy: 1.0000 - val_loss: 0.0473 - val_accuracy: 0.9943 - 6s/epoch - 30ms/step\n",
      "Epoch 952/1000\n",
      "\n",
      "Epoch 952: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.2478e-05 - accuracy: 1.0000 - val_loss: 0.0471 - val_accuracy: 0.9943 - 6s/epoch - 29ms/step\n",
      "Epoch 953/1000\n",
      "\n",
      "Epoch 953: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.1140e-04 - accuracy: 0.9999 - val_loss: 0.0546 - val_accuracy: 0.9937 - 6s/epoch - 29ms/step\n",
      "Epoch 954/1000\n",
      "\n",
      "Epoch 954: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.7705e-05 - accuracy: 1.0000 - val_loss: 0.0528 - val_accuracy: 0.9944 - 6s/epoch - 29ms/step\n",
      "Epoch 955/1000\n",
      "\n",
      "Epoch 955: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.0700e-04 - accuracy: 0.9998 - val_loss: 0.0616 - val_accuracy: 0.9929 - 6s/epoch - 29ms/step\n",
      "Epoch 956/1000\n",
      "\n",
      "Epoch 956: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.6990e-04 - accuracy: 0.9998 - val_loss: 0.0566 - val_accuracy: 0.9934 - 6s/epoch - 30ms/step\n",
      "Epoch 957/1000\n",
      "\n",
      "Epoch 957: val_accuracy did not improve from 0.99518\n",
      "197/197 - 7s - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0500 - val_accuracy: 0.9940 - 7s/epoch - 34ms/step\n",
      "Epoch 958/1000\n",
      "\n",
      "Epoch 958: val_accuracy did not improve from 0.99518\n",
      "197/197 - 7s - loss: 6.4629e-04 - accuracy: 0.9999 - val_loss: 0.0529 - val_accuracy: 0.9936 - 7s/epoch - 34ms/step\n",
      "Epoch 959/1000\n",
      "\n",
      "Epoch 959: val_accuracy did not improve from 0.99518\n",
      "197/197 - 8s - loss: 6.9303e-04 - accuracy: 0.9998 - val_loss: 0.0486 - val_accuracy: 0.9946 - 8s/epoch - 40ms/step\n",
      "Epoch 960/1000\n",
      "\n",
      "Epoch 960: val_accuracy did not improve from 0.99518\n",
      "197/197 - 7s - loss: 1.8170e-04 - accuracy: 1.0000 - val_loss: 0.0468 - val_accuracy: 0.9943 - 7s/epoch - 38ms/step\n",
      "Epoch 961/1000\n",
      "\n",
      "Epoch 961: val_accuracy did not improve from 0.99518\n",
      "197/197 - 7s - loss: 4.4753e-04 - accuracy: 0.9999 - val_loss: 0.0487 - val_accuracy: 0.9942 - 7s/epoch - 35ms/step\n",
      "Epoch 962/1000\n",
      "\n",
      "Epoch 962: val_accuracy did not improve from 0.99518\n",
      "197/197 - 7s - loss: 1.3181e-04 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 0.9943 - 7s/epoch - 34ms/step\n",
      "Epoch 963/1000\n",
      "\n",
      "Epoch 963: val_accuracy did not improve from 0.99518\n",
      "197/197 - 7s - loss: 1.9533e-04 - accuracy: 1.0000 - val_loss: 0.0470 - val_accuracy: 0.9943 - 7s/epoch - 34ms/step\n",
      "Epoch 964/1000\n",
      "\n",
      "Epoch 964: val_accuracy did not improve from 0.99518\n",
      "197/197 - 8s - loss: 3.5260e-04 - accuracy: 0.9999 - val_loss: 0.0508 - val_accuracy: 0.9941 - 8s/epoch - 40ms/step\n",
      "Epoch 965/1000\n",
      "\n",
      "Epoch 965: val_accuracy did not improve from 0.99518\n",
      "197/197 - 7s - loss: 6.3876e-05 - accuracy: 1.0000 - val_loss: 0.0500 - val_accuracy: 0.9946 - 7s/epoch - 37ms/step\n",
      "Epoch 966/1000\n",
      "\n",
      "Epoch 966: val_accuracy did not improve from 0.99518\n",
      "197/197 - 7s - loss: 1.4427e-04 - accuracy: 1.0000 - val_loss: 0.0502 - val_accuracy: 0.9943 - 7s/epoch - 36ms/step\n",
      "Epoch 967/1000\n",
      "\n",
      "Epoch 967: val_accuracy did not improve from 0.99518\n",
      "197/197 - 7s - loss: 2.1126e-04 - accuracy: 0.9999 - val_loss: 0.0512 - val_accuracy: 0.9939 - 7s/epoch - 35ms/step\n",
      "Epoch 968/1000\n",
      "\n",
      "Epoch 968: val_accuracy did not improve from 0.99518\n",
      "197/197 - 7s - loss: 2.6079e-05 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 0.9943 - 7s/epoch - 35ms/step\n",
      "Epoch 969/1000\n",
      "\n",
      "Epoch 969: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.3179e-04 - accuracy: 0.9999 - val_loss: 0.0483 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 970/1000\n",
      "\n",
      "Epoch 970: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.2202e-04 - accuracy: 0.9999 - val_loss: 0.0478 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 971/1000\n",
      "\n",
      "Epoch 971: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 4.0350e-04 - accuracy: 0.9999 - val_loss: 0.0506 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 972/1000\n",
      "\n",
      "Epoch 972: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.8183e-04 - accuracy: 1.0000 - val_loss: 0.0499 - val_accuracy: 0.9941 - 6s/epoch - 31ms/step\n",
      "Epoch 973/1000\n",
      "\n",
      "Epoch 973: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.5311e-04 - accuracy: 0.9999 - val_loss: 0.0484 - val_accuracy: 0.9943 - 6s/epoch - 31ms/step\n",
      "Epoch 974/1000\n",
      "\n",
      "Epoch 974: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.4074e-05 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 0.9946 - 6s/epoch - 31ms/step\n",
      "Epoch 975/1000\n",
      "\n",
      "Epoch 975: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.5915e-04 - accuracy: 1.0000 - val_loss: 0.0430 - val_accuracy: 0.9943 - 6s/epoch - 31ms/step\n",
      "Epoch 976/1000\n",
      "\n",
      "Epoch 976: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.0185e-05 - accuracy: 1.0000 - val_loss: 0.0467 - val_accuracy: 0.9941 - 6s/epoch - 31ms/step\n",
      "Epoch 977/1000\n",
      "\n",
      "Epoch 977: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.4963e-04 - accuracy: 0.9999 - val_loss: 0.0510 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 978/1000\n",
      "\n",
      "Epoch 978: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 7.7641e-04 - accuracy: 0.9999 - val_loss: 0.0470 - val_accuracy: 0.9942 - 6s/epoch - 31ms/step\n",
      "Epoch 979/1000\n",
      "\n",
      "Epoch 979: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.6913e-04 - accuracy: 0.9998 - val_loss: 0.0559 - val_accuracy: 0.9929 - 6s/epoch - 31ms/step\n",
      "Epoch 980/1000\n",
      "\n",
      "Epoch 980: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.9132e-04 - accuracy: 0.9999 - val_loss: 0.0443 - val_accuracy: 0.9942 - 6s/epoch - 31ms/step\n",
      "Epoch 981/1000\n",
      "\n",
      "Epoch 981: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.1402e-04 - accuracy: 0.9998 - val_loss: 0.0503 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 982/1000\n",
      "\n",
      "Epoch 982: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.9006e-05 - accuracy: 1.0000 - val_loss: 0.0492 - val_accuracy: 0.9936 - 6s/epoch - 31ms/step\n",
      "Epoch 983/1000\n",
      "\n",
      "Epoch 983: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.0444e-05 - accuracy: 1.0000 - val_loss: 0.0494 - val_accuracy: 0.9939 - 6s/epoch - 31ms/step\n",
      "Epoch 984/1000\n",
      "\n",
      "Epoch 984: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 6.0408e-05 - accuracy: 1.0000 - val_loss: 0.0499 - val_accuracy: 0.9937 - 6s/epoch - 31ms/step\n",
      "Epoch 985/1000\n",
      "\n",
      "Epoch 985: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 9.6941e-05 - accuracy: 1.0000 - val_loss: 0.0536 - val_accuracy: 0.9936 - 6s/epoch - 31ms/step\n",
      "Epoch 986/1000\n",
      "\n",
      "Epoch 986: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 3.8811e-04 - accuracy: 0.9999 - val_loss: 0.0494 - val_accuracy: 0.9940 - 6s/epoch - 31ms/step\n",
      "Epoch 987/1000\n",
      "\n",
      "Epoch 987: val_accuracy did not improve from 0.99518\n",
      "197/197 - 7s - loss: 3.3082e-04 - accuracy: 0.9999 - val_loss: 0.0491 - val_accuracy: 0.9931 - 7s/epoch - 33ms/step\n",
      "Epoch 988/1000\n",
      "\n",
      "Epoch 988: val_accuracy did not improve from 0.99518\n",
      "197/197 - 7s - loss: 2.7712e-04 - accuracy: 0.9999 - val_loss: 0.0537 - val_accuracy: 0.9932 - 7s/epoch - 38ms/step\n",
      "Epoch 989/1000\n",
      "\n",
      "Epoch 989: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.2850e-04 - accuracy: 1.0000 - val_loss: 0.0520 - val_accuracy: 0.9938 - 6s/epoch - 33ms/step\n",
      "Epoch 990/1000\n",
      "\n",
      "Epoch 990: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.8093e-04 - accuracy: 1.0000 - val_loss: 0.0472 - val_accuracy: 0.9942 - 6s/epoch - 32ms/step\n",
      "Epoch 991/1000\n",
      "\n",
      "Epoch 991: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.1849e-04 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 0.9940 - 6s/epoch - 32ms/step\n",
      "Epoch 992/1000\n",
      "\n",
      "Epoch 992: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.5713e-05 - accuracy: 1.0000 - val_loss: 0.0474 - val_accuracy: 0.9940 - 6s/epoch - 32ms/step\n",
      "Epoch 993/1000\n",
      "\n",
      "Epoch 993: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.5915e-04 - accuracy: 0.9999 - val_loss: 0.0514 - val_accuracy: 0.9936 - 6s/epoch - 32ms/step\n",
      "Epoch 994/1000\n",
      "\n",
      "Epoch 994: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 8.2960e-04 - accuracy: 0.9998 - val_loss: 0.0519 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 995/1000\n",
      "\n",
      "Epoch 995: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 1.7863e-04 - accuracy: 0.9999 - val_loss: 0.0492 - val_accuracy: 0.9939 - 6s/epoch - 32ms/step\n",
      "Epoch 996/1000\n",
      "\n",
      "Epoch 996: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 2.0478e-04 - accuracy: 0.9999 - val_loss: 0.0503 - val_accuracy: 0.9937 - 6s/epoch - 32ms/step\n",
      "Epoch 997/1000\n",
      "\n",
      "Epoch 997: val_accuracy did not improve from 0.99518\n",
      "197/197 - 7s - loss: 2.7744e-04 - accuracy: 0.9999 - val_loss: 0.0530 - val_accuracy: 0.9936 - 7s/epoch - 34ms/step\n",
      "Epoch 998/1000\n",
      "\n",
      "Epoch 998: val_accuracy did not improve from 0.99518\n",
      "197/197 - 7s - loss: 8.4777e-05 - accuracy: 1.0000 - val_loss: 0.0514 - val_accuracy: 0.9939 - 7s/epoch - 35ms/step\n",
      "Epoch 999/1000\n",
      "\n",
      "Epoch 999: val_accuracy did not improve from 0.99518\n",
      "197/197 - 7s - loss: 9.6473e-04 - accuracy: 0.9998 - val_loss: 0.0489 - val_accuracy: 0.9942 - 7s/epoch - 35ms/step\n",
      "Epoch 1000/1000\n",
      "\n",
      "Epoch 1000: val_accuracy did not improve from 0.99518\n",
      "197/197 - 6s - loss: 5.0762e-05 - accuracy: 1.0000 - val_loss: 0.0472 - val_accuracy: 0.9943 - 6s/epoch - 32ms/step\n",
      "525/525 [==============================] - 2s 3ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Accuracy': 0.9951785714285715, 'Training Loss': 5.0762453611241654e-05, 'Validation Loss': 0.047197628766298294}\n"
     ]
    }
   ],
   "source": [
    "class CNN2Wrapper:\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        Initialize the CNN2Wrapper with training data.\n",
    "\n",
    "        Parameters:\n",
    "        X (np.ndarray): The training features.\n",
    "        y (np.ndarray): The training labels.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.results = {}\n",
    "        self.history = None\n",
    "        self.model = None\n",
    "        self.best_model_path = 'best_model.h5'\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prepare the data for training and testing.\n",
    "        \"\"\"\n",
    "        lb = LabelBinarizer()\n",
    "        y = lb.fit_transform(self.y)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, y, test_size=0.4, random_state=42, stratify=self.y)\n",
    "        self.X_train = self.X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "        self.X_test = self.X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "    def create_cnn_model_2(self):\n",
    "        \"\"\"\n",
    "        Create the second CNN model.\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            Input(shape=(28, 28, 1)),\n",
    "            Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Dropout(0.2),\n",
    "\n",
    "            Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Dropout(0.2),\n",
    "\n",
    "            Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Dropout(0.3),\n",
    "\n",
    "            Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D((2, 2)),\n",
    "            Dropout(0.2),\n",
    "\n",
    "            Flatten(),\n",
    "            Dense(512, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Train and evaluate the CNN2 model.\n",
    "        \"\"\"\n",
    "        self.prepare_data()\n",
    "        self.model = self.create_cnn_model_2()\n",
    "        \n",
    "        # To ensure TensorFlow uses the GPU if available\n",
    "        physical_devices = tf.config.list_physical_devices('GPU')\n",
    "        if len(physical_devices) > 0:\n",
    "            tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "            print(\"Using GPU\")\n",
    "        else:\n",
    "            print(\"No GPU found, using CPU\")\n",
    "\n",
    "        # Create a callback to save the best model based on validation accuracy\n",
    "        checkpoint = ModelCheckpoint(self.best_model_path, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "        self.history = self.model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            epochs=1000,\n",
    "            validation_data=(self.X_test, self.y_test),\n",
    "            callbacks=[checkpoint],\n",
    "            verbose=2,\n",
    "            batch_size=128\n",
    "        )\n",
    "        \n",
    "        # Load the best model\n",
    "        self.model = tf.keras.models.load_model(self.best_model_path)\n",
    "\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        y_true = np.argmax(self.y_test, axis=1)\n",
    "\n",
    "        test_accuracy = accuracy_score(y_true, y_pred_classes)\n",
    "\n",
    "        results = {\n",
    "            \"Test Accuracy\": test_accuracy,\n",
    "            \"Training Loss\": self.history.history['loss'][-1],\n",
    "            \"Validation Loss\": self.history.history['val_loss'][-1]\n",
    "        }\n",
    "\n",
    "        self.results = results\n",
    "\n",
    "        self.plot_history()\n",
    "        self.plot_confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "    def plot_history(self):\n",
    "        \"\"\"\n",
    "        Plot the training and validation loss and accuracy.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Plot training & validation loss values\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history.history['loss'], label='Training Loss')\n",
    "        plt.plot(self.history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        # Plot training & validation accuracy values\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(self.history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend(loc='lower right')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    def plot_confusion_matrix(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Plot the confusion matrix with proportions.\n",
    "        \"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(np.unique(y_true)))\n",
    "        plt.xticks(tick_marks, tick_marks, rotation=45)\n",
    "        plt.yticks(tick_marks, tick_marks)\n",
    "\n",
    "        fmt = '.2f'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in np.ndindex(cm.shape):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X_new_df, save_csv=False):\n",
    "        \"\"\"\n",
    "        Predict the class for new data and add a label column with the predicted class.\n",
    "\n",
    "        Parameters:\n",
    "        X_new_df (pd.DataFrame): The new data as a DataFrame.\n",
    "        save_csv (bool): Whether to save the predictions as a CSV file.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: The DataFrame with columns 'ImageId' and 'Label'.\n",
    "        \"\"\"\n",
    "        X_new = X_new_df.values.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "        predictions = self.model.predict(X_new)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        results_df = pd.DataFrame({'ImageId': np.arange(1, len(predicted_classes) + 1), 'Label': predicted_classes})\n",
    "        \n",
    "        if save_csv:\n",
    "            results_df.to_csv(\"submission.csv\", index=False)\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "    def print_results(self):\n",
    "        \"\"\"\n",
    "        Print the results of the models.\n",
    "        \"\"\"\n",
    "        print(self.results)\n",
    "\n",
    "\n",
    "X = train.drop(\"label\", axis=1).values\n",
    "y = train[\"label\"].values\n",
    "\n",
    "cnn2_wrapper = CNN2Wrapper(X, y)\n",
    "cnn2_wrapper.evaluate_model()\n",
    "cnn2_wrapper.print_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "527890b3-ec16-4ba7-ba19-e380db33b778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875/875 [==============================] - 2s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      9\n",
       "3            4      0\n",
       "4            5      3\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn2_wrapper.predict(test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d2256c-fce8-4db1-b00f-abf15d653c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
