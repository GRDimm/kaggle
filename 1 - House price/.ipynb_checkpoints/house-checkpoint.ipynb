{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef5858c2-b68b-4366-aed3-b607cc94499b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SalePrice']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "pd.set_option('display.max_columns', 99999)\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, target, train_df, test_df):\n",
    "        self.target = target\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        self.rare_categories_columns = [\"MSSubClass\", \"MSZoning\", \"LotShape\", \"LotConfig\", \"LandSlope\", \n",
    "                                            \"ExterCond\", \"RoofStyle\", \"Foundation\", \"HeatingQC\", \"Electrical\", \n",
    "                                            \"BsmtFullBath\", \"BsmtHalfBath\", \"HalfBath\", \"KitchenAbvGr\", \"Fireplaces\", \n",
    "                                            \"GarageType\", \"GarageCars\", \"SaleType\", \"SaleCondition\"]\n",
    "        self.outliers_columns = [\"LotFrontage\", \"LotArea\", \"MasVnrArea\", \"TotalBsmtSF\", \"1stFlrSF\", \n",
    "                                                \"GrLivArea\", \"SalePrice\"]\n",
    "        self.columns_to_drop = [\"Id\", \"MiscVal\", \"Utilities\", \"Condition2\", \"YearRemodAdd\", \"RoofMatl\", \"Street\", \n",
    "                              \"BsmtFinSF2\", \"Heating\", \"Functional\", \"GarageQual\", \"GarageCond\", \"EnclosedPorch\", \n",
    "                              \"LowQualFinSF\", \"3SsnPorch\", \"ScreenPorch\"]\n",
    "\n",
    "        self.to_norm_columns = [\"LotFrontage\", \"LotArea\", \"OverallQual\", \"OverallCond\", \"TotalBsmtSF\", \"1stFlrSF\", \n",
    "                                 \"GrLivArea\", \"BedroomAbvGr\", \"TotRmsAbvGrd\", \"YearBuilt\", \"GarageYrBlt\", \"MoSold\", \n",
    "                                 \"YrSold\"]\n",
    "        self.to_norm_non_zeros_column = [\"GarageArea\", \"2ndFlrSF\", \"BsmtUnfSF\", \"BsmtFinSF1\", \"MasVnrArea\", \n",
    "                                          \"WoodDeckSF\", \"OpenPorchSF\"]\n",
    "\n",
    "        self.to_encode_columns = [\"Fence\", \"HeatingQC\", \"BsmtFinType2\", \"BsmtFinType1\", \"BsmtQual\", \n",
    "                                                    \"MSSubClass\", \"MSZoning\", \"Alley\", \"LotShape\", \"LandContour\", \"LotConfig\", \n",
    "                                                    \"LandSlope\", \"Neighborhood\", \"Condition1\", \"BldgType\", \"HouseStyle\", \n",
    "                                                    \"RoofStyle\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"ExterQual\", \n",
    "                                                    \"ExterCond\", \"Foundation\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \n",
    "                                                    \"BsmtFinType1\", \"BsmtFinType2\", \"HeatingQC\", \"CentralAir\", \"Electrical\", \n",
    "                                                    \"BsmtFullBath\", \"FullBath\", \"HalfBath\", \"BsmtHalfBath\", \"KitchenQual\", \n",
    "                                                    \"Functional\", \"Fireplaces\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"PavedDrive\", \"SaleType\", \"SaleCondition\", \"Street\"]\n",
    "\n",
    "    def drop_columns_with_missing_values(self, df, alpha):\n",
    "        threshold = alpha / 100 * len(df)\n",
    "        columns_to_drop = df.columns[df.isnull().sum() > threshold]\n",
    "        df.drop(columns=columns_to_drop, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def one_hot_encode_columns(self, df, columns):\n",
    "        onehot_encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "        columns_to_encode = df[self.inter(df.columns, columns)]\n",
    "        encoded_data = onehot_encoder.fit_transform(columns_to_encode)\n",
    "        encoded_columns = onehot_encoder.get_feature_names_out(columns_to_encode.columns)\n",
    "        encoded_df = pd.DataFrame(encoded_data, columns=encoded_columns, index=df.index)\n",
    "        df = df.drop(self.inter(df.columns, columns), axis=1)\n",
    "        df = pd.concat([df, encoded_df], axis=1)\n",
    "        return df\n",
    "\n",
    "    def detect_and_drop_outliers(self, df, columns, iqr_multiplier=2):\n",
    "        for col in self.inter(columns, df.columns):\n",
    "            Q1 = df[col].quantile(0.075)\n",
    "            Q3 = df[col].quantile(0.925)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - iqr_multiplier * IQR\n",
    "            upper_bound = Q3 + iqr_multiplier * IQR\n",
    "            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "        return df\n",
    "\n",
    "    def drop_rare_categories(self, df, columns, max_drop_percentage=0.2):\n",
    "        for col in columns:\n",
    "            if col in df.columns:\n",
    "                freq = df[col].value_counts(normalize=True)\n",
    "                num_categories_to_examine = int(len(freq) * max_drop_percentage)\n",
    "                categories_to_examine = freq.tail(num_categories_to_examine).index\n",
    "                median_freq = freq[~freq.index.isin(categories_to_examine)].median()\n",
    "                categories_to_drop = [cat for cat in categories_to_examine if freq[cat] < median_freq]\n",
    "                df = df[~df[col].isin(categories_to_drop)]\n",
    "        return df\n",
    "\n",
    "    def normalize(self, df, columns, scaler=None):\n",
    "        if scaler:\n",
    "            df[columns] = scaler.transform(df[columns])\n",
    "        else:\n",
    "            scaler = StandardScaler()\n",
    "            df[columns] = scaler.fit_transform(df[columns])\n",
    "            \n",
    "        return df, scaler\n",
    "\n",
    "    def normalize_target(self, df):\n",
    "        df[self.target] = StandardScaler().fit_transform(df[[self.target]])\n",
    "        return df\n",
    "\n",
    "    def normalize_non_zero(self, df, columns, scaler=None):\n",
    "        combined_non_zero_mask = (df[columns] != 0).any(axis=1)\n",
    "        if combined_non_zero_mask.any():\n",
    "            non_zero_values = df.loc[combined_non_zero_mask, columns]\n",
    "            if scaler:\n",
    "                scaled_values = scaler.transform(non_zero_values)\n",
    "            else:\n",
    "                scaler = StandardScaler()\n",
    "                scaled_values = scaler.fit_transform(non_zero_values)\n",
    "                \n",
    "            df.loc[combined_non_zero_mask, columns] = scaled_values\n",
    "        return df, scaler\n",
    "\n",
    "    def impute_missing_values(self, df, imputer=None):\n",
    "        numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "        if imputer:\n",
    "            imputed_data = imputer.fit_transform(df[numeric_columns])\n",
    "        else:\n",
    "            imputer = SimpleImputer(strategy='mean')\n",
    "            imputed_data = imputer.fit_transform(df[numeric_columns])\n",
    "            \n",
    "        df[numeric_columns] = pd.DataFrame(imputed_data, columns=numeric_columns, index=df.index)\n",
    "        return df, imputer\n",
    "\n",
    "    def create_features(self, df):\n",
    "        df[\"Pool\"] = np.where(df[\"PoolArea\"] > 0, 1, 0)\n",
    "        return df\n",
    "\n",
    "    def inter(self, cols1, cols2):\n",
    "        return list(set(cols1).intersection(set(cols2)))\n",
    "\n",
    "    def without(self, cols1, cols2):\n",
    "        return list(set(cols1) - set(cols2))\n",
    "\n",
    "    def drop_entire_columns(self, df):\n",
    "        pass\n",
    "\n",
    "    def set_numeric_to_float(self):\n",
    "        def convert_column_type(column):\n",
    "            if column.dtype.kind in 'iufc':  # includes int, uint, float, complex\n",
    "                return column.astype('float64')\n",
    "            return column\n",
    "        \n",
    "        self.train_df = self.train_df.apply(convert_column_type)\n",
    "        self.test_df = self.test_df.apply(convert_column_type)\n",
    "    \n",
    "    def result(self):\n",
    "        self.set_numeric_to_float()\n",
    "\n",
    "        self.train_df = self.create_features(self.train_df)\n",
    "        self.test_df = self.create_features(self.test_df)\n",
    "\n",
    "        # Drop rows (only for train because we can't drop for test)\n",
    "        self.train_df = self.detect_and_drop_outliers(self.train_df, self.outliers_columns)\n",
    "        self.train_df = self.drop_rare_categories(self.train_df, self.rare_categories_columns)\n",
    "\n",
    "        # Scale\n",
    "        self.train_df, scaler = self.normalize(self.train_df, self.to_norm_columns)\n",
    "        self.train_df, non_zeros_scaler = self.normalize_non_zero(self.train_df, self.to_norm_non_zeros_column)\n",
    "\n",
    "        self.test_df, _ = self.normalize(self.test_df, self.to_norm_columns, scaler)\n",
    "        self.test_df, _ = self.normalize_non_zero(self.test_df, self.to_norm_non_zeros_column, non_zeros_scaler)\n",
    "        \n",
    "        #self.train_df = self.normalize_target(self.train_df)\n",
    "\n",
    "        # Impute\n",
    "        self.train_df, imputer = self.impute_missing_values(self.train_df)\n",
    "        self.test_df, _ = self.impute_missing_values(self.test_df, imputer)\n",
    "\n",
    "        # Encode\n",
    "        self.train_df = self.one_hot_encode_columns(self.train_df, self.to_encode_columns)\n",
    "        self.test_df = self.one_hot_encode_columns(self.test_df, self.to_encode_columns)\n",
    "        \n",
    "        # Drop cols\n",
    "        self.train_df = self.drop_columns_with_missing_values(self.train_df, 90)\n",
    "        self.test_df = self.drop_columns_with_missing_values(self.test_df, 90)\n",
    "\n",
    "        self.train_df = self.train_df.drop(columns=self.inter(self.train_df.columns, self.columns_to_drop))\n",
    "        self.test_df = self.test_df.drop(columns=self.inter(self.test_df.columns, self.columns_to_drop))\n",
    "\n",
    "        common_columns = self.inter(self.train_df.columns, self.test_df.columns)\n",
    "        \n",
    "        return self.train_df[common_columns + [self.target]], self.test_df[common_columns]\n",
    "\n",
    "# Assuming train and test DataFrames are already defined\n",
    "processor = Preprocessor(\"SalePrice\", train, test)\n",
    "\n",
    "train_processed, test_processed = processor.result()\n",
    "print(processor.without(train_processed.columns, test_processed.columns))\n",
    "print(processor.without(test_processed.columns, train_processed.columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cab795-e54b-4dbb-ba84-49cd9908977a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  42%|▍| 720/1728 [03:26<08:09,  2.06it/s, Current score=0.1354, Best score=0.1198, Iteration=720/"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.pyll.base import scope\n",
    "\n",
    "class XGBoostModelTuner:\n",
    "    def __init__(self, train_df, test_df, target):\n",
    "        \"\"\"\n",
    "        Initialise l'objet XGBoostModelTuner avec les DataFrames prétraités et le nom de la colonne cible.\n",
    "\n",
    "        Paramètres:\n",
    "        train_df (pd.DataFrame): Le DataFrame contenant les données d'entraînement.\n",
    "        test_df (pd.DataFrame): Le DataFrame contenant les données de test.\n",
    "        target (str): Le nom de la colonne cible.\n",
    "        \"\"\"\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.target = target\n",
    "        self.model = XGBRegressor()\n",
    "        self.results = {}\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prépare les données pour l'entraînement et le test.\n",
    "        \"\"\"\n",
    "        X = self.train_df.drop(self.target, axis=1)\n",
    "        y = self.train_df[self.target]\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    def tune_model(self):\n",
    "        \"\"\"\n",
    "        Effectue la fine-tuning du modèle XGBoost en utilisant une recherche de grille personnalisée.\n",
    "        \"\"\"\n",
    "        self.prepare_data()\n",
    "        \n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 5, 9],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'subsample': [0.7, 1],\n",
    "            'colsample_bytree': [0.7, 1],\n",
    "            'gamma': [0, 0.1],\n",
    "            'min_child_weight': [1, 3],\n",
    "            'reg_alpha': [0, 0.1],\n",
    "            'reg_lambda': [0.5, 1]\n",
    "        }\n",
    "        \n",
    "        param_list = list(ParameterGrid(param_grid))\n",
    "        best_score = float('inf')\n",
    "        best_params = None\n",
    "        total_iterations = len(param_list)\n",
    "        \n",
    "        # Utiliser tqdm pour afficher la barre de progression\n",
    "        pbar = tqdm(total=total_iterations, desc=\"Grid Search Progress\")\n",
    "    \n",
    "        for i, params in enumerate(param_list):\n",
    "            self.model.set_params(**params)\n",
    "            self.model.fit(self.X_train, self.y_train)\n",
    "            preds = self.model.predict(self.X_val)\n",
    "            score = root_mean_squared_error(np.log(self.y_val), np.log(preds))\n",
    "    \n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "            \n",
    "            # Mettre à jour la barre de progression et afficher les scores en temps réel\n",
    "            pbar.set_postfix({\n",
    "                'Current score': f'{score:.4f}', \n",
    "                'Best score': f'{best_score:.4f}',\n",
    "                'Iteration': f'{i+1}/{total_iterations}'\n",
    "            })\n",
    "            pbar.update(1)\n",
    "        \n",
    "        pbar.close()\n",
    "    \n",
    "        self.results['best_params'] = best_params\n",
    "\n",
    "        # Calculer la meilleure MSE obtenue\n",
    "        self.model = XGBRegressor(**best_params)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        best_preds = self.model.predict(self.X_val)\n",
    "        best_mse = mean_squared_error(self.y_val, best_preds)\n",
    "        \n",
    "        print(\"Meilleurs paramètres trouvés : \", best_params)\n",
    "        print(\"Meilleure MSE trouvée : \", best_mse)\n",
    "\n",
    "    def tune_model_grad(self, max_evals=1000):\n",
    "        \"\"\"\n",
    "        Effectue la fine-tuning du modèle XGBoost en utilisant l'optimisation bayésienne avec hyperopt.\n",
    "    \n",
    "        Parameters:\n",
    "        max_evals (int): Le nombre maximal d'évaluations à effectuer pour trouver les meilleurs hyperparamètres.\n",
    "        \"\"\"\n",
    "        self.prepare_data()\n",
    "        \n",
    "        def objective(params):\n",
    "            model = XGBRegressor(**params)\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            preds = model.predict(self.X_val)\n",
    "            score = root_mean_squared_error(np.log(self.y_val), np.log(preds))\n",
    "            return {'loss': score, 'status': STATUS_OK}\n",
    "        \n",
    "        param_space = {\n",
    "            'n_estimators': scope.int(hp.quniform('n_estimators', 50, 300, 50)),\n",
    "            'max_depth': scope.int(hp.quniform('max_depth', 3, 9, 1)),\n",
    "            'learning_rate': hp.loguniform('learning_rate', -5, -1),\n",
    "            'subsample': hp.uniform('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0),\n",
    "            'gamma': hp.uniform('gamma', 0.0, 0.2),\n",
    "            'min_child_weight': scope.int(hp.quniform('min_child_weight', 1, 5, 1)),\n",
    "            'reg_alpha': hp.loguniform('reg_alpha', -3, 0),\n",
    "            'reg_lambda': hp.uniform('reg_lambda', 1.0, 2.0)\n",
    "        }\n",
    "        \n",
    "        trials = Trials()\n",
    "        \n",
    "        # Utiliser tqdm pour afficher la barre de progression\n",
    "        with tqdm(total=max_evals, desc=\"Hyperopt Progress\") as pbar:\n",
    "            for i in range(max_evals):\n",
    "                # Ajuster le learning rate après chaque itération\n",
    "                if i > 0:\n",
    "                    learning_rate_values = [trial['misc']['vals']['learning_rate'][0] for trial in trials.trials]\n",
    "                    mean_learning_rate = sum(learning_rate_values) / len(learning_rate_values)\n",
    "                    param_space['learning_rate'] = hp.loguniform('learning_rate', -5, -1) * mean_learning_rate\n",
    "    \n",
    "                fmin(fn=objective,\n",
    "                     space=param_space,\n",
    "                     algo=tpe.suggest,\n",
    "                     max_evals=len(trials.trials) + 1,\n",
    "                     trials=trials,\n",
    "                     show_progressbar=False)  # Désactiver la barre de progression par défaut de fmin\n",
    "                pbar.update(1)\n",
    "        \n",
    "        # Obtenir les meilleurs paramètres trouvés par hyperopt\n",
    "        best = trials.best_trial['result']['loss']\n",
    "        \n",
    "        # Obtenir les paramètres associés au meilleur essai\n",
    "        best_params = {key: int(trials.best_trial['misc']['vals'][key][0]) if key in ['n_estimators', 'max_depth', 'min_child_weight'] else trials.best_trial['misc']['vals'][key][0] for key in trials.best_trial['misc']['vals']}\n",
    "        \n",
    "        self.results['best_params'] = best_params\n",
    "        \n",
    "        # Calculer la meilleure MSE obtenue\n",
    "        self.model = XGBRegressor(**best_params)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        best_preds = self.model.predict(self.X_val)\n",
    "        best_mse = mean_squared_error(self.y_val, best_preds)\n",
    "        \n",
    "        print(\"Meilleurs paramètres trouvés : \", best_params)\n",
    "        print(\"Meilleure MSE trouvée : \", best_mse)\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Évalue le modèle sur l'ensemble de validation et enregistre les statistiques.\n",
    "        \"\"\"\n",
    "        y_pred = self.model.predict(self.X_val)\n",
    "        mse = mean_squared_error(self.y_val, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(self.y_val, y_pred)\n",
    "        r2 = r2_score(self.y_val, y_pred)\n",
    "\n",
    "        self.results['test_mse'] = mse\n",
    "        self.results['test_rmse'] = rmse\n",
    "        self.results['test_mae'] = mae\n",
    "        self.results['r2_score'] = r2\n",
    "\n",
    "        # Print statistics\n",
    "        print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "        print(f\"R^2 Score: {r2:.4f}\")\n",
    "\n",
    "    def plot_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Affiche les importances des caractéristiques du modèle XGBoost.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plot_importance(self.model, importance_type='weight')\n",
    "        plt.title('Feature Importance')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_residuals(self):\n",
    "        \"\"\"\n",
    "        Affiche le graphique des résidus.\n",
    "        \"\"\"\n",
    "        y_pred = self.model.predict(self.X_val)\n",
    "        residuals = self.y_val - y_pred\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(residuals, kde=True)\n",
    "        plt.title('Residuals Histogram')\n",
    "        plt.xlabel('Residuals')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "        self.residuals = residuals\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Exécute les étapes de fine-tuning, d'entraînement, d'évaluation et de génération des graphiques.\n",
    "        \"\"\"\n",
    "        self.tune_model()\n",
    "        self.evaluate_model()\n",
    "        self.plot_feature_importance()\n",
    "        self.plot_residuals()\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Prédit les valeurs cibles pour les données de test en utilisant le modèle entraîné.\n",
    "    \n",
    "        Retourne:\n",
    "        np.ndarray: Les prédictions du modèle.\n",
    "        \"\"\"\n",
    "        predictions = self.model.predict(self.test_df)\n",
    "        return predictions\n",
    "\n",
    "def predict_save(tuner):\n",
    "    predictions = tuner.predict()\n",
    "    # Load the sample submission file\n",
    "    submission = pd.read_csv(\"sample_submission.csv\")\n",
    "    #849764555\n",
    "    #643001230\n",
    "    # Update the \"SalePrice\" column with predictions\n",
    "    submission[\"SalePrice\"] = predictions\n",
    "    \n",
    "    # Drop the \"Unnamed: 0\" column if it exists\n",
    "    if 'Unnamed: 0' in submission.columns:\n",
    "        submission.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "    # Save the updated submission file without the index column\n",
    "    submission.to_csv(\"sample_submission.csv\", index=False)\n",
    "    \n",
    "tuner_XGB = XGBoostModelTuner(train_processed, test_processed, 'SalePrice')\n",
    "tuner_XGB.run()\n",
    "predict_save(tuner_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65de1cd8-4fcd-4c5a-92f5-8a079ae25b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_save(tuner_XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e241531c-caf1-4d93-82a8-ce9f44c266fc",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c48bb10-e0ce-4003-b078-277767f4edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from lightgbm import LGBMRegressor, plot_importance\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.pyll.base import scope\n",
    "from tqdm import tqdm\n",
    "\n",
    "class LightGBMModelTuner:\n",
    "    def __init__(self, train_df, test_df, target):\n",
    "        \"\"\"\n",
    "        Initialise l'objet LightGBMModelTuner avec les DataFrames prétraités et le nom de la colonne cible.\n",
    "\n",
    "        Paramètres:\n",
    "        train_df (pd.DataFrame): Le DataFrame contenant les données d'entraînement.\n",
    "        test_df (pd.DataFrame): Le DataFrame contenant les données de test.\n",
    "        target (str): Le nom de la colonne cible.\n",
    "        \"\"\"\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.target = target\n",
    "        self.model = LGBMRegressor(verbose=-1)\n",
    "        self.results = {}\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prépare les données pour l'entraînement et le test.\n",
    "        \"\"\"\n",
    "        X = self.train_df.drop(self.target, axis=1)\n",
    "        y = self.train_df[self.target]\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "    def tune_model(self):\n",
    "        \"\"\"\n",
    "        Effectue la fine-tuning du modèle LightGBM en utilisant une recherche de grille personnalisée.\n",
    "        \"\"\"\n",
    "        self.prepare_data()\n",
    "        \n",
    "        param_grid = {\n",
    "            'n_estimators': [200, 500, 600],\n",
    "            'max_depth': [3, 5, 7, 9],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'subsample': [0.7, 1],\n",
    "            'colsample_bytree': [0.7, 1],\n",
    "            'min_child_samples': [10, 20, 30],\n",
    "            'reg_alpha': [0, 0.1, 0.25],\n",
    "            'reg_lambda': [0.5, 1]\n",
    "        }\n",
    "    \n",
    "        param_list = list(ParameterGrid(param_grid))\n",
    "        best_score = float('inf')\n",
    "        best_params = None\n",
    "        total_iterations = len(param_list)\n",
    "        \n",
    "        # Utiliser tqdm pour afficher la barre de progression\n",
    "        pbar = tqdm(total=total_iterations, desc=\"Grid Search Progress\")\n",
    "    \n",
    "        for i, params in enumerate(param_list):\n",
    "            self.model.set_params(**params)\n",
    "            self.model.fit(self.X_train, self.y_train)\n",
    "            preds = self.model.predict(self.X_val)\n",
    "            score = root_mean_squared_error(np.log(self.y_val), np.log(preds))\n",
    "    \n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "            \n",
    "            # Mettre à jour la barre de progression et afficher les scores en temps réel\n",
    "            pbar.set_postfix({\n",
    "                'Current score': f'{score:.4f}', \n",
    "                'Best score': f'{best_score:.4f}',\n",
    "                'Iteration': f'{i+1}/{total_iterations}'\n",
    "            })\n",
    "            pbar.update(1)\n",
    "        \n",
    "        pbar.close()\n",
    "    \n",
    "        self.results['best_params'] = best_params\n",
    "        \n",
    "        # Calculer la meilleure MSE obtenue\n",
    "        self.model = LGBMRegressor(**best_params, verbose=-1)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        best_preds = self.model.predict(self.X_val)\n",
    "        best_mse = mean_squared_error(self.y_val, best_preds)\n",
    "        \n",
    "        print(\"Meilleurs paramètres trouvés : \", best_params)\n",
    "        print(\"Meilleure MSE trouvée : \", best_mse)\n",
    "\n",
    "    def tune_model_grad(self, max_evals=1000):\n",
    "        \"\"\"\n",
    "        Effectue la fine-tuning du modèle LightGBM en utilisant l'optimisation bayésienne avec hyperopt.\n",
    "    \n",
    "        Parameters:\n",
    "        max_evals (int): Le nombre maximal d'évaluations à effectuer pour trouver les meilleurs hyperparamètres.\n",
    "        \"\"\"\n",
    "        self.prepare_data()\n",
    "        \n",
    "        def objective(params):\n",
    "            model = LGBMRegressor(**params, verbose=-1)\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            preds = model.predict(self.X_val)\n",
    "            score = root_mean_squared_error(np.log(self.y_val), np.log(preds))\n",
    "            return {'loss': score, 'status': STATUS_OK}\n",
    "        \n",
    "        param_space = {\n",
    "            'n_estimators': scope.int(hp.quniform('n_estimators', 50, 300, 50)),\n",
    "            'max_depth': scope.int(hp.quniform('max_depth', 3, 9, 1)),\n",
    "            'learning_rate': hp.loguniform('learning_rate', -5, -1),\n",
    "            'subsample': hp.uniform('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0),\n",
    "            'min_child_samples': scope.int(hp.quniform('min_child_samples', 5, 50, 5)),\n",
    "            'reg_alpha': hp.loguniform('reg_alpha', -3, 0),\n",
    "            'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "            'num_leaves': scope.int(hp.quniform('num_leaves', 20, 150, 5)),\n",
    "            'min_split_gain': hp.uniform('min_split_gain', 0.0, 1.0)\n",
    "        }\n",
    "                \n",
    "        trials = Trials()\n",
    "        \n",
    "        # Utiliser tqdm pour afficher la barre de progression\n",
    "        with tqdm(total=max_evals, desc=\"Hyperopt Progress\") as pbar:\n",
    "            for i in range(max_evals):\n",
    "                # Ajuster le learning rate après chaque itération\n",
    "                if i > 0:\n",
    "                    learning_rate_values = [trial['misc']['vals']['learning_rate'][0] for trial in trials.trials]\n",
    "                    mean_learning_rate = sum(learning_rate_values) / len(learning_rate_values)\n",
    "                    param_space['learning_rate'] = hp.loguniform('learning_rate', -5, -1) * mean_learning_rate\n",
    "    \n",
    "                fmin(fn=objective,\n",
    "                     space=param_space,\n",
    "                     algo=tpe.suggest,\n",
    "                     max_evals=len(trials.trials) + 1,\n",
    "                     trials=trials,\n",
    "                     show_progressbar=False)  # Désactiver la barre de progression par défaut de fmin\n",
    "                pbar.update(1)\n",
    "        \n",
    "        # Obtenir les meilleurs paramètres trouvés par hyperopt\n",
    "        best = trials.best_trial['result']['loss']\n",
    "        \n",
    "        # Obtenir les paramètres associés au meilleur essai\n",
    "        best_params = {key: int(trials.best_trial['misc']['vals'][key][0]) if key in ['n_estimators', 'max_depth', 'min_child_samples', 'num_leaves'] else trials.best_trial['misc']['vals'][key][0] for key in trials.best_trial['misc']['vals']}\n",
    "        \n",
    "        self.results['best_params'] = best_params\n",
    "        \n",
    "        # Calculer la meilleure MSE obtenue\n",
    "        self.model = LGBMRegressor(**best_params, verbose=-1)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        best_preds = self.model.predict(self.X_val)\n",
    "        best_mse = mean_squared_error(self.y_val, best_preds)\n",
    "        \n",
    "        print(\"Meilleurs paramètres trouvés : \", best_params)\n",
    "        print(\"Meilleure MSE trouvée : \", best_mse)\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Évalue le modèle sur l'ensemble de validation et enregistre les statistiques.\n",
    "        \"\"\"\n",
    "        y_pred = self.model.predict(self.X_val)\n",
    "        mse = mean_squared_error(self.y_val, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(self.y_val, y_pred)\n",
    "        r2 = r2_score(self.y_val, y_pred)\n",
    "\n",
    "        self.results['test_mse'] = mse\n",
    "        self.results['test_rmse'] = rmse\n",
    "        self.results['test_mae'] = mae\n",
    "        self.results['r2_score'] = r2\n",
    "\n",
    "        # Print statistics\n",
    "        print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "        print(f\"R^2 Score: {r2:.4f}\")\n",
    "\n",
    "    def plot_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Affiche les importances des caractéristiques du modèle LightGBM.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plot_importance(self.model, importance_type='split')\n",
    "        plt.title('Feature Importance')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_residuals(self):\n",
    "        \"\"\"\n",
    "        Affiche le graphique des résidus.\n",
    "        \"\"\"\n",
    "        y_pred = self.model.predict(self.X_val)\n",
    "        residuals = self.y_val - y_pred\n",
    "        self.residuals = residuals\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(residuals, kde=True)\n",
    "        plt.title('Residuals Histogram')\n",
    "        plt.xlabel('Residuals')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Exécute les étapes de fine-tuning, d'entraînement, d'évaluation et de génération des graphiques.\n",
    "        \"\"\"\n",
    "        self.tune_model()\n",
    "        self.evaluate_model()\n",
    "        self.plot_feature_importance()\n",
    "        self.plot_residuals()\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Prédit les valeurs cibles pour les données de test en utilisant le modèle entraîné.\n",
    "    \n",
    "        Retourne:\n",
    "        np.ndarray: Les prédictions du modèle.\n",
    "        \"\"\"\n",
    "        predictions = self.model.predict(self.test_df)\n",
    "        return predictions\n",
    "\n",
    "tuner_LGBM = LightGBMModelTuner(train_processed, test_processed, 'SalePrice')\n",
    "tuner_LGBM.run()\n",
    "predict_save(tuner_LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c90474-7ffb-42a1-b403-b117d1b2d9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(tuner_XGB.residuals, kde=True, alpha=0.5, label=\"XGB\", stat='density')\n",
    "sns.histplot(tuner_LGBM.residuals, kde=True, alpha=0.5, label=\"LGBM\", stat='density')\n",
    "plt.title('Residuals Histogram')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "bce3059a-aaa4-4dd9-a544-3925b1e92a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model Linear Regression...\n",
      "Testing model Ridge Regression...\n",
      "Testing model Lasso Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimma\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.486e+10, tolerance: 4.353e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\dimma\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.002e+11, tolerance: 4.450e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\dimma\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.956e+10, tolerance: 3.918e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\dimma\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+10, tolerance: 4.491e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\dimma\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.289e+10, tolerance: 4.364e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\dimma\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.226e+11, tolerance: 5.397e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model ElasticNet...\n",
      "Testing model Bayesian Ridge...\n",
      "Testing model Random Forest...\n",
      "Testing model Gradient Boosting...\n",
      "Testing model AdaBoost...\n",
      "Testing model Support Vector Regression...\n",
      "Testing model K-Neighbors Regressor...\n",
      "Testing model Decision Tree...\n",
      "Testing model MLP Regressor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimma\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dimma\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dimma\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dimma\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dimma\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dimma\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model XGBoost...\n",
      "Testing model LightGBM...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000683 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2342\n",
      "[LightGBM] [Info] Number of data points in the train set: 716, number of used features: 136\n",
      "[LightGBM] [Info] Start training from score 181154.203911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2330\n",
      "[LightGBM] [Info] Number of data points in the train set: 716, number of used features: 132\n",
      "[LightGBM] [Info] Start training from score 179903.885475\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2335\n",
      "[LightGBM] [Info] Number of data points in the train set: 716, number of used features: 132\n",
      "[LightGBM] [Info] Start training from score 178449.236034\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000703 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2355\n",
      "[LightGBM] [Info] Number of data points in the train set: 716, number of used features: 137\n",
      "[LightGBM] [Info] Start training from score 183074.642458\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000692 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2356\n",
      "[LightGBM] [Info] Number of data points in the train set: 716, number of used features: 135\n",
      "[LightGBM] [Info] Start training from score 181802.160615\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000843 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2706\n",
      "[LightGBM] [Info] Number of data points in the train set: 895, number of used features: 147\n",
      "[LightGBM] [Info] Start training from score 180876.825698\n",
      "Linear Regression:\n",
      "  Cross-Validation Mean Squared Error: 814651855.0880\n",
      "  Test Mean Squared Error: 804407465.5719\n",
      "  Test Root Mean Squared Error: 28362.0779\n",
      "  Test Mean Absolute Error: 17852.1557\n",
      "  Test R^2 Score: 0.8905\n",
      "\n",
      "Ridge Regression:\n",
      "  Cross-Validation Mean Squared Error: 611958112.4098\n",
      "  Test Mean Squared Error: 741386454.3744\n",
      "  Test Root Mean Squared Error: 27228.4126\n",
      "  Test Mean Absolute Error: 17170.7616\n",
      "  Test R^2 Score: 0.8990\n",
      "\n",
      "Lasso Regression:\n",
      "  Cross-Validation Mean Squared Error: 769153843.4003\n",
      "  Test Mean Squared Error: 798077902.4084\n",
      "  Test Root Mean Squared Error: 28250.2726\n",
      "  Test Mean Absolute Error: 17757.2947\n",
      "  Test R^2 Score: 0.8913\n",
      "\n",
      "ElasticNet:\n",
      "  Cross-Validation Mean Squared Error: 833410705.3174\n",
      "  Test Mean Squared Error: 1108786644.2036\n",
      "  Test Root Mean Squared Error: 33298.4481\n",
      "  Test Mean Absolute Error: 20009.8475\n",
      "  Test R^2 Score: 0.8490\n",
      "\n",
      "Bayesian Ridge:\n",
      "  Cross-Validation Mean Squared Error: 575700703.7201\n",
      "  Test Mean Squared Error: 701735237.3631\n",
      "  Test Root Mean Squared Error: 26490.2857\n",
      "  Test Mean Absolute Error: 16893.0630\n",
      "  Test R^2 Score: 0.9044\n",
      "\n",
      "Random Forest:\n",
      "  Cross-Validation Mean Squared Error: 772053342.3459\n",
      "  Test Mean Squared Error: 880064573.9559\n",
      "  Test Root Mean Squared Error: 29665.8823\n",
      "  Test Mean Absolute Error: 18765.7603\n",
      "  Test R^2 Score: 0.8802\n",
      "\n",
      "Gradient Boosting:\n",
      "  Cross-Validation Mean Squared Error: 583427557.4942\n",
      "  Test Mean Squared Error: 712517184.4822\n",
      "  Test Root Mean Squared Error: 26693.0175\n",
      "  Test Mean Absolute Error: 16983.2525\n",
      "  Test R^2 Score: 0.9030\n",
      "\n",
      "AdaBoost:\n",
      "  Cross-Validation Mean Squared Error: 989703409.6443\n",
      "  Test Mean Squared Error: 1262196116.2882\n",
      "  Test Root Mean Squared Error: 35527.3995\n",
      "  Test Mean Absolute Error: 23947.8244\n",
      "  Test R^2 Score: 0.8281\n",
      "\n",
      "Support Vector Regression:\n",
      "  Cross-Validation Mean Squared Error: 6448069605.1390\n",
      "  Test Mean Squared Error: 7871204148.8426\n",
      "  Test Root Mean Squared Error: 88719.8070\n",
      "  Test Mean Absolute Error: 60872.0769\n",
      "  Test R^2 Score: -0.0718\n",
      "\n",
      "K-Neighbors Regressor:\n",
      "  Cross-Validation Mean Squared Error: 1208384689.0324\n",
      "  Test Mean Squared Error: 1138598388.8336\n",
      "  Test Root Mean Squared Error: 33743.1236\n",
      "  Test Mean Absolute Error: 20393.1875\n",
      "  Test R^2 Score: 0.8450\n",
      "\n",
      "Decision Tree:\n",
      "  Cross-Validation Mean Squared Error: 1398376747.1810\n",
      "  Test Mean Squared Error: 1911857552.5893\n",
      "  Test Root Mean Squared Error: 43724.7933\n",
      "  Test Mean Absolute Error: 26604.8393\n",
      "  Test R^2 Score: 0.7397\n",
      "\n",
      "MLP Regressor:\n",
      "  Cross-Validation Mean Squared Error: 37320886627.5752\n",
      "  Test Mean Squared Error: 38804711435.8159\n",
      "  Test Root Mean Squared Error: 196989.1150\n",
      "  Test Mean Absolute Error: 177919.8805\n",
      "  Test R^2 Score: -4.2839\n",
      "\n",
      "XGBoost:\n",
      "  Cross-Validation Mean Squared Error: 806539230.8307\n",
      "  Test Mean Squared Error: 1012491913.4418\n",
      "  Test Root Mean Squared Error: 31819.6781\n",
      "  Test Mean Absolute Error: 19668.7549\n",
      "  Test R^2 Score: 0.8621\n",
      "\n",
      "LightGBM:\n",
      "  Cross-Validation Mean Squared Error: 652465993.9425\n",
      "  Test Mean Squared Error: 726230380.0681\n",
      "  Test Root Mean Squared Error: 26948.6619\n",
      "  Test Mean Absolute Error: 17398.4216\n",
      "  Test R^2 Score: 0.9011\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, df, target):\n",
    "        \"\"\"\n",
    "        Initialise l'objet ModelEvaluator avec un DataFrame et le nom de la colonne cible.\n",
    "\n",
    "        Paramètres:\n",
    "        df (pd.DataFrame): Le DataFrame contenant les données.\n",
    "        target (str): Le nom de la colonne cible.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.target = target\n",
    "        self.models = {\n",
    "            \"Linear Regression\": LinearRegression(),\n",
    "            \"Ridge Regression\": Ridge(),\n",
    "            \"Lasso Regression\": Lasso(),\n",
    "            \"ElasticNet\": ElasticNet(),\n",
    "            \"Bayesian Ridge\": BayesianRidge(),\n",
    "            \"Random Forest\": RandomForestRegressor(),\n",
    "            \"Gradient Boosting\": GradientBoostingRegressor(),\n",
    "            \"AdaBoost\": AdaBoostRegressor(),\n",
    "            \"Support Vector Regression\": SVR(),\n",
    "            \"K-Neighbors Regressor\": KNeighborsRegressor(),\n",
    "            \"Decision Tree\": DecisionTreeRegressor(),\n",
    "            \"MLP Regressor\": MLPRegressor(),\n",
    "            \"XGBoost\": XGBRegressor(),\n",
    "            \"LightGBM\": LGBMRegressor()\n",
    "        }\n",
    "        self.results = {}\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prépare les données pour l'entraînement et le test.\n",
    "        \"\"\"\n",
    "        X = self.df.drop(self.target, axis=1)\n",
    "        y = self.df[self.target]\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    def evaluate_models(self):\n",
    "        \"\"\"\n",
    "        Entraîne et évalue les modèles en utilisant la validation croisée et l'ensemble de test.\n",
    "        \"\"\"\n",
    "        self.prepare_data()\n",
    "        for name, model in self.models.items():\n",
    "            print(f'Testing model {name}...')\n",
    "            # Validation croisée\n",
    "            scores = cross_val_score(model, self.X_train, self.y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "            mean_cv_score = -scores.mean()\n",
    "\n",
    "            # Entraînement sur l'ensemble d'entraînement complet et évaluation sur l'ensemble de test\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            y_pred = model.predict(self.X_test)\n",
    "            test_mse = mean_squared_error(self.y_test, y_pred)\n",
    "            test_rmse = np.sqrt(test_mse)\n",
    "            test_mae = mean_absolute_error(self.y_test, y_pred)\n",
    "            test_r2 = r2_score(self.y_test, y_pred)\n",
    "\n",
    "            # Stockage des résultats\n",
    "            self.results[name] = {\n",
    "                \"Cross-Validation MSE\": mean_cv_score,\n",
    "                \"Test MSE\": test_mse,\n",
    "                \"Test RMSE\": test_rmse,\n",
    "                \"Test MAE\": test_mae,\n",
    "                \"Test R^2\": test_r2\n",
    "            }\n",
    "\n",
    "    def print_results(self):\n",
    "        \"\"\"\n",
    "        Affiche les résultats des modèles.\n",
    "        \"\"\"\n",
    "        for name, metrics in self.results.items():\n",
    "            print(f\"{name}:\")\n",
    "            print(f\"  Cross-Validation Mean Squared Error: {metrics['Cross-Validation MSE']:.4f}\")\n",
    "            print(f\"  Test Mean Squared Error: {metrics['Test MSE']:.4f}\")\n",
    "            print(f\"  Test Root Mean Squared Error: {metrics['Test RMSE']:.4f}\")\n",
    "            print(f\"  Test Mean Absolute Error: {metrics['Test MAE']:.4f}\")\n",
    "            print(f\"  Test R^2 Score: {metrics['Test R^2']:.4f}\\n\")\n",
    "\n",
    "# Example usage\n",
    "m = ModelEvaluator(train_processed, \"SalePrice\")\n",
    "m.evaluate_models()\n",
    "m.print_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a42d3606-9c47-4d3f-bda2-57fbedeacf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimma\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Summarize dataset:  94%|█████████████████████████████████  | 83/88 [00:01<00:00, 76.83it/s, Calculate auto correlation]C:\\Users\\dimma\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\ydata_profiling\\model\\correlations.py:66: UserWarning: There was an attempt to calculate the auto correlation, but this failed.\n",
      "To hide this warning, disable the calculation\n",
      "(using `df.profile_report(correlations={\"auto\": {\"calculate\": False}})`\n",
      "If this is problematic for your use case, please report this as an issue:\n",
      "https://github.com/ydataai/ydata-profiling/issues\n",
      "(include the error message: 'could not convert string to float: 'Grvl'')\n",
      "  warnings.warn(\n",
      "Summarize dataset: 100%|██████████████████████████████████████████████████| 991/991 [01:19<00:00, 12.52it/s, Completed]\n",
      "Generate report structure: 100%|█████████████████████████████████████████████████████████| 1/1 [00:16<00:00, 16.39s/it]\n",
      "Render HTML: 100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:16<00:00, 16.19s/it]\n",
      "Export report to file: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Load your DataFrame (replace with your data loading method)\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Generate a profiling report\n",
    "profile = ProfileReport(df, title=\"Pandas Profiling Report\", explorative=True)\n",
    "\n",
    "# Save the report as an HTML file\n",
    "profile.to_file(\"data_profile_report.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "80237b87-6f88-412c-9087-e470106fe446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0, 3.0 (1.0, 3.00)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_str_to_number(s):\n",
    "    try:\n",
    "        # Essayer de convertir en int\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        try:\n",
    "            # Essayer de convertir en float\n",
    "            return float(s)\n",
    "        except ValueError:\n",
    "            # Lever une exception si la conversion échoue\n",
    "            raise ValueError(f\"La valeur '{s}' ne peut être convertie ni en int ni en float.\")\n",
    "\n",
    "\n",
    "def check(s):\n",
    "    s1 = s[1:-1].split(\",\")\n",
    "    a = convert_str_to_number(s1[0])\n",
    "    b = convert_str_to_number(s1[1])\n",
    "    print(str(a) + \", \" + str(b), s)\n",
    "\n",
    "    return len(s) - 2 == len(str(a) + \", \" + str(b))\n",
    "\n",
    "check(\"(1.0, 3.00)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4685e8d-dc16-43bd-91f3-19988e02838c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
